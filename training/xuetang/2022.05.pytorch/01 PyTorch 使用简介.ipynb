{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PyTorch 入门与实战 实验1：PyTorch使用简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h2><a id=\"introduction\">一、实验介绍</a></h2>\n",
    "\n",
    "\n",
    "### 1.1 实验内容\n",
    "\n",
    "Pytorch 是由 Facebook 支持的一套深度学习开源框架，\n",
    "\n",
    "相比较 Tensorflow，它更加容易快速上手，所以一经推出就广受欢迎。\n",
    "\n",
    "本课程是采用 Pytorch 开源框架进行案例讲解的深度学习课程。\n",
    "\n",
    "Tensor（张量）是 PyTorch 的基础数据结构，自动微分运算是深度学习的核心。\n",
    "\n",
    "在本实验中我们将学习 PyTorch 中 Tensor 的用法，以及简单的自动微分变量原理，\n",
    "\n",
    "最后，我们还会使用 PyTorch 构建一个简单的线性回归网络。\n",
    "\n",
    "### 1.2 实验知识点 \n",
    "\n",
    "- PyTorch 简介\n",
    "\n",
    "- PyTorch 中的张量及其运算\n",
    "\n",
    "- PyTorch 中的自动微分运算\n",
    "\n",
    "- 用 PyTorch 实现线性回归\n",
    "\n",
    "### 1.3 实验环境\n",
    "\n",
    "- Python 3.7\n",
    "\n",
    "- PyTorch 0.4.0\n",
    "\n",
    "- Jupyter Notebook\n",
    "\n",
    "\n",
    "### 1.4 适合人群\n",
    "\n",
    "本课程难度为一般，属于初级实践级别课程，适合具有 Python 基础并对深度学习有一定认识的用户，将 PyTorch 应用到简单问题的解决中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h3 ><a id = \"index\"> 1.5 索引目录</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a href=\"#introduction\">一、实验介绍</a>\n",
    "\n",
    "<a href=\"#start\">二、有关张量（Tensor）运算的练习</a>\n",
    "\n",
    "<a href=\"#train\">三、有关自动微分（Autograd）变量的练习</a>\n",
    "\n",
    "<a href=\"#load\">四、利用PyTorch实现简单的线性回归算法</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h2 ><a id = \"start\">二、有关张量（Tensor）运算的练习</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1  使用 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/ca1a7a29da164dedb0e9965723a8f466e033e96be07a488e9fb011f5f29bdd55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PyTorch 是一个开源的深度学习框架，由 Facebook 支持开发。\n",
    "\n",
    "它的前身为 Torch，但因为 Torch 使用的编程语言是 Lua，在国内流行度很小。\n",
    "\n",
    "Facebook 为了迎合数量更多的 Python 用户的需求，推出了 PyTorch。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PyTorch 完全开源意味着你可以轻易获取它的代码，并按照自己的需求对它进行修改。\n",
    "\n",
    "比如让 PyTorch 支持复数运算等等。\n",
    "\n",
    "PyTorch 还有另外一个非常出众的特点是，\n",
    "\n",
    "使用 PyTorch 框架编写出的神经网络模型的代码非常简洁。\n",
    "\n",
    "实现同样的功能，使用 PyTorch 框架编写的代码往往更清晰明了，\n",
    "\n",
    "这点我们可以从下图中略见一斑："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/6e3e018baff74cfc992eb405047c9b7f15c8d4e035d442418d6c7eb23d037744)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PyTorch 的基本数据单元是张量（Tensor），它实际上是一种 N 维数组。\n",
    "\n",
    "下面我们列举了三种张量，可以看到它们的维度阶数是不同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ad68350fd8ea4bf3add4fc90ab1774a945f0e4d289694b1a94c4f67293994e05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1 阶的张量可以看做是一个向量，通过索引可以取到一个“值”。\n",
    "\n",
    "2 阶张量可以看做为一个矩阵，通过索引可以取到一个个的向量。\n",
    "\n",
    "3 阶张量有点抽象，不过我们可以从图中看出，\n",
    "\n",
    "3 阶张量其实就是在 2 阶张量的矩阵中增加了一个深度。\n",
    "\n",
    "也就是说在 3 阶张量中我们可以通过索引取到一个个的矩阵。\n",
    "\n",
    "我们不难想象，4 阶张量也就是在 3 阶张量上增加了另外一个轴……\n",
    "\n",
    "我们可以使用 `Tensor.size()` 方法获得一个张量的“尺寸”。\n",
    "\n",
    "在这里注意“尺寸”和维度是两个概念。\n",
    "\n",
    "就比如对于上图中的 1 阶张量，它的维度为 1，尺寸为 8；\n",
    "\n",
    "对于上图中的 2 阶张量，它的维度为 2，尺寸为（8，6）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "要使用 PyTorch，首先需要在 Python 中引入 PyTorch 的包。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "import torch  #导入torch包\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以通过以下代码查看当前系统中 PyTorch 的版本："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "print(torch.__version__)\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以生成随机数张量："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "x = torch.rand(5, 3)  #产生一个5*3的tensor，随机取值\n",
    "x  #显示x的值\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7087, 0.5821, 0.6089],\n",
       "        [0.7223, 0.1104, 0.4178],\n",
       "        [0.4896, 0.1000, 0.5388],\n",
       "        [0.6833, 0.3905, 0.6496],\n",
       "        [0.3805, 0.5553, 0.2154]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以使用 `zeros`，`ones` 方法生成包含固定值的张量："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "y = torch.ones(5, 3) #产生一个5*3的Tensor，元素都是1\n",
    "y\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2  基本 Tensor 运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "两个 2 阶张量相加的方法实际上就是矩阵加法。\n",
    "\n",
    "注意，要使两个张量相加，必须保证两个张量的尺寸是一致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "z = x + y #两个tensor可以直接相加\n",
    "z\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7087, 1.5821, 1.6089],\n",
       "        [1.7223, 1.1104, 1.4178],\n",
       "        [1.4896, 1.1000, 1.5388],\n",
       "        [1.6833, 1.3905, 1.6496],\n",
       "        [1.3805, 1.5553, 1.2154]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x+y\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下面的语句展示了两个 tensor 按照矩阵的方式相乘，注意 x 的尺寸是 5*3，\n",
    "\n",
    "y 的尺寸也是 5*3 无法进行矩阵乘法，所以先将 y 进行转置。\n",
    "\n",
    "转置操作可以用 `.t()` 来完成，也可以用 `transpose(0, 1)` 来完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/4be92f3f5e49431fb98ff6fba6caaf20ff3008003609477fafccd0603bc2880a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "q = x.mm(y.t()) #x乘以y的转置\n",
    "q\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8997, 1.8997, 1.8997, 1.8997, 1.8997],\n",
       "        [1.2505, 1.2505, 1.2505, 1.2505, 1.2505],\n",
       "        [1.1285, 1.1285, 1.1285, 1.1285, 1.1285],\n",
       "        [1.7233, 1.7233, 1.7233, 1.7233, 1.7233],\n",
       "        [1.1512, 1.1512, 1.1512, 1.1512, 1.1512]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = x.mm(y.t())\n",
    "q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "所有的Tensor的使用方法请见参考链接中的“Tensor支持的所有操作”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3  Tensor 与 numpy.ndarray 之间的转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PyTorch 的 Tensor 可以与 Python 的常用数据处理包 Numpy 中的多维数组进行转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "import numpy as np #导入numpy包\n",
    "a = np.ones([5, 3]) #建立一个5*3全是m1的二维数组（矩阵）\n",
    "b = torch.from_numpy(a) #利用from_numpy将其转换为tensor\n",
    "b\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones([5,3])\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下面是另外一种转换 Tensor 的方法，类型为 FloatTensor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "# 还可以使LongTensor，整型数据类型\n",
    "c = torch.FloatTensor(a) \n",
    "c\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.FloatTensor(a)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "还可以从一个 tensor 转化为 numpy 的多维数组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "b.numpy()  \n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Tensor 和 Numpy 的最大区别在于 Tensor 可以在 GPU 上进行运算。\n",
    "\n",
    "默认情况下，Tensor 是在 CPU 上进行运算的，\n",
    "\n",
    "如果我们需要一个 Tensor 在 GPU 上的实例，需要运行这个 Tensor 的 `.cuda()` 方法。\n",
    "\n",
    "在下面的代码中，首先判断在本机上是否有 GPU 环境可用（有 NVIDIA的 GPU，并安装了驱动）。\n",
    "\n",
    "如果有 GPU 环境可用，那么再去获得张量 `x`，`y` 的 GPU 实例。\n",
    "\n",
    "注意在最后打印 `x` 和 `y` 这两个 GPU 张量的和的时候，我们调用了 `.cpu()` 方法，\n",
    "\n",
    "意思是将 GPU 张量转化为 CPU 张量，否则系统会报错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "if torch.cuda.is_available():  #检测本机器上有无GPU可用\n",
    "    x = x.cuda() #返回x的GPU上运算的版本\n",
    "    y = y.cuda()\n",
    "    z = x + y\n",
    "print(z.cpu()) # 打印时注意要把GPU变量转化为CPU变量。\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7087, 1.5821, 1.6089],\n",
      "        [1.7223, 1.1104, 1.4178],\n",
      "        [1.4896, 1.1000, 1.5388],\n",
      "        [1.6833, 1.3905, 1.6496],\n",
      "        [1.3805, 1.5553, 1.2154]])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "z = x + y\n",
    "print(z.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a href=\"#index\">回到目录</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h2 ><a id = \"train\">三、有关自动微分（Autograd）变量的练习</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "动态运算图(Dynamic Computation Graph)是 PyTorch 的最主要特性，\n",
    "\n",
    "它可以让我们的计算模型更灵活、复杂，并可以让反向传播算法随时进行。\n",
    "\n",
    "而反向传播算法就是深度神经网络的核心。\n",
    "\n",
    "下面是一个计算图的结构以及与它对应的 PyTorch 代码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c39b05601ddc4dd98aa8bb2b5e3e82724e3cf42a03984de481b3960a0d6d602a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "用来构建计算图的数据叫做自动微分变量（Variable），它与 Tensor 不同。\n",
    "\n",
    "每个 Variable 包含三个属性，分别对应着数据（data），父节点（creator），以及梯度（grad）。\n",
    "\n",
    "其中“梯度”就是反向传播算法所要传播的信息。\n",
    "\n",
    "而父节点用于将每个节点连接起来构建计算图（如上图所示）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ac35a254318a4c77a6fb6213a2300e1ffb40ca4def864704b00b32bd596a0844)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下面我们编写代码实际使用自动微分变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "#导入自动梯度的运算包，主要用Variable这个类\n",
    "from torch.autograd import Variable  \n",
    "\n",
    "#创建一个Variable，包裹了一个2*2张量，将需要计算梯度属性置为True\n",
    "\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)  \n",
    "x\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in /root/anaconda3/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in /root/anaconda3/lib/python3.9/site-packages (from torchviz) (1.11.0)\n",
      "Requirement already satisfied: graphviz in /root/anaconda3/lib/python3.9/site-packages (from torchviz) (0.20)\n",
      "Requirement already satisfied: typing_extensions in /root/anaconda3/lib/python3.9/site-packages (from torch->torchviz) (4.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:298: UserWarning: torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\n",
      "  warnings.warn(\"torch.autograd.variable(...) is deprecated, use torch.tensor(...) instead\")\n",
      "/root/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:299: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "from torch.autograd import variable\n",
    "\n",
    "x = variable(torch.ones(2,2), requires_grad=True)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"67pt\" height=\"39pt\"\n viewBox=\"0.00 0.00 67.00 39.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 35)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-35 63,-35 63,4 -4,4\"/>\n<!-- 140323078808896 -->\n<g id=\"node1\" class=\"node\">\n<title>140323078808896</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"59,-31 0,-31 0,0 59,0 59,-31\"/>\n<text text-anchor=\"middle\" x=\"29.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2, 2)</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f9f8329ecd0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,2,requires_grad=True)\n",
    "\n",
    "make_dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以按照 Tensor 的方式进行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "y = x + 2  #可以按照Tensor的方式进行计算\n",
    "y.grad_fn  #每个Variable都有一个creator（创造者节点）\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f9f833a5580>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x+2\n",
    "y.grad_fn\n",
    "\n",
    "# make_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"109pt\" height=\"216pt\"\n viewBox=\"0.00 0.00 109.00 216.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 212)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-212 105,-212 105,4 -4,4\"/>\n<!-- 140323129863280 -->\n<g id=\"node1\" class=\"node\">\n<title>140323129863280</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"80,-31 21,-31 21,0 80,0 80,-31\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2, 2)</text>\n</g>\n<!-- 140323078165888 -->\n<g id=\"node2\" class=\"node\">\n<title>140323078165888</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-86 6,-86 6,-67 95,-67 95,-86\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 140323078165888&#45;&gt;140323129863280 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140323078165888&#45;&gt;140323129863280</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-66.9688C50.5,-60.1289 50.5,-50.5621 50.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-41.3678 50.5,-31.3678 47.0001,-41.3678 54.0001,-41.3678\"/>\n</g>\n<!-- 140323078165696 -->\n<g id=\"node3\" class=\"node\">\n<title>140323078165696</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140323078165696&#45;&gt;140323078165888 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140323078165696&#45;&gt;140323078165888</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-121.9197C50.5,-114.9083 50.5,-105.1442 50.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-96.3408 50.5,-86.3408 47.0001,-96.3409 54.0001,-96.3408\"/>\n</g>\n<!-- 140323078808896 -->\n<g id=\"node4\" class=\"node\">\n<title>140323078808896</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"80,-208 21,-208 21,-177 80,-177 80,-208\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2, 2)</text>\n</g>\n<!-- 140323078808896&#45;&gt;140323078165696 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140323078808896&#45;&gt;140323078165696</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-176.791C50.5,-169.0249 50.5,-159.5706 50.5,-151.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-151.0647 50.5,-141.0648 47.0001,-151.0648 54.0001,-151.0647\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f9f8335cd90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "经过上面变量 `x` 和 `y` 的运算，我们就有了一个简单的计算图，它是下面这个样子的："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/566dd7a2870649f2a3cd1686bece0e8e1f7d0d316ab1487dbf5fdb7586f04b3d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下面我们让计算图再复杂一点，我们再加入变量 z:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**注意，.data 可以反回一个 Variable 所包裹的 Tensor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "z = torch.mean(y * y)  #也可以进行复合运算，比如求均值mean\n",
    "z.data #.data属性可以返回z所包裹的tensor\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.mean(y*y)\n",
    "z.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"109pt\" height=\"326pt\"\n viewBox=\"0.00 0.00 109.00 326.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 322)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-322 105,-322 105,4 -4,4\"/>\n<!-- 140323129861920 -->\n<g id=\"node1\" class=\"node\">\n<title>140323129861920</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"77.5,-31 23.5,-31 23.5,0 77.5,0 77.5,-31\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140323077154752 -->\n<g id=\"node2\" class=\"node\">\n<title>140323077154752</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"98,-86 3,-86 3,-67 98,-67 98,-86\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MeanBackward0</text>\n</g>\n<!-- 140323077154752&#45;&gt;140323129861920 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140323077154752&#45;&gt;140323129861920</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-66.9688C50.5,-60.1289 50.5,-50.5621 50.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-41.3678 50.5,-31.3678 47.0001,-41.3678 54.0001,-41.3678\"/>\n</g>\n<!-- 140323077154656 -->\n<g id=\"node3\" class=\"node\">\n<title>140323077154656</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-141 6,-141 6,-122 95,-122 95,-141\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140323077154656&#45;&gt;140323077154752 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140323077154656&#45;&gt;140323077154752</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-121.9197C50.5,-114.9083 50.5,-105.1442 50.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-96.3408 50.5,-86.3408 47.0001,-96.3409 54.0001,-96.3408\"/>\n</g>\n<!-- 140323078165888 -->\n<g id=\"node4\" class=\"node\">\n<title>140323078165888</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-196 6,-196 6,-177 95,-177 95,-196\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 140323078165888&#45;&gt;140323077154656 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140323078165888&#45;&gt;140323077154656</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M45.2332,-176.9197C43.8546,-169.9083 43.4371,-160.1442 43.9804,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"47.479,-151.6838 45.1826,-141.3408 40.5278,-150.8583 47.479,-151.6838\"/>\n</g>\n<!-- 140323078165888&#45;&gt;140323077154656 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140323078165888&#45;&gt;140323077154656</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M55.7668,-176.9197C57.1454,-169.9083 57.5629,-160.1442 57.0196,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"60.4722,-150.8583 55.8174,-141.3408 53.521,-151.6838 60.4722,-150.8583\"/>\n</g>\n<!-- 140323078165696 -->\n<g id=\"node5\" class=\"node\">\n<title>140323078165696</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-251 0,-251 0,-232 101,-232 101,-251\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140323078165696&#45;&gt;140323078165888 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140323078165696&#45;&gt;140323078165888</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-231.9197C50.5,-224.9083 50.5,-215.1442 50.5,-206.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-206.3408 50.5,-196.3408 47.0001,-206.3409 54.0001,-206.3408\"/>\n</g>\n<!-- 140323078808896 -->\n<g id=\"node6\" class=\"node\">\n<title>140323078808896</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"80,-318 21,-318 21,-287 80,-287 80,-318\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2, 2)</text>\n</g>\n<!-- 140323078808896&#45;&gt;140323078165696 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140323078808896&#45;&gt;140323078165696</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-286.791C50.5,-279.0249 50.5,-269.5706 50.5,-261.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-261.0647 50.5,-251.0648 47.0001,-261.0648 54.0001,-261.0647\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f9f832aed00>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "现在我们的计算图是这个样子的："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c72d451e4b6c41769f09f12e70bbd2a3b76da5e80a6a487bb17eaf132f0f5f26)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`backward` 可以实施反向传播算法，并计算所有计算图上叶子节点（没有子节点）的导数（梯度）信息。\n",
    "\n",
    "注意，由于 z 和 y 都不是叶子节点，所以都没有梯度信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "z.backward() #梯度反向传播\n",
    "print(z.grad) # 无梯度信息\n",
    "print(y.grad) # 无梯度信息\n",
    "print(x.grad)\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "tensor([[1.5000, 1.5000],\n",
      "        [1.5000, 1.5000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/build/aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(z.grad)\n",
    "print(y.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在下面的例子中，会让矩阵 x 反复作用在向量 s 上，系统会自动记录中间的依赖关系和长路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "s = Variable(torch.FloatTensor([[0.01, 0.02]]), requires_grad = True) #创建一个1*2的Variable（1维向量）\n",
    "x = Variable(torch.ones(2, 2), requires_grad = True) #创建一个2*2的矩阵型Variable\n",
    "for i in range(10):\n",
    "    s = s.mm(x)  #反复用s乘以x（矩阵乘法），注意s始终是1*2的Variable\n",
    "z = torch.mean(s) #对s中的各个元素求均值，得到一个1*1的scalar（标量，即1*1张量）\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"445pt\" height=\"766pt\"\n viewBox=\"0.00 0.00 445.00 766.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 762)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-762 441,-762 441,4 -4,4\"/>\n<!-- 140326605510832 -->\n<g id=\"node1\" class=\"node\">\n<title>140326605510832</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"416.5,-31 362.5,-31 362.5,0 416.5,0 416.5,-31\"/>\n<text text-anchor=\"middle\" x=\"389.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140323076664672 -->\n<g id=\"node2\" class=\"node\">\n<title>140323076664672</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"437,-86 342,-86 342,-67 437,-67 437,-86\"/>\n<text text-anchor=\"middle\" x=\"389.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MeanBackward0</text>\n</g>\n<!-- 140323076664672&#45;&gt;140326605510832 -->\n<g id=\"edge24\" class=\"edge\">\n<title>140323076664672&#45;&gt;140326605510832</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M389.5,-66.9688C389.5,-60.1289 389.5,-50.5621 389.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"393.0001,-41.3678 389.5,-31.3678 386.0001,-41.3678 393.0001,-41.3678\"/>\n</g>\n<!-- 140323076664816 -->\n<g id=\"node3\" class=\"node\">\n<title>140323076664816</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"431,-141 348,-141 348,-122 431,-122 431,-141\"/>\n<text text-anchor=\"middle\" x=\"389.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 140323076664816&#45;&gt;140323076664672 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140323076664816&#45;&gt;140323076664672</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M389.5,-121.9197C389.5,-114.9083 389.5,-105.1442 389.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"393.0001,-96.3408 389.5,-86.3408 386.0001,-96.3409 393.0001,-96.3408\"/>\n</g>\n<!-- 140323076664768 -->\n<g id=\"node4\" class=\"node\">\n<title>140323076664768</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"396,-196 313,-196 313,-177 396,-177 396,-196\"/>\n<text text-anchor=\"middle\" x=\"354.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 140323076664768&#45;&gt;140323076664816 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140323076664768&#45;&gt;140323076664816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M360.5965,-176.9197C365.3062,-169.5188 371.9676,-159.0509 377.7086,-150.0294\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"380.8217,-151.6565 383.2377,-141.3408 374.916,-147.8984 380.8217,-151.6565\"/>\n</g>\n<!-- 140323076664000 -->\n<g id=\"node5\" class=\"node\">\n<title>140323076664000</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"358,-251 275,-251 275,-232 358,-232 358,-251\"/>\n<text text-anchor=\"middle\" x=\"316.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 140323076664000&#45;&gt;140323076664768 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140323076664000&#45;&gt;140323076664768</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M323.1191,-231.9197C328.2863,-224.4409 335.6174,-213.8301 341.8944,-204.745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"344.8961,-206.5577 347.7009,-196.3408 339.137,-202.5786 344.8961,-206.5577\"/>\n</g>\n<!-- 140323076664144 -->\n<g id=\"node6\" class=\"node\">\n<title>140323076664144</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"320,-306 237,-306 237,-287 320,-287 320,-306\"/>\n<text text-anchor=\"middle\" x=\"278.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 140323076664144&#45;&gt;140323076664000 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140323076664144&#45;&gt;140323076664000</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M285.1191,-286.9197C290.2863,-279.4409 297.6174,-268.8301 303.8944,-259.745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"306.8961,-261.5577 309.7009,-251.3408 301.137,-257.5786 306.8961,-261.5577\"/>\n</g>\n<!-- 140323076664240 -->\n<g id=\"node7\" class=\"node\">\n<title>140323076664240</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"282,-361 199,-361 199,-342 282,-342 282,-361\"/>\n<text text-anchor=\"middle\" x=\"240.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 140323076664240&#45;&gt;140323076664144 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140323076664240&#45;&gt;140323076664144</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M247.1191,-341.9197C252.2863,-334.4409 259.6174,-323.8301 265.8944,-314.745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"268.8961,-316.5577 271.7009,-306.3408 263.137,-312.5786 268.8961,-316.5577\"/>\n</g>\n<!-- 140323076664336 -->\n<g id=\"node8\" class=\"node\">\n<title>140323076664336</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"244,-416 161,-416 161,-397 244,-397 244,-416\"/>\n<text text-anchor=\"middle\" x=\"202.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 140323076664336&#45;&gt;140323076664240 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140323076664336&#45;&gt;140323076664240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M209.1191,-396.9197C214.2863,-389.4409 221.6174,-378.8301 227.8944,-369.745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"230.8961,-371.5577 233.7009,-361.3408 225.137,-367.5786 230.8961,-371.5577\"/>\n</g>\n<!-- 140323076664432 -->\n<g id=\"node9\" class=\"node\">\n<title>140323076664432</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"206,-471 123,-471 123,-452 206,-452 206,-471\"/>\n<text text-anchor=\"middle\" x=\"164.5\" y=\"-459\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 140323076664432&#45;&gt;140323076664336 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140323076664432&#45;&gt;140323076664336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M171.1191,-451.9197C176.2863,-444.4409 183.6174,-433.8301 189.8944,-424.745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"192.8961,-426.5577 195.7009,-416.3408 187.137,-422.5786 192.8961,-426.5577\"/>\n</g>\n<!-- 140323076664528 -->\n<g id=\"node10\" class=\"node\">\n<title>140323076664528</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"168,-526 85,-526 85,-507 168,-507 168,-526\"/>\n<text text-anchor=\"middle\" x=\"126.5\" y=\"-514\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 140323076664528&#45;&gt;140323076664432 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140323076664528&#45;&gt;140323076664432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M133.1191,-506.9197C138.2863,-499.4409 145.6174,-488.8301 151.8944,-479.745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"154.8961,-481.5577 157.7009,-471.3408 149.137,-477.5786 154.8961,-481.5577\"/>\n</g>\n<!-- 140323076665056 -->\n<g id=\"node11\" class=\"node\">\n<title>140323076665056</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"130,-581 47,-581 47,-562 130,-562 130,-581\"/>\n<text text-anchor=\"middle\" x=\"88.5\" y=\"-569\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 140323076665056&#45;&gt;140323076664528 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140323076665056&#45;&gt;140323076664528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M95.1191,-561.9197C100.2863,-554.4409 107.6174,-543.8301 113.8944,-534.745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.8961,-536.5577 119.7009,-526.3408 111.137,-532.5786 116.8961,-536.5577\"/>\n</g>\n<!-- 140323076665152 -->\n<g id=\"node12\" class=\"node\">\n<title>140323076665152</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"92,-636 9,-636 9,-617 92,-617 92,-636\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-624\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MmBackward0</text>\n</g>\n<!-- 140323076665152&#45;&gt;140323076665056 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140323076665152&#45;&gt;140323076665056</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M57.1191,-616.9197C62.2863,-609.4409 69.6174,-598.8301 75.8944,-589.745\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"78.8961,-591.5577 81.7009,-581.3408 73.137,-587.5786 78.8961,-591.5577\"/>\n</g>\n<!-- 140323076665248 -->\n<g id=\"node13\" class=\"node\">\n<title>140323076665248</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-691 0,-691 0,-672 101,-672 101,-691\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-679\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140323076665248&#45;&gt;140323076665152 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140323076665248&#45;&gt;140323076665152</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-671.9197C50.5,-664.9083 50.5,-655.1442 50.5,-646.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-646.3408 50.5,-636.3408 47.0001,-646.3409 54.0001,-646.3408\"/>\n</g>\n<!-- 140323076674320 -->\n<g id=\"node14\" class=\"node\">\n<title>140323076674320</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"80,-758 21,-758 21,-727 80,-727 80,-758\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-734\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1, 2)</text>\n</g>\n<!-- 140323076674320&#45;&gt;140323076665248 -->\n<g id=\"edge12\" class=\"edge\">\n<title>140323076674320&#45;&gt;140323076665248</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-726.791C50.5,-719.0249 50.5,-709.5706 50.5,-701.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-701.0647 50.5,-691.0648 47.0001,-701.0648 54.0001,-701.0647\"/>\n</g>\n<!-- 140323076664624 -->\n<g id=\"node15\" class=\"node\">\n<title>140323076664624</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"304,-691 203,-691 203,-672 304,-672 304,-691\"/>\n<text text-anchor=\"middle\" x=\"253.5\" y=\"-679\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140323076664624&#45;&gt;140323076664816 -->\n<g id=\"edge23\" class=\"edge\">\n<title>140323076664624&#45;&gt;140323076664816</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M302.0443,-671.9998C352.5994,-658.9529 424.5,-630.0971 424.5,-571.5 424.5,-571.5 424.5,-571.5 424.5,-241.5 424.5,-208.1097 409.6746,-171.5882 399.3215,-150.1549\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"402.4302,-148.5462 394.8157,-141.1793 396.1742,-151.6867 402.4302,-148.5462\"/>\n</g>\n<!-- 140323076664624&#45;&gt;140323076664768 -->\n<g id=\"edge22\" class=\"edge\">\n<title>140323076664624&#45;&gt;140323076664768</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M283.017,-671.9419C322.1999,-657.1825 386.5,-624.8366 386.5,-571.5 386.5,-571.5 386.5,-571.5 386.5,-296.5 386.5,-263.5124 373.0677,-227.0959 363.6084,-205.5404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"366.6963,-203.8765 359.3601,-196.2325 360.3283,-206.783 366.6963,-203.8765\"/>\n</g>\n<!-- 140323076664624&#45;&gt;140323076664000 -->\n<g id=\"edge21\" class=\"edge\">\n<title>140323076664624&#45;&gt;140323076664000</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M270.9245,-671.8518C298.3502,-655.2353 348.5,-618.4341 348.5,-571.5 348.5,-571.5 348.5,-571.5 348.5,-351.5 348.5,-318.5124 335.0677,-282.0959 325.6084,-260.5404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"328.6963,-258.8765 321.3601,-251.2325 322.3283,-261.783 328.6963,-258.8765\"/>\n</g>\n<!-- 140323076664624&#45;&gt;140323076664144 -->\n<g id=\"edge20\" class=\"edge\">\n<title>140323076664624&#45;&gt;140323076664144</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M262.658,-671.7721C278.7706,-653.5941 310.5,-612.5819 310.5,-571.5 310.5,-571.5 310.5,-571.5 310.5,-406.5 310.5,-373.5124 297.0677,-337.0959 287.6084,-315.5404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"290.6963,-313.8765 283.3601,-306.2325 284.3283,-316.783 290.6963,-313.8765\"/>\n</g>\n<!-- 140323076664624&#45;&gt;140323076664240 -->\n<g id=\"edge19\" class=\"edge\">\n<title>140323076664624&#45;&gt;140323076664240</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M256.3035,-671.8849C261.5887,-652.8819 272.5,-609.0972 272.5,-571.5 272.5,-571.5 272.5,-571.5 272.5,-461.5 272.5,-428.5124 259.0677,-392.0959 249.6084,-370.5404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"252.6963,-368.8765 245.3601,-361.2325 246.3283,-371.783 252.6963,-368.8765\"/>\n</g>\n<!-- 140323076664624&#45;&gt;140323076664336 -->\n<g id=\"edge18\" class=\"edge\">\n<title>140323076664624&#45;&gt;140323076664336</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M250.6965,-671.8849C245.4113,-652.8819 234.5,-609.0972 234.5,-571.5 234.5,-571.5 234.5,-571.5 234.5,-516.5 234.5,-483.5124 221.0677,-447.0959 211.6084,-425.5404\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"214.6963,-423.8765 207.3601,-416.2325 208.3283,-426.783 214.6963,-423.8765\"/>\n</g>\n<!-- 140323076664624&#45;&gt;140323076664432 -->\n<g id=\"edge17\" class=\"edge\">\n<title>140323076664624&#45;&gt;140323076664432</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M245.1932,-671.9705C237.8548,-663.1474 227.2926,-649.4049 220.5,-636 213.3229,-621.8361 183.2879,-523.6564 170.4072,-481.1005\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"173.7022,-479.9043 167.4598,-471.3437 167.0013,-481.9286 173.7022,-479.9043\"/>\n</g>\n<!-- 140323076664624&#45;&gt;140323076664528 -->\n<g id=\"edge16\" class=\"edge\">\n<title>140323076664624&#45;&gt;140323076664528</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M232.2767,-671.9343C216.8347,-664.0977 196.2977,-651.7457 182.5,-636 156.1931,-605.9791 139.7564,-561.4742 131.9395,-536.1031\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"135.2085,-534.8134 129.0314,-526.2056 128.4924,-536.7868 135.2085,-534.8134\"/>\n</g>\n<!-- 140323076664624&#45;&gt;140323076665056 -->\n<g id=\"edge15\" class=\"edge\">\n<title>140323076664624&#45;&gt;140323076665056</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M219.9945,-671.9733C197.3206,-664.499 167.4386,-652.5719 144.5,-636 126.7507,-623.1771 110.8464,-603.6885 100.5636,-589.4894\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"103.2561,-587.2324 94.6552,-581.0457 97.5207,-591.2457 103.2561,-587.2324\"/>\n</g>\n<!-- 140323076664624&#45;&gt;140323076665152 -->\n<g id=\"edge13\" class=\"edge\">\n<title>140323076664624&#45;&gt;140323076665152</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M218.1401,-671.9197C184.1866,-662.7205 132.7435,-648.7827 95.8027,-638.7741\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"96.362,-635.2995 85.7947,-636.0626 94.5314,-642.0559 96.362,-635.2995\"/>\n</g>\n<!-- 140323125092976 -->\n<g id=\"node16\" class=\"node\">\n<title>140323125092976</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"283,-758 224,-758 224,-727 283,-727 283,-758\"/>\n<text text-anchor=\"middle\" x=\"253.5\" y=\"-734\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (2, 2)</text>\n</g>\n<!-- 140323125092976&#45;&gt;140323076664624 -->\n<g id=\"edge14\" class=\"edge\">\n<title>140323125092976&#45;&gt;140323076664624</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M253.5,-726.791C253.5,-719.0249 253.5,-709.5706 253.5,-701.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"257.0001,-701.0647 253.5,-691.0648 250.0001,-701.0648 257.0001,-701.0647\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f9f83236a60>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.FloatTensor([[0.01,0.02]])\n",
    "\n",
    "s.requires_grad = True\n",
    "\n",
    "# s = variable(torch.FloatTensor([[0.01, 0.02]]), requires_grad = True)\n",
    "\n",
    "x = torch.ones(2,2, requires_grad=True)\n",
    "\n",
    "for i in range(10):\n",
    "    s = s.mm(x)\n",
    "    # if s.grad is not None:\n",
    "    #     s.grad.data.zero_()\n",
    "z = torch.mean(s)\n",
    "\n",
    "make_dot(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "然后我们得到了一个复杂的“深度”计算图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/da4a32609df244a88fd80efa1c5c6a50d4a33a3746f4408e81a1612604d4cd52)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "z.backward() #在具有很长的依赖路径的计算图上用反向传播算法计算叶节点的梯度\n",
    "print(x.grad)  #x作为叶节点可以获得这部分梯度信息\n",
    "print(s.grad)  #s不是叶节点，没有梯度信息\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[37.1200, 37.1200],\n",
      "        [39.6800, 39.6800]])\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/build/aten/src/ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(s.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "如果大家觉得去理解这个计算图的叶子结点很困难，这也没有关系。\n",
    "\n",
    "因为我们研究的主题是深度学习，PyTorch 框架会自动搭建好计算图的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a href=\"#index\">回到目录</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h2 ><a id = \"load\">四、利用PyTorch实现简单的线性回归算法</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.1 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下面使用 PyTorch 实现一个简单的线性回归算法。\n",
    "\n",
    "线性回归是机器学习中最基础和简单的算法，你可以将它视为深度学习界的 Hello World。\n",
    "\n",
    "如果不了解线性回归，你可以简单的理解为：训练一条直线，让这条直线拟合一些数据点的趋势。\n",
    "\n",
    "大部分的同学在高中或者大学肯定接触过线性回归，所以我在这里也不做过多的介绍了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "首先生成一些样本点作为原始数据。\n",
    "\n",
    "这些原始“数据点”就是直线需要拟合的对象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "# linspace可以生成0-100之间的均匀的100个数字\n",
    "x = Variable(torch.linspace(0, 100).type(torch.FloatTensor)) \n",
    "\n",
    "# 随机生成100个满足标准正态分布的随机数，均值为0，方差为1.\n",
    "# 将这个数字乘以10，标准方差变为10\n",
    "rand = Variable(torch.randn(100)) * 10 \n",
    "\n",
    "# 将x和rand相加，得到伪造的标签数据y。\n",
    "# 所以(x,y)应能近似地落在y=x这条直线上\n",
    "y = x + rand \n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 12.5685, -15.3181,  19.9585,  -0.3206,   5.4090,  10.1961,  26.9607,\n",
       "         21.9912,  16.5674,  38.0884,  -7.2593,  17.5441, -17.0731,  17.6397,\n",
       "         14.3823,  19.3625,  14.6550,  27.7373,  12.8454,  -6.9307,  32.1929,\n",
       "         10.8110,   6.0826,  28.8345,  16.0299,  21.7565,  15.2801,  22.3711,\n",
       "         28.9794,  24.4778,  35.3973,  19.8770,  26.7719,  52.0735,  36.3763,\n",
       "         43.4007,  32.5129,  40.3941,  40.7739,  45.8651,  42.6047,  42.3146,\n",
       "         31.1757,  39.4736,  60.6086,  48.3106,  59.5185,  37.6708,  47.7744,\n",
       "         59.8849,  42.9379,  45.0178,  47.5458,  49.9977,  50.6089,  61.2814,\n",
       "         62.2930,  41.8670,  51.9778,  64.2337,  73.0912,  89.1251,  53.0376,\n",
       "         62.0565,  59.4844,  62.6214,  74.2905,  53.4048,  79.1301,  50.6076,\n",
       "         59.9209,  72.2833,  65.6623,  65.9256,  78.1246,  66.3762,  82.8123,\n",
       "         69.5327,  80.7689,  73.9994,  88.2132,  79.3049,  77.4547,  69.6615,\n",
       "         85.1014,  92.1332,  86.5644,  88.2861,  86.2726, 103.2327,  76.4259,\n",
       "         70.0035, 103.9740,  99.8293,  86.1888, 105.3737,  93.1739, 102.3351,\n",
       "        100.9761,  89.9434])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(1,100, 100).type(torch.FloatTensor)\n",
    "\n",
    "rand = torch.randn(100) * 10\n",
    "\n",
    "y = x + rand\n",
    "\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "将生成的原始数据点画在图上，用视觉观察下数据点的“趋势”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt #导入画图的程序包\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,8)) #设定绘制窗口大小为10*8 inch\n",
    "# 绘制数据，考虑到x和y都是Variable，\n",
    "# 需要用data获取它们包裹的Tensor，并专成numpy\n",
    "plt.plot(x.data.numpy(), y.data.numpy(), 'o') \n",
    "plt.xlabel('X') #添加X轴的标注\n",
    "plt.ylabel('Y') #添加Y周的标注\n",
    "plt.show() #将图形画在下面\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9f832687c0>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAHSCAYAAAAwpbX/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmEklEQVR4nO3db4xd6X0f9u+v3LU9imuMFa2U3ZGU3aALqrLYhPHAVcsi0J8IVBLBS9C1KyNONrWKRQE3ttOGMWm/CPpCJYENghhIXGDhOFEQwbJqbSghSrx2RQkJhFjO0Ayy1h9Wgi1LO7vRMpHZFC1hr9ZPX8xwl5ydmZ2Ze+4959zz+QDC8J47M/fRHC7vd57n9/yeaq0FAIDu/Cd9DwAAYNkIWAAAHROwAAA6JmABAHRMwAIA6JiABQDQsXv6HsCdXve617UHH3yw72EAALyqq1ev/vvW2n27PTeogPXggw9mY2Oj72EAALyqqvrdvZ6zRAgA0DEBCwCgYwIWAEDHBCwAgI4JWAAAHROwAAA6JmABAHRMwAIA6JiABQDQMQELAKBjAhYAQMcELACAjglYAAAdE7AAADomYAEAdOyevgcAAEzT5Wubefyp63n25q08sLqSc6eP58zJtb6H1QkBCwBYuMvXNnPhyadz64UXkySbN2/lwpNPJ8lShCxLhADAwj3+1PWXwtVtt154MY8/db2nEXVLwAIAFu7Zm7cOdX1sLBECAEd21DqqB1ZXsrlLmHpgdWUew1w4M1gAwJHcrqPavHkrLS/XUV2+tvmqX3vu9PGs3Hvsrmsr9x7LudPH5zTaxRKwAIAjmaWO6szJtVw8eyJrqyupJGurK7l49sRSFLgnlggBYPKOusw3ax3VmZNrSxOodjKDBQATNssy3171UstSRzULAQsAJmyWZb5F1lFdvraZU5eu5KHzn8ypS1cOFAD7ZIkQACZslmW+28t78+7GPsampAIWAEzYrO0SFlFHtd8s21ADliVCAJiwMbRLGGNTUgELACZsDO0SxlhMX621vsfwkvX19baxsdH3MACAAdlZg5VszbL9wPeu5dNfujHX+q/9VNXV1tr6bs+pwQIABm23Yvp3vuW+fOzq5mAL3w8csKrqF5K8L8nzrbW3bV97bZJfSvJgkq8m+aHW2u9tP3chyQeSvJjkx1trT3U6cgBgMnYW05+6dGXQhe+HqcH6h0neu+Pa+SSfaq09nORT249TVW9N8v4k37P9NT9XVccCANCBoRe+Hzhgtdb+RZJv7rj8SJIPbf/5Q0nO3HH9I62132+t/U6SryT5vtmGCgCMwSKagg698H3WXYRvaK09lyTbH1+/fX0tydfv+Lxntq8BAEtslqN3DmPo7SXm1aahdrm263bFqnqsqjaqauPGjRtzGg4AsAizHL1zGENvLzHrLsJvVNX9rbXnqur+JM9vX38myZvu+Lw3Jnl2t2/QWnsiyRPJVpuGGccDAPRokbVRi+gif1SzzmB9Ismj239+NMnH77j+/qr69qp6KMnDSX5jxtcCAAZu6LVRi3LggFVVv5jkXyU5XlXPVNUHklxK8p6q+nKS92w/Tmvt80k+muQLSX4lyY+11l7c/TsDAMti6LVRi3LgJcLW2g/v8dS79/j8Dyb54FEGBQCM025NQRfdYX0IdHIHADo15NqoRXHYMwBAxwQsAICOCVgAAB0TsAAAOiZgAQB0TMACAOiYgAUA0DEBCwCgYwIWAEDHBCwAgI4JWAAAHROwAAA6JmABAHTsnr4HAABwp8vXNvP4U9fz7M1beWB1JedOH8+Zk2t9D+tQBCwAYDAuX9vMhSefzq0XXkySbN68lQtPPp0kowpZlggBgMF4/KnrL4Wr22698GIef+p6TyM6GjNYADAiy7B8tp9nb9461PWhMoMFACNxe/ls8+attLy8fHb52mbfQ+vMA6srh7o+VAIWAIzEsiyf7efc6eNZuffYXddW7j2Wc6eP9zSio7FECAAjsSzLZ/u5vdw59mVQAQsARuKB1ZVs7hKmxrZ89mrOnFwbXaDayRIhAIzEsiyfTYEZLAAYiWVZPpsCAQsARmQZls+mQMACAJbGUPqECVgAwFIY0jE7AhbAEhjKb+3Qp/36hAlYABzKkH5rhz4NqU+YNg0AIzeF7t5wEEM6ZscMFsDIDem39qkb0lLtkMayKOdOH79rNjfpr0+YgAUwclPp7j10Q1qqHdJYFmlIfcIELICRG9Jv7VM2pALrIY1l0YbSJ0zAAhi5If3WPmVDWqod0limSsACWAJD+a19yoa0VDuksUyVXYQA0IE+D2K+fG0zpy5dyUPnP5lTl67knW+5z6HQPTODBQAd6GupdreC9o9d3cwPfO9aPv2lG68YyxR3F/ZBwAKAjvSxVLtXQfunv3Qjnz3/rruu97m7cGrBzhIhAIzYYQra+2pKezvYbd68lZaXg93la5tzfd0+CVgAMGKH6V7e1+7CKZ42IGABwEDtLF7fbcbnMMX1fR0lM8W2EQIWAAzQQZfVzpxcy8WzJ7K2upJKsra6kotnT+xa39TXTschnRG4KIrcAWCADtON/aDF9X3tdJziaQMCFgAM0LyW1frY6TjF0wYELAAYoGXrxj610wbUYAHAAPXZGZ7ZmcECgAGa4rLaMhGwAGCgprastkwELAAmaWpHt7BYAhYAk9PnmXxMgyJ3ACZnike3sFgCFgCTM8WjW1gsAQuAyZni0S0sloAFwOToMcW8KXIHYHIO02PKbkOOQsACYJIO0mPKbkOOqpMlwqr6a1X1+ar6rar6xar6jqp6bVX9WlV9efvjd3fxWgCwKHYbclQzB6yqWkvy40nWW2tvS3IsyfuTnE/yqdbaw0k+tf0YAEbDbkOOqqsi93uSrFTVPUlek+TZJI8k+dD28x9Kcqaj1wKAhbDbkKOauQartbZZVX8rydeS3Eryq621X62qN7TWntv+nOeq6vWzvhYA47AsheHnTh+/qwYrOfxuw2X5WXA4Mwes7dqqR5I8lORmkv+jqn7kEF//WJLHkuTNb37zrMMBoGfLVBh+mN2Gu1mmnwWHU6212b5B1Q8meW9r7QPbj/9ykrcneXeSd2zPXt2f5DOttX0j//r6etvY2JhpPAD069SlK9ncpUZpbXUlnz3/rh5G1B8/i+VWVVdba+u7PddFDdbXkry9ql5TVZWtYPXFJJ9I8uj25zya5OMdvBYAA6cw/GV+FtPVRQ3W56rql5P8ZpJvJbmW5Ikk35nko1X1gWyFsB+c9bUAGL4HVld2nbVZVGH4kGqe+v5Z0J9OdhG21v5ma+0trbW3tdb+Umvt91tr/6G19u7W2sPbH7/ZxWsBMGx9HkNzu+Zp8+attLxc83T52ubcX3s3juSZLp3cAejUrIXhs9ivMWgfs1h7/SySrfqsIcyyMR8CFgCdO8gxNPMwxJqnnT8LOwunoatGowDQuzE0BnX8zjQIWAAsjTHUPA1xlm03l69t5tSlK3no/Cdz6tKV3urYxsoSIQBLo8/6r4Maw85Cy5izE7AAWCp91X8dVBfH78zb0DYLjJGABQALNIZZtrEsYw6ZgAUACzb0WbYxLGMOnSJ3AOAuY9gsMHRmsACAu4xhGXPoBCwADmxI5/wxX0Nfxhw6AQuAA7F1/2WCJq9GDRYAB6ID+ZahHSjNMAlYAByIrftbBE0OwhIhwMj0tTxl6/4WQZODMIMFMCJ9Lk/Zur9lDAdK0z8BC2BE+lyeOnNyLRfPnsja6koqydrqSi6ePTG54m5Bk4OwRAgwIn0vTy1i6/7Qd+jpEcVBCFgAI7LsdVBjaQWhRxSvxhIhwIgs+/KUHXosCzNYACOy7MtTfS+BQlcELICRWeblqWVfAmU6LBECMBjLvgTKdJjBAmAwln0JlOkQsAAYlGVeAh27obfQGBIBCwC29Rkghh5extJCYyjUYAFA+j2GqM/XPigtNA5HwAKA9BsgxhBetNA4HAELANJvgBhDeHHI9eEIWACQfgPEGMKLFhqHI2ABQPoNEGMIL2dOruXi2RNZW11JJVlbXcnFsycUuO/BLkIASL89uMbS/0sLjYOr1lrfY3jJ+vp629jY6HsYAACvqqquttbWd3vOEiEAQMcELACAjglYAAAdU+QOwNIb+jE0LB8BC4DROkhwcoYefRCwABiFnWHqnW+5Lx+7uvmqwWm/Y2gELOZFDRYAg7fbYcgf/vWvHej8vjEcQ8PyEbAAGLzdZqH26uK4MziN4Rgalo+ABcDgHWa2aWdwGsMxNCwfAQuAwdtrtql2PN4tODlDjz4ocgcYMO0Ftpw7ffyunYDJVpj6ge9dy6e/dONVfz7O0GPRBCyAgdJe4GVjOQwZbhOwAAZKe4G7mYViTNRgAQyU9gIwXgIWwEBpLwDjJWABDFTf7QUuX9vMqUtX8tD5T+bUpSu5fG1zIa8Ly0ANFsCcHXUnYJ+F3QrsYTYCFsAczRpUui7sPmjYU2APs7FECDBH+wWVRdvtPL8LTz6969KfAnuYjRksgA7tnCHaHFBQOcys1F5jV2APB2MGC6Aju80Q7TzK5bY+gsphZqX6LrCHsTODBSzMsh/7stsMUcvWeXntjmt9BZXDzErpnA6z6SRgVdVqkp9P8rZs/Tvyo0muJ/mlJA8m+WqSH2qt/V4XrweMzxR2pe01Q9SydcBw30Flr/P89gp7OqfD0XU1g/WzSX6ltfbfVtW3JXlNkp9O8qnW2qWqOp/kfJKf6uj1gJGZwq60vWaI1lZX8tnz7+phRHcb4qzUss9qMl0zB6yq+q4kfybJX0mS1tofJPmDqnokyTu2P+1DST4TAQsma5G70vp60z7sDFEfhjQrNYVZTaarixmsP5HkRpJ/UFV/MsnVJD+R5A2tteeSpLX2XFW9voPXAkbqMPU/swSkPt+0hzhDdFB9hNIpzGoyXV0ErHuS/Okkf7W19rmq+tlsLQceSFU9luSxJHnzm9/cwXCAITro7M6sAanvN+0hzRAdVF+hVK8tllkXbRqeSfJMa+1z249/OVuB6xtVdX+SbH98frcvbq090Vpbb62t33fffR0MBxiiMyfXcvHsiaytrqSyVZd08eyJV7yBz9qY05v24fXVDNVh1iyzmWewWmv/rqq+XlXHW2vXk7w7yRe2//dokkvbHz8+62sB43aQ2Z1ZA5IGmYfXVygdQ80aHFVXjUb/apIPV9W/TfKnkvxv2QpW76mqLyd5z/ZjgH3NOquhQebh9TWTdNBZTRijTto0tNb+TZL1XZ56dxffH5iOWWc1xlxo3pc+Z5LGWLMGB6GTOzAoXQQkb9qHI5RC96q19uqftSDr6+ttY2Oj72EAALyqqrraWtttBc9hzwAAXROwAAA6JmABAHRMwAIA6JiABQDQMQELAKBjAhYAQMcELACAjglYAAAdE7AAADomYAEAdMxhzwDM5PK1TQdFww4CFgBHdvnaZi48+XRuvfBikmTz5q1cePLpJBGymDQBC2BJLWJm6fGnrr8Urm679cKLefyp6wIWkyZgASyhRc0sPXvz1qGuw1QocgdYQvvNLHXpgdWVQ12HqTCDBQOiWHjaurz/i5pZOnf6+F0zZUmycu+xnDt9vNPXgbERsGAgFAtPW9f3/4HVlWzuEqa6nlm6PTa/GMDdBCwYCMXC09b1/V/kzNKZk2v+jsIOAhYMhGLhaev6/ptZgn4JWDAQi1rSYZjmcf/NLEF/7CKEgTh3+nhW7j121zXFwsN1+dpmTl26kofOfzKnLl3J5WubM32/Kdz/rn9mMGRmsGAgLOkM187dfe98y3352NXNTjckLPv9t4mDqanWWt9jeMn6+nrb2NjoexgAL9kZDJKkkuz2L+fa6ko+e/5dCxvbmJy6dGXXJVA/M8asqq621tZ3e84SIcA+dtvdt9evpTYk7M0mDqZGwALYx2ECgA0Je9PxnakRsAD2sVcAqB2Pl60gvWtTKOKHOwlYAPvYKxj8xbe/OWurK6ls1RFdPHtCsfY+zpxcy8WzJ/zMmAy7CIFeDf38xWXf3bdI+nIxJQIW0JuxbN0XDIDDErCA3nRx/l7XM2BDn1EDxkHAAnoz69b9rmfAxjKjBgyfInegN7Nu3d9vBuwouv5+wHQJWEBvZt2633XzSs0wga4IWEBvZt2633XzSs0wga6owQJ6NcsOvXOnj7/inMBZmld2/f2A6RKwgCTj3D3XdY8qPa+ArlRrex1bunjr6+ttY2Oj72HA5OzcPZdszdzotA2wt6q62lpb3+05NViA3XMAHbNECEx299wYl0WBcTCDBUxy99ztZdHNm7fS8nJT0cvXNvseGrAEBCxg5n5UY2RZFJgnS4TAJHfPTXVZFFgMAQtIMls/qjF6YHUlm7uEqWVeFgUWxxIhjNDla5s5delKHjr/yZy6dEXd0BFMcVkUWBwBC0ZGcfbR7AylSWY6pgdgP5YIYWT2K87uOhwsSxuDnY1Ub4fSi2dP5LPn39Xz6IBlJGDByCyqOHuvULLxu9/Mp790Y1Sha5GhFCCxRAijs6ieVXuFkg//+tdGtzxpxyCwaAIWjMyiirP3Ch87Ty8dQ++oITZStVEBlpuABSNz5uTaQoqzDxM+hj4TNLQdgzYqwPJTgwUjtIieVedOH7+rBitJKq+cwUqG3ztqaI1U1YTB8hOwYGIOujNwt1Dyzrfcl49d3bwrHIyld9SQGqmqCYPlJ2DBEtsZpnYGpNtLU0n2DFk7r6//8dcOZiZorHSRh+VXre024X+Eb1R1LMlGks3W2vuq6rVJfinJg0m+muSHWmu/t9/3WF9fbxsbG52MB6ZuZ5uFZO8lvrXVFf2gFmi3e7Ny7zGNTmFkqupqa219t+e6LHL/iSRfvOPx+SSfaq09nORT24+BBdmtzmevX6csTS3WojYqAP3pZImwqt6Y5C8k+WCS/3n78iNJ3rH95w8l+UySn+ri9YBXd5jQNI+lqWXpAj8vQ6oJA7rX1QzW30nyN5L84R3X3tBaey5Jtj++vqPXAg5gr9BUOx7Po0hdGwJg6mYOWFX1viTPt9auHvHrH6uqjarauHHjxqzDAbbt1fvpL779zXNfmtqvDQHAFHSxRHgqyfdX1Z9P8h1Jvquq/nGSb1TV/a2156rq/iTP7/bFrbUnkjyRbBW5dzAeIP32ftKGAJi6mQNWa+1CkgtJUlXvSPLXW2s/UlWPJ3k0yaXtjx+f9bWAw+mrzkcbAmDq5nlUzqUk76mqLyd5z/ZjjsCZZYzN0I6mAVi0ThuNttY+k63dgmmt/Yck7+7y+0/Rzn45r9YYEoZgXsuTdiYCY6GT+8A5s4yx6np50i8bwJjMc4mQDigWhi12JgJjImAN3F5FwYqFmRq/bABjImANnGJhbHLY4pcNYEwErIFzZtm06Yj+Mr9sAGOiyH0EnFk2XTY5vKzPxqkAhyVgwYCpO7qbXzaAsRCwoENd92nSER1gnNRgQUfmUS+l7ghgnAQs6Mg8+jTZ5AAwTpYIoSPzqpdSdwQwPmawoCP6NAFwm4AFHVEvBcBtlgihI/o0AXCbgAUdUi8FQGKJEACgc2awYEl03eQUgKMTsOidYDC7201Ob/fhut3kNImfJUAPLBHSq3l0P5+ieTQ5BeDoBCx6JRh0w6HQAMNiiZBeCQbd6PtQaMu8AHczg0WvdD/vRp9NTi3zArySgEWvdD/vRp+HQlvmBXglS4T0Svfz7vTV5NQyL8ArCVj0Tvfzceu7/gtgiCwRAjOxzAvwSmawgJlY5gV4JQELDkAbgv1Z5gW4m4DF6M07/DiGBoDDUoPFqC2iB5M2BAAcloDFqC0i/GhDAMBhWSKkE33VKC0i/GhDAMBhmcFiZn0elbKIo3a0IQDgsAQsZtZnjdIiwk+fx9AAME6WCBdg2bf491mjtKgeTPNoQ7Dsfy8ApkzAmrMpbPHvu0ZpjD2YpvD3AmDKLBHO2RS2+KtROrwp/L0AmDIzWHM2hS3+jko5vCn8vQCYMgFrzvpePluUMS7T9Wkqfy8ApsoS4ZxZPmM3/l4ALDczWHNm+Yzd+HsBsNyqtdb3GF6yvr7eNjY2+h4GAMCrqqqrrbX13Z4zgwVHpI8VAHsRsOAI9LECYD+K3OEI9LECYD8CFhyBPlYA7EfAgiPYq1+VPlYAJAIWHIk+VgDsR5E7HIE+VgDsR8CCHQ7afsHxQADsRcDqkT5Kw6P9AgBdELB64o188Q4SaPdrv+C+AHBQitx7oo/SYt0OtJs3b6Xl5UB7+drmXZ+n/QIAXRCweuKNfLEOGmi1XwCgCwJWT7yRL9ZBA632CwB0QcDqiTfyxTpooD1zci0Xz57I2upKKsna6kounj2h/gqAQ5m5yL2q3pTkHyX5Y0n+MMkTrbWfrarXJvmlJA8m+WqSH2qt/d6sr7cIi9jdp4/SYp07ffyuTQXJ3oFW+wUAZlWttdm+QdX9Se5vrf1mVf2nSa4mOZPkryT5ZmvtUlWdT/LdrbWf2u97ra+vt42NjZnGM6udu/uSrTdisxjj12dbDC05AJZPVV1tra3v+tysAWuXF/t4kr+7/b93tNae2w5hn2mt7bv+NYSAderSlWzuUq+ztrqSz55/Vw8j6pdgMDuhHWA57RewOq3BqqoHk5xM8rkkb2itPZck2x9fv8fXPFZVG1W1cePGjS6HcyR2973soK0N2J+WHADT01nAqqrvTPKxJD/ZWvuPB/261toTrbX11tr6fffd19VwjszuvpcJBt0Q2gGmp5OAVVX3Zitcfbi19uT25W9sLw3ertN6vovXmje7+14mGHRDaAeYnpkDVlVVkr+f5Iuttb99x1OfSPLo9p8fTfLxWV9rEWzTf5lg0A2hHWB6ujiL8FSSv5Tk6ar6N9vXfjrJpSQfraoPJPlakh/s4LUWwjb9LYdpbcDetOQAmJ7OdxHOYgi7CMfsoDv+DrMz0C5CANjdfrsIu5jBYgB2tgK4veMvyV2B6KCfd5vZPAA4PEflLImD7vizMxAA5s8M1sAcdUnuoDv+7AwEgPkzgzUgszT2POiOPzsDAWD+BKwBmWX57qCtAKbSMuDytc2cunQlD53/ZE5duqL7PAALZYlwQGZZvjtoK4AptAw4bCE/AHRNwBqQB1ZXdj1o+qDLdwfd8bfsOwP3mwlc5v/fAAyHJcIBmcry3bwdZibQUiIA82AGa0CmsHy3CAedCbSUCMC8CFgD09fy3TJ1dz/oET+WEgGYFwGLQ83kjGHW56AzgXqCATAvAhaHmskZy6zPQWYCZ91UAAB7UeTOoWZylmnWx6YCAOZFwOJQ3d2XqRP8mZNruXj2RNZWV1JJ1lZXcvHsiUHNxAEwTpYIOXBR+GE/d+jF8Mny9wQDoB+TClhjeMPvw2HaQxz0c8dQDA8A81Kttb7H8JL19fW2sbExl++98w0/2Zp5sSQ0H6cuXdm1gHxtdSWfPf+uHkYEAN2qqquttfXdnptMDdYsBylzeMtUDA8AhzWZgOUNf7GWqRgeAA5rMgHLG/5iaYEAwJRNJmB5w18sLRAAmLLJ7CJ0kPLizdICwY5PAMZsMgEr0fNoLLR4AGDsJhWwFsHMy+zGct4hAOxFwOqQmZdu2PEJwNhNpsh9EfTa6oYdnwCMnYDVITMv3bDjE4CxE7A6ZOalG1o8ADB2arA6dO708V3POzTzcnh2fAIwZgJWh/TaAgASAatzZl4AADVYAAAdE7AAADomYAEAdEzAAgDomIAFANAxuwgZDQdpAzAWAhaj4CBtAMZEwJqBGZXF2e8gbT9zAIZGwDoiMyqL5SBtAMZEkfsR7TejQvccpA3AmAhYR2RGZbHOnT6elXuP3XXNQdoADJWAdURmVBbrzMm1XDx7ImurK6kka6sruXj2hOVYAAZJDdYRnTt9/K4arMSMyrw5SBuAsRCwjuj2G71dhADATgLWDMyoAAC7UYMFANAxAQsAoGMCFgBAxwQsAICOCVgAAB0TsAAAOiZgAQB0TMACAOiYgAUA0LG5B6yqem9VXa+qr1TV+Xm/HgBA3+YasKrqWJK/l+TPJXlrkh+uqrfO8zUBAPo277MIvy/JV1prv50kVfWRJI8k+cKcX7dzl69tOtgZADiQeQestSRfv+PxM0n+yzm/ZucuX9vMhSefzq0XXkySbN68lQtPPp0kQhYA8ArzrsGqXa61uz6h6rGq2qiqjRs3bsx5OEfz+FPXXwpXt9164cU8/tT1nkYEAAzZvAPWM0nedMfjNyZ59s5PaK090Vpbb62t33fffXMeztE8e/PWoa4DANM274D1r5M8XFUPVdW3JXl/kk/M+TU798DqyqGuAwDTNteA1Vr7VpL/KclTSb6Y5KOttc/P8zXn4dzp41m599hd11buPZZzp4/3NCIAYMjmXeSe1to/S/LP5v0683S7kN0uQgDgIOYesJbFmZNrAhUAcCCOygEA6JiABQDQMQELAKBjAhYAQMcELACAjglYAAAdE7AAADomYAEAdEzAAgDomIAFANAxAQsAoGMCFgBAxwQsAICO3dP3AJiWy9c28/hT1/PszVt5YHUl504fz5mTa30PCwA6JWCN1BiDyuVrm7nw5NO59cKLSZLNm7dy4cmnk2TwYweAw7BEOEK3g8rmzVtpeTmoXL622ffQ9vX4U9dfCle33XrhxTz+1PWeRgQA8yFgjdBYg8qzN28d6joAjJWANUJjDSoPrK4c6joAjJWANUJjDSrnTh/Pyr3H7rq2cu+xnDt9vKcRAcB8CFgjNNagcubkWi6ePZG11ZVUkrXVlVw8e0KBOwBLxy7CEbodSMa2izDZGvsYxgkAsxCwRkpQAYDhskQIANAxAQsAoGMCFgBAxwQsAICOCVgAAB0TsAAAOiZgAQB0TMACAOiYgAUA0DEBCwCgYwIWAEDHBCwAgI4JWAAAHROwAAA6JmABAHRMwAIA6JiABQDQMQELAKBj9/Q9gL5dvraZx5+6nmdv3soDqys5d/p4zpxc63tYAMCITTpgXb62mQtPPp1bL7yYJNm8eSsXnnw6SYQsAODIJr1E+PhT118KV7fdeuHFPP7U9Z5GBAAsg0kHrGdv3jrUdQCAg5h0wHpgdeVQ1wEADmLSAevc6eNZuffYXddW7j2Wc6eP9zQiAGAZTLrI/XYh+zLvIrRLEgAWb9IBK9kKWcsaOOySBIB+THqJcNnZJQkA/RCwlphdkgDQDwFridklCQD9ELCWmF2SANCPyRe5L7Mp7JIEgCESsJbcMu+SBIChskQIANCxmQJWVT1eVV+qqn9bVf+kqlbveO5CVX2lqq5X1emZRwoAMBKzzmD9WpK3tdb+iyT/V5ILSVJVb03y/iTfk+S9SX6uqo7t+V0AAJbITAGrtfarrbVvbT/89SRv3P7zI0k+0lr7/dba7yT5SpLvm+W1AADGossarB9N8s+3/7yW5Ot3PPfM9rVXqKrHqmqjqjZu3LjR4XAAAPrxqrsIq+r/TPLHdnnqZ1prH9/+nJ9J8q0kH779Zbt8ftvt+7fWnkjyRJKsr6/v+jkAAGPyqgGrtfZn93u+qh5N8r4k726t3Q5IzyR50x2f9sYkzx51kAAAYzLrLsL3JvmpJN/fWvv/7njqE0neX1XfXlUPJXk4yW/M8loAAGMxa6PRv5vk25P8WlUlya+31v7H1trnq+qjSb6QraXDH2utvTjjawEAjMJMAau19p/t89wHk3xwlu8PADBGOrkDAHRMwAIA6JiABQDQMQELAKBjAhYAQMfq5d6g/auqG0l+t+Nv+7ok/77j78ns3Jfhcm+GyX0ZJvdluBZxb/54a+2+3Z4YVMCah6raaK2t9z0O7ua+DJd7M0zuyzC5L8PV972xRAgA0DEBCwCgY1MIWE/0PQB25b4Ml3szTO7LMLkvw9XrvVn6GiwAgEWbwgwWAMBCLW3Aqqr3VtX1qvpKVZ3vezxTVVVvqqpPV9UXq+rzVfUT29dfW1W/VlVf3v743X2Pdaqq6lhVXauqf7r92L3pWVWtVtUvV9WXtv/b+a/cl2Goqr+2/W/Zb1XVL1bVd7g3/aiqX6iq56vqt+64tue9qKoL25ngelWdnvf4ljJgVdWxJH8vyZ9L8tYkP1xVb+13VJP1rST/S2vtP0/y9iQ/tn0vzif5VGvt4SSf2n5MP34iyRfveOze9O9nk/xKa+0tSf5ktu6P+9KzqlpL8uNJ1ltrb0tyLMn749705R8mee+Oa7vei+33nfcn+Z7tr/m57awwN0sZsJJ8X5KvtNZ+u7X2B0k+kuSRnsc0Sa2151prv7n95/8nW28Ua9m6Hx/a/rQPJTnTywAnrqremOQvJPn5Oy67Nz2qqu9K8meS/P0kaa39QWvtZtyXobgnyUpV3ZPkNUmejXvTi9bav0jyzR2X97oXjyT5SGvt91trv5PkK9nKCnOzrAFrLcnX73j8zPY1elRVDyY5meRzSd7QWnsu2QphSV7f49Cm7O8k+RtJ/vCOa+5Nv/5EkhtJ/sH20u3PV9UfifvSu9baZpK/leRrSZ5L8n+31n417s2Q7HUvFp4LljVg1S7XbJfsUVV9Z5KPJfnJ1tp/7Hs8JFX1viTPt9au9j0W7nJPkj+d5H9vrZ1M8v/GktMgbNfzPJLkoSQPJPkjVfUj/Y6KA1p4LljWgPVMkjfd8fiN2ZrGpQdVdW+2wtWHW2tPbl/+RlXdv/38/Ume72t8E3YqyfdX1VeztYz+rqr6x3Fv+vZMkmdaa5/bfvzL2Qpc7kv//myS32mt3WitvZDkyST/ddybIdnrXiw8FyxrwPrXSR6uqoeq6tuyVdj2iZ7HNElVVdmqJflia+1v3/HUJ5I8uv3nR5N8fNFjm7rW2oXW2htbaw9m67+RK621H4l706vW2r9L8vWqOr596d1JvhD3ZQi+luTtVfWa7X/b3p2tulL3Zjj2uhefSPL+qvr2qnooycNJfmOeA1naRqNV9eezVV9yLMkvtNY+2O+Ipqmq/psk/zLJ03m5zuens1WH9dEkb87WP1o/2FrbWazIglTVO5L89dba+6rqj8a96VVV/alsbTz4tiS/neS/z9YvxO5Lz6rqf03y32Vrh/S1JP9Dku+Me7NwVfWLSd6R5HVJvpHkbya5nD3uRVX9TJIfzda9+8nW2j+f6/iWNWABAPRlWZcIAQB6I2ABAHRMwAIA6JiABQDQMQELAKBjAhYAQMcELACAjglYAAAd+/8BUTyCIC1bIQQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(x.data.numpy(), y.data.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "所有的数据点以视觉可见的增势增长。下面我们构造模型，来拟合这些数据点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.2 构造模型 计算损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "我们要使用一条直线去拟合若干个点的走势，那在数学上怎么表示这条直线哪？\n",
    "\n",
    "这个大家在高中应该都学过，可以用 `ax+b` 来表示一条直线。\n",
    "\n",
    "因为这条直线是由参数 `a` 和 `b` 控制的，所以模型就是要“学习”出这两个参数。\n",
    "\n",
    "那么下面首先建立变量，随机初始化用于线性拟合的参数 `a` 和 `b`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "#创建a变量，并随机赋值初始化\n",
    "a = Variable(torch.rand(1), requires_grad = True) \n",
    "#创建b变量，并随机赋值初始化\n",
    "b = Variable(torch.rand(1), requires_grad = True) \n",
    "print('Initial parameters:', [a, b])\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init parameters:  [tensor([0.6073], requires_grad=True), tensor([0.3048], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad = True)\n",
    "\n",
    "b = torch.rand(1, requires_grad = True)\n",
    "\n",
    "print('init parameters: ', [a, b])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在当前的模型中，这两个参数的初始值无关紧要，因为下面会通过 1000 次的训练，来反复修正这两个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在下面的代码中，需要注意 `expand_as` 和 `mul` 的使用。\n",
    "\n",
    "首先，a 的维度为 1，x 是维度为 100\\*1 的 Tensor，这两者不能直接相乘，因为维度不同。\n",
    "\n",
    "所以，先要将 a 升维成 1\\*1 的 Tensor。\n",
    "\n",
    "这就好比将原本在直线上的点被升维到了二维平面上，同时直线仍然在二维平面中。\n",
    "\n",
    "```expand_as(x)``` 可以将张量升维成与 x 同维度的张量。所以如果 a = 1, x 为尺寸为 100，\n",
    "\n",
    "那么，a.expand_as(x)$ = (1, 1, \\cdot\\cdot\\cdot, 1)^T$\n",
    "\n",
    "```x * y``` 为两个 1 维张量的乘积，计算结果：\n",
    "\n",
    "$(x * y)_i = x_i \\cdot y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "learning_rate = 0.0001 #设置学习率\n",
    "for i in range(1000):\n",
    "    ### 下面这三行代码非常重要，这部分代码，清空存储在变量a，b中的梯度信息，\n",
    "    ### 以免在backward的过程中会反复不停地累加\n",
    "    #如果a和b的梯度都不是空\n",
    "    if (a.grad is not None) and (b.grad is not None):  \n",
    "        a.grad.data.zero_() #清空a的数值\n",
    "        b.grad.data.zero_() #清空b的数值\n",
    "    #计算在当前a、b条件下的模型预测数值\n",
    "    predictions = a.expand_as(x) * x + b.expand_as(x)  \n",
    "    #通过与标签数据y比较，计算误差\n",
    "    loss = torch.mean((predictions - y) ** 2) \n",
    "    print('loss:', loss.data.numpy())\n",
    "    loss.backward() #对损失函数进行梯度反传\n",
    "    #利用上一步计算中得到的a的梯度信息更新a中的data数值\n",
    "    a.data.add_(- learning_rate * a.grad.data)\n",
    "    #利用上一步计算中得到的b的梯度信息更新b中的data数值\n",
    "    b.data.add_(- learning_rate * b.grad.data)  \n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  562.79346\n",
      "loss:  156.9168\n",
      "loss:  114.53298\n",
      "loss:  110.10701\n",
      "loss:  109.64481\n",
      "loss:  109.59654\n",
      "loss:  109.59151\n",
      "loss:  109.59097\n",
      "loss:  109.590935\n",
      "loss:  109.59091\n",
      "loss:  109.5909\n",
      "loss:  109.59091\n",
      "loss:  109.59091\n",
      "loss:  109.59092\n",
      "loss:  109.59092\n",
      "loss:  109.5909\n",
      "loss:  109.5909\n",
      "loss:  109.5909\n",
      "loss:  109.5909\n",
      "loss:  109.5909\n",
      "loss:  109.59089\n",
      "loss:  109.59089\n",
      "loss:  109.5909\n",
      "loss:  109.59088\n",
      "loss:  109.59088\n",
      "loss:  109.59089\n",
      "loss:  109.59088\n",
      "loss:  109.59088\n",
      "loss:  109.590866\n",
      "loss:  109.59088\n",
      "loss:  109.590866\n",
      "loss:  109.590866\n",
      "loss:  109.590866\n",
      "loss:  109.590866\n",
      "loss:  109.590866\n",
      "loss:  109.59086\n",
      "loss:  109.59086\n",
      "loss:  109.59085\n",
      "loss:  109.59086\n",
      "loss:  109.59085\n",
      "loss:  109.59085\n",
      "loss:  109.59085\n",
      "loss:  109.59084\n",
      "loss:  109.59084\n",
      "loss:  109.59084\n",
      "loss:  109.59084\n",
      "loss:  109.59083\n",
      "loss:  109.59082\n",
      "loss:  109.59082\n",
      "loss:  109.59083\n",
      "loss:  109.59083\n",
      "loss:  109.59082\n",
      "loss:  109.59082\n",
      "loss:  109.59081\n",
      "loss:  109.59082\n",
      "loss:  109.5908\n",
      "loss:  109.59082\n",
      "loss:  109.5908\n",
      "loss:  109.59081\n",
      "loss:  109.5908\n",
      "loss:  109.5908\n",
      "loss:  109.59079\n",
      "loss:  109.5908\n",
      "loss:  109.59079\n",
      "loss:  109.59079\n",
      "loss:  109.5908\n",
      "loss:  109.59079\n",
      "loss:  109.59078\n",
      "loss:  109.59078\n",
      "loss:  109.59079\n",
      "loss:  109.59078\n",
      "loss:  109.590775\n",
      "loss:  109.59076\n",
      "loss:  109.59078\n",
      "loss:  109.590775\n",
      "loss:  109.59076\n",
      "loss:  109.590775\n",
      "loss:  109.590775\n",
      "loss:  109.59076\n",
      "loss:  109.59076\n",
      "loss:  109.590775\n",
      "loss:  109.59076\n",
      "loss:  109.59075\n",
      "loss:  109.59075\n",
      "loss:  109.59075\n",
      "loss:  109.590744\n",
      "loss:  109.590744\n",
      "loss:  109.59072\n",
      "loss:  109.59073\n",
      "loss:  109.59073\n",
      "loss:  109.590744\n",
      "loss:  109.59072\n",
      "loss:  109.59072\n",
      "loss:  109.59072\n",
      "loss:  109.59072\n",
      "loss:  109.590706\n",
      "loss:  109.59072\n",
      "loss:  109.59071\n",
      "loss:  109.590706\n",
      "loss:  109.590706\n",
      "loss:  109.59071\n",
      "loss:  109.590706\n",
      "loss:  109.590706\n",
      "loss:  109.590706\n",
      "loss:  109.59069\n",
      "loss:  109.590706\n",
      "loss:  109.59069\n",
      "loss:  109.590706\n",
      "loss:  109.59069\n",
      "loss:  109.59069\n",
      "loss:  109.59069\n",
      "loss:  109.59068\n",
      "loss:  109.590675\n",
      "loss:  109.590675\n",
      "loss:  109.590675\n",
      "loss:  109.590675\n",
      "loss:  109.59068\n",
      "loss:  109.59068\n",
      "loss:  109.59068\n",
      "loss:  109.59067\n",
      "loss:  109.59065\n",
      "loss:  109.59067\n",
      "loss:  109.590675\n",
      "loss:  109.59065\n",
      "loss:  109.59067\n",
      "loss:  109.59065\n",
      "loss:  109.59065\n",
      "loss:  109.59067\n",
      "loss:  109.590645\n",
      "loss:  109.59065\n",
      "loss:  109.590645\n",
      "loss:  109.59064\n",
      "loss:  109.590645\n",
      "loss:  109.590645\n",
      "loss:  109.590645\n",
      "loss:  109.59064\n",
      "loss:  109.59064\n",
      "loss:  109.59064\n",
      "loss:  109.59062\n",
      "loss:  109.590614\n",
      "loss:  109.590614\n",
      "loss:  109.59062\n",
      "loss:  109.59062\n",
      "loss:  109.590614\n",
      "loss:  109.59061\n",
      "loss:  109.59061\n",
      "loss:  109.59061\n",
      "loss:  109.5906\n",
      "loss:  109.59061\n",
      "loss:  109.59061\n",
      "loss:  109.59061\n",
      "loss:  109.5906\n",
      "loss:  109.5906\n",
      "loss:  109.59058\n",
      "loss:  109.5906\n",
      "loss:  109.59058\n",
      "loss:  109.5906\n",
      "loss:  109.59058\n",
      "loss:  109.59058\n",
      "loss:  109.59057\n",
      "loss:  109.590576\n",
      "loss:  109.590576\n",
      "loss:  109.590576\n",
      "loss:  109.59057\n",
      "loss:  109.59057\n",
      "loss:  109.59057\n",
      "loss:  109.590546\n",
      "loss:  109.59055\n",
      "loss:  109.59055\n",
      "loss:  109.59055\n",
      "loss:  109.590546\n",
      "loss:  109.590546\n",
      "loss:  109.59054\n",
      "loss:  109.590546\n",
      "loss:  109.590546\n",
      "loss:  109.590546\n",
      "loss:  109.59054\n",
      "loss:  109.59054\n",
      "loss:  109.59054\n",
      "loss:  109.59053\n",
      "loss:  109.59053\n",
      "loss:  109.59054\n",
      "loss:  109.59054\n",
      "loss:  109.590515\n",
      "loss:  109.590515\n",
      "loss:  109.59053\n",
      "loss:  109.59053\n",
      "loss:  109.59051\n",
      "loss:  109.59051\n",
      "loss:  109.59051\n",
      "loss:  109.590515\n",
      "loss:  109.59051\n",
      "loss:  109.59051\n",
      "loss:  109.5905\n",
      "loss:  109.59051\n",
      "loss:  109.590485\n",
      "loss:  109.590485\n",
      "loss:  109.5905\n",
      "loss:  109.5905\n",
      "loss:  109.590485\n",
      "loss:  109.59048\n",
      "loss:  109.590485\n",
      "loss:  109.590485\n",
      "loss:  109.590485\n",
      "loss:  109.59048\n",
      "loss:  109.59048\n",
      "loss:  109.59047\n",
      "loss:  109.59046\n",
      "loss:  109.59046\n",
      "loss:  109.59047\n",
      "loss:  109.59048\n",
      "loss:  109.59046\n",
      "loss:  109.59045\n",
      "loss:  109.59046\n",
      "loss:  109.59047\n",
      "loss:  109.59045\n",
      "loss:  109.59044\n",
      "loss:  109.59044\n",
      "loss:  109.59044\n",
      "loss:  109.59043\n",
      "loss:  109.59044\n",
      "loss:  109.59043\n",
      "loss:  109.59044\n",
      "loss:  109.59043\n",
      "loss:  109.59043\n",
      "loss:  109.59043\n",
      "loss:  109.59043\n",
      "loss:  109.59043\n",
      "loss:  109.59042\n",
      "loss:  109.59043\n",
      "loss:  109.59042\n",
      "loss:  109.59043\n",
      "loss:  109.59041\n",
      "loss:  109.59041\n",
      "loss:  109.5904\n",
      "loss:  109.59041\n",
      "loss:  109.59041\n",
      "loss:  109.59041\n",
      "loss:  109.59039\n",
      "loss:  109.5904\n",
      "loss:  109.59039\n",
      "loss:  109.59039\n",
      "loss:  109.59039\n",
      "loss:  109.59039\n",
      "loss:  109.59037\n",
      "loss:  109.59038\n",
      "loss:  109.59039\n",
      "loss:  109.59037\n",
      "loss:  109.59038\n",
      "loss:  109.59037\n",
      "loss:  109.59038\n",
      "loss:  109.59037\n",
      "loss:  109.59037\n",
      "loss:  109.59036\n",
      "loss:  109.59036\n",
      "loss:  109.59036\n",
      "loss:  109.59036\n",
      "loss:  109.590355\n",
      "loss:  109.590355\n",
      "loss:  109.59036\n",
      "loss:  109.590355\n",
      "loss:  109.590355\n",
      "loss:  109.59034\n",
      "loss:  109.590355\n",
      "loss:  109.59034\n",
      "loss:  109.59034\n",
      "loss:  109.59034\n",
      "loss:  109.59033\n",
      "loss:  109.590324\n",
      "loss:  109.59033\n",
      "loss:  109.59031\n",
      "loss:  109.590324\n",
      "loss:  109.59031\n",
      "loss:  109.590324\n",
      "loss:  109.590324\n",
      "loss:  109.59033\n",
      "loss:  109.59031\n",
      "loss:  109.59031\n",
      "loss:  109.59031\n",
      "loss:  109.59031\n",
      "loss:  109.59031\n",
      "loss:  109.5903\n",
      "loss:  109.5903\n",
      "loss:  109.5903\n",
      "loss:  109.590294\n",
      "loss:  109.590294\n",
      "loss:  109.590294\n",
      "loss:  109.59029\n",
      "loss:  109.590294\n",
      "loss:  109.59029\n",
      "loss:  109.59029\n",
      "loss:  109.59029\n",
      "loss:  109.59027\n",
      "loss:  109.59029\n",
      "loss:  109.59027\n",
      "loss:  109.59027\n",
      "loss:  109.590256\n",
      "loss:  109.59026\n",
      "loss:  109.590256\n",
      "loss:  109.59026\n",
      "loss:  109.59026\n",
      "loss:  109.59026\n",
      "loss:  109.590256\n",
      "loss:  109.590256\n",
      "loss:  109.59024\n",
      "loss:  109.59024\n",
      "loss:  109.590256\n",
      "loss:  109.59024\n",
      "loss:  109.59023\n",
      "loss:  109.590256\n",
      "loss:  109.59024\n",
      "loss:  109.59024\n",
      "loss:  109.59023\n",
      "loss:  109.59023\n",
      "loss:  109.590225\n",
      "loss:  109.590225\n",
      "loss:  109.590225\n",
      "loss:  109.59023\n",
      "loss:  109.590225\n",
      "loss:  109.590225\n",
      "loss:  109.590225\n",
      "loss:  109.59022\n",
      "loss:  109.59022\n",
      "loss:  109.59022\n",
      "loss:  109.590195\n",
      "loss:  109.5902\n",
      "loss:  109.5902\n",
      "loss:  109.5902\n",
      "loss:  109.590195\n",
      "loss:  109.5902\n",
      "loss:  109.5902\n",
      "loss:  109.590195\n",
      "loss:  109.59019\n",
      "loss:  109.59018\n",
      "loss:  109.590195\n",
      "loss:  109.59019\n",
      "loss:  109.59018\n",
      "loss:  109.59019\n",
      "loss:  109.59019\n",
      "loss:  109.59019\n",
      "loss:  109.59019\n",
      "loss:  109.590164\n",
      "loss:  109.590164\n",
      "loss:  109.590164\n",
      "loss:  109.59016\n",
      "loss:  109.59015\n",
      "loss:  109.590164\n",
      "loss:  109.590164\n",
      "loss:  109.59016\n",
      "loss:  109.59016\n",
      "loss:  109.59015\n",
      "loss:  109.59016\n",
      "loss:  109.59015\n",
      "loss:  109.59015\n",
      "loss:  109.590126\n",
      "loss:  109.59013\n",
      "loss:  109.59013\n",
      "loss:  109.59013\n",
      "loss:  109.59013\n",
      "loss:  109.590126\n",
      "loss:  109.590126\n",
      "loss:  109.59012\n",
      "loss:  109.590126\n",
      "loss:  109.590126\n",
      "loss:  109.590126\n",
      "loss:  109.59011\n",
      "loss:  109.590126\n",
      "loss:  109.59011\n",
      "loss:  109.59012\n",
      "loss:  109.59011\n",
      "loss:  109.590096\n",
      "loss:  109.590096\n",
      "loss:  109.59011\n",
      "loss:  109.59009\n",
      "loss:  109.590096\n",
      "loss:  109.590096\n",
      "loss:  109.590096\n",
      "loss:  109.590096\n",
      "loss:  109.59009\n",
      "loss:  109.59009\n",
      "loss:  109.59009\n",
      "loss:  109.59009\n",
      "loss:  109.59008\n",
      "loss:  109.59008\n",
      "loss:  109.59008\n",
      "loss:  109.590065\n",
      "loss:  109.59009\n",
      "loss:  109.590065\n",
      "loss:  109.590065\n",
      "loss:  109.59006\n",
      "loss:  109.59006\n",
      "loss:  109.590065\n",
      "loss:  109.590065\n",
      "loss:  109.59005\n",
      "loss:  109.59005\n",
      "loss:  109.59005\n",
      "loss:  109.59006\n",
      "loss:  109.59004\n",
      "loss:  109.59004\n",
      "loss:  109.59004\n",
      "loss:  109.59005\n",
      "loss:  109.59004\n",
      "loss:  109.59003\n",
      "loss:  109.59002\n",
      "loss:  109.59004\n",
      "loss:  109.59004\n",
      "loss:  109.59003\n",
      "loss:  109.59003\n",
      "loss:  109.59003\n",
      "loss:  109.59002\n",
      "loss:  109.59001\n",
      "loss:  109.59001\n",
      "loss:  109.59\n",
      "loss:  109.59002\n",
      "loss:  109.59\n",
      "loss:  109.59001\n",
      "loss:  109.59002\n",
      "loss:  109.59\n",
      "loss:  109.59001\n",
      "loss:  109.59\n",
      "loss:  109.58999\n",
      "loss:  109.58998\n",
      "loss:  109.59\n",
      "loss:  109.58999\n",
      "loss:  109.58999\n",
      "loss:  109.58998\n",
      "loss:  109.58998\n",
      "loss:  109.58997\n",
      "loss:  109.58998\n",
      "loss:  109.58997\n",
      "loss:  109.58997\n",
      "loss:  109.58996\n",
      "loss:  109.58998\n",
      "loss:  109.58998\n",
      "loss:  109.58996\n",
      "loss:  109.58997\n",
      "loss:  109.58996\n",
      "loss:  109.58996\n",
      "loss:  109.58995\n",
      "loss:  109.58995\n",
      "loss:  109.58994\n",
      "loss:  109.58996\n",
      "loss:  109.58996\n",
      "loss:  109.58994\n",
      "loss:  109.58995\n",
      "loss:  109.58995\n",
      "loss:  109.58995\n",
      "loss:  109.58992\n",
      "loss:  109.58994\n",
      "loss:  109.58994\n",
      "loss:  109.58992\n",
      "loss:  109.589935\n",
      "loss:  109.58992\n",
      "loss:  109.589935\n",
      "loss:  109.58991\n",
      "loss:  109.58992\n",
      "loss:  109.58992\n",
      "loss:  109.58992\n",
      "loss:  109.589905\n",
      "loss:  109.58991\n",
      "loss:  109.589905\n",
      "loss:  109.58991\n",
      "loss:  109.58991\n",
      "loss:  109.58988\n",
      "loss:  109.589905\n",
      "loss:  109.589905\n",
      "loss:  109.589905\n",
      "loss:  109.58989\n",
      "loss:  109.58988\n",
      "loss:  109.58988\n",
      "loss:  109.58988\n",
      "loss:  109.58988\n",
      "loss:  109.58988\n",
      "loss:  109.589874\n",
      "loss:  109.58989\n",
      "loss:  109.589874\n",
      "loss:  109.58987\n",
      "loss:  109.589874\n",
      "loss:  109.58987\n",
      "loss:  109.58985\n",
      "loss:  109.58987\n",
      "loss:  109.58987\n",
      "loss:  109.58987\n",
      "loss:  109.58985\n",
      "loss:  109.58987\n",
      "loss:  109.58987\n",
      "loss:  109.58984\n",
      "loss:  109.58984\n",
      "loss:  109.58984\n",
      "loss:  109.58985\n",
      "loss:  109.58984\n",
      "loss:  109.58984\n",
      "loss:  109.589836\n",
      "loss:  109.589836\n",
      "loss:  109.58982\n",
      "loss:  109.58982\n",
      "loss:  109.58982\n",
      "loss:  109.589836\n",
      "loss:  109.589836\n",
      "loss:  109.58981\n",
      "loss:  109.58982\n",
      "loss:  109.58981\n",
      "loss:  109.58981\n",
      "loss:  109.58981\n",
      "loss:  109.58981\n",
      "loss:  109.58982\n",
      "loss:  109.58981\n",
      "loss:  109.589806\n",
      "loss:  109.5898\n",
      "loss:  109.5898\n",
      "loss:  109.5898\n",
      "loss:  109.5898\n",
      "loss:  109.5898\n",
      "loss:  109.589806\n",
      "loss:  109.58978\n",
      "loss:  109.58978\n",
      "loss:  109.58978\n",
      "loss:  109.58978\n",
      "loss:  109.58978\n",
      "loss:  109.58978\n",
      "loss:  109.589775\n",
      "loss:  109.589775\n",
      "loss:  109.58978\n",
      "loss:  109.589775\n",
      "loss:  109.58977\n",
      "loss:  109.58975\n",
      "loss:  109.58977\n",
      "loss:  109.58977\n",
      "loss:  109.58975\n",
      "loss:  109.58975\n",
      "loss:  109.58975\n",
      "loss:  109.589745\n",
      "loss:  109.589745\n",
      "loss:  109.58975\n",
      "loss:  109.58975\n",
      "loss:  109.589745\n",
      "loss:  109.589745\n",
      "loss:  109.589745\n",
      "loss:  109.58974\n",
      "loss:  109.589745\n",
      "loss:  109.58973\n",
      "loss:  109.58974\n",
      "loss:  109.58974\n",
      "loss:  109.58974\n",
      "loss:  109.58971\n",
      "loss:  109.58973\n",
      "loss:  109.58973\n",
      "loss:  109.58973\n",
      "loss:  109.58973\n",
      "loss:  109.58971\n",
      "loss:  109.589714\n",
      "loss:  109.58971\n",
      "loss:  109.5897\n",
      "loss:  109.58971\n",
      "loss:  109.5897\n",
      "loss:  109.5897\n",
      "loss:  109.58971\n",
      "loss:  109.58969\n",
      "loss:  109.58969\n",
      "loss:  109.5897\n",
      "loss:  109.589676\n",
      "loss:  109.589676\n",
      "loss:  109.58969\n",
      "loss:  109.58969\n",
      "loss:  109.589676\n",
      "loss:  109.589676\n",
      "loss:  109.58967\n",
      "loss:  109.589676\n",
      "loss:  109.589676\n",
      "loss:  109.58967\n",
      "loss:  109.58967\n",
      "loss:  109.58967\n",
      "loss:  109.58966\n",
      "loss:  109.58966\n",
      "loss:  109.58966\n",
      "loss:  109.58966\n",
      "loss:  109.58966\n",
      "loss:  109.589645\n",
      "loss:  109.58966\n",
      "loss:  109.589645\n",
      "loss:  109.589645\n",
      "loss:  109.589645\n",
      "loss:  109.58964\n",
      "loss:  109.58964\n",
      "loss:  109.589645\n",
      "loss:  109.58964\n",
      "loss:  109.58963\n",
      "loss:  109.58964\n",
      "loss:  109.58963\n",
      "loss:  109.58963\n",
      "loss:  109.58963\n",
      "loss:  109.58963\n",
      "loss:  109.58962\n",
      "loss:  109.58963\n",
      "loss:  109.5896\n",
      "loss:  109.58961\n",
      "loss:  109.58961\n",
      "loss:  109.58961\n",
      "loss:  109.58962\n",
      "loss:  109.58961\n",
      "loss:  109.58961\n",
      "loss:  109.58959\n",
      "loss:  109.5896\n",
      "loss:  109.58959\n",
      "loss:  109.5896\n",
      "loss:  109.58959\n",
      "loss:  109.58959\n",
      "loss:  109.58958\n",
      "loss:  109.58959\n",
      "loss:  109.58959\n",
      "loss:  109.58958\n",
      "loss:  109.58959\n",
      "loss:  109.58958\n",
      "loss:  109.58959\n",
      "loss:  109.58956\n",
      "loss:  109.58957\n",
      "loss:  109.58957\n",
      "loss:  109.58957\n",
      "loss:  109.589554\n",
      "loss:  109.58956\n",
      "loss:  109.58956\n",
      "loss:  109.58956\n",
      "loss:  109.58957\n",
      "loss:  109.589554\n",
      "loss:  109.58956\n",
      "loss:  109.589554\n",
      "loss:  109.589554\n",
      "loss:  109.58954\n",
      "loss:  109.58954\n",
      "loss:  109.58954\n",
      "loss:  109.589554\n",
      "loss:  109.58954\n",
      "loss:  109.58954\n",
      "loss:  109.58954\n",
      "loss:  109.58952\n",
      "loss:  109.58953\n",
      "loss:  109.58951\n",
      "loss:  109.58953\n",
      "loss:  109.58952\n",
      "loss:  109.58952\n",
      "loss:  109.58951\n",
      "loss:  109.58952\n",
      "loss:  109.58951\n",
      "loss:  109.58951\n",
      "loss:  109.58952\n",
      "loss:  109.5895\n",
      "loss:  109.58951\n",
      "loss:  109.5895\n",
      "loss:  109.5895\n",
      "loss:  109.5895\n",
      "loss:  109.5895\n",
      "loss:  109.589485\n",
      "loss:  109.5895\n",
      "loss:  109.58949\n",
      "loss:  109.58949\n",
      "loss:  109.58947\n",
      "loss:  109.58949\n",
      "loss:  109.58949\n",
      "loss:  109.589485\n",
      "loss:  109.589485\n",
      "loss:  109.58947\n",
      "loss:  109.58947\n",
      "loss:  109.58946\n",
      "loss:  109.58947\n",
      "loss:  109.589455\n",
      "loss:  109.58946\n",
      "loss:  109.58946\n",
      "loss:  109.58946\n",
      "loss:  109.58946\n",
      "loss:  109.589455\n",
      "loss:  109.589455\n",
      "loss:  109.589455\n",
      "loss:  109.589455\n",
      "loss:  109.58945\n",
      "loss:  109.58945\n",
      "loss:  109.58943\n",
      "loss:  109.58943\n",
      "loss:  109.589455\n",
      "loss:  109.58945\n",
      "loss:  109.58945\n",
      "loss:  109.58943\n",
      "loss:  109.58945\n",
      "loss:  109.589424\n",
      "loss:  109.58943\n",
      "loss:  109.589424\n",
      "loss:  109.58942\n",
      "loss:  109.58942\n",
      "loss:  109.589424\n",
      "loss:  109.5894\n",
      "loss:  109.5894\n",
      "loss:  109.589424\n",
      "loss:  109.58942\n",
      "loss:  109.58939\n",
      "loss:  109.58939\n",
      "loss:  109.5894\n",
      "loss:  109.58939\n",
      "loss:  109.58939\n",
      "loss:  109.589386\n",
      "loss:  109.589386\n",
      "loss:  109.58939\n",
      "loss:  109.58939\n",
      "loss:  109.58939\n",
      "loss:  109.589386\n",
      "loss:  109.58939\n",
      "loss:  109.58938\n",
      "loss:  109.58938\n",
      "loss:  109.58938\n",
      "loss:  109.58938\n",
      "loss:  109.58938\n",
      "loss:  109.58936\n",
      "loss:  109.589355\n",
      "loss:  109.58938\n",
      "loss:  109.58938\n",
      "loss:  109.589355\n",
      "loss:  109.58936\n",
      "loss:  109.589355\n",
      "loss:  109.58936\n",
      "loss:  109.58935\n",
      "loss:  109.58935\n",
      "loss:  109.58935\n",
      "loss:  109.589355\n",
      "loss:  109.58933\n",
      "loss:  109.58935\n",
      "loss:  109.58935\n",
      "loss:  109.58933\n",
      "loss:  109.58933\n",
      "loss:  109.58933\n",
      "loss:  109.589325\n",
      "loss:  109.589325\n",
      "loss:  109.58933\n",
      "loss:  109.589325\n",
      "loss:  109.589325\n",
      "loss:  109.58932\n",
      "loss:  109.58932\n",
      "loss:  109.58931\n",
      "loss:  109.58932\n",
      "loss:  109.58932\n",
      "loss:  109.58932\n",
      "loss:  109.58931\n",
      "loss:  109.58931\n",
      "loss:  109.58931\n",
      "loss:  109.58931\n",
      "loss:  109.58931\n",
      "loss:  109.58929\n",
      "loss:  109.589294\n",
      "loss:  109.589294\n",
      "loss:  109.589294\n",
      "loss:  109.58929\n",
      "loss:  109.58928\n",
      "loss:  109.589294\n",
      "loss:  109.58928\n",
      "loss:  109.58929\n",
      "loss:  109.58928\n",
      "loss:  109.58928\n",
      "loss:  109.58928\n",
      "loss:  109.589264\n",
      "loss:  109.589264\n",
      "loss:  109.58928\n",
      "loss:  109.58926\n",
      "loss:  109.58926\n",
      "loss:  109.58926\n",
      "loss:  109.589264\n",
      "loss:  109.58926\n",
      "loss:  109.58925\n",
      "loss:  109.58925\n",
      "loss:  109.58925\n",
      "loss:  109.58925\n",
      "loss:  109.58924\n",
      "loss:  109.58924\n",
      "loss:  109.58924\n",
      "loss:  109.58925\n",
      "loss:  109.58925\n",
      "loss:  109.589226\n",
      "loss:  109.58924\n",
      "loss:  109.589226\n",
      "loss:  109.589226\n",
      "loss:  109.58922\n",
      "loss:  109.58922\n",
      "loss:  109.58922\n",
      "loss:  109.58922\n",
      "loss:  109.58921\n",
      "loss:  109.58922\n",
      "loss:  109.58921\n",
      "loss:  109.58921\n",
      "loss:  109.58921\n",
      "loss:  109.58921\n",
      "loss:  109.58922\n",
      "loss:  109.5892\n",
      "loss:  109.5892\n",
      "loss:  109.58919\n",
      "loss:  109.58919\n",
      "loss:  109.5892\n",
      "loss:  109.5892\n",
      "loss:  109.58919\n",
      "loss:  109.58918\n",
      "loss:  109.5892\n",
      "loss:  109.58918\n",
      "loss:  109.58919\n",
      "loss:  109.58918\n",
      "loss:  109.58919\n",
      "loss:  109.58917\n",
      "loss:  109.58917\n",
      "loss:  109.58918\n",
      "loss:  109.58918\n",
      "loss:  109.58917\n",
      "loss:  109.58916\n",
      "loss:  109.58917\n",
      "loss:  109.58916\n",
      "loss:  109.58916\n",
      "loss:  109.58915\n",
      "loss:  109.58915\n",
      "loss:  109.58915\n",
      "loss:  109.58916\n",
      "loss:  109.58915\n",
      "loss:  109.58915\n",
      "loss:  109.58915\n",
      "loss:  109.58914\n",
      "loss:  109.58914\n",
      "loss:  109.58915\n",
      "loss:  109.58914\n",
      "loss:  109.58914\n",
      "loss:  109.58912\n",
      "loss:  109.58912\n",
      "loss:  109.589134\n",
      "loss:  109.58912\n",
      "loss:  109.58912\n",
      "loss:  109.58912\n",
      "loss:  109.58912\n",
      "loss:  109.589134\n",
      "loss:  109.58911\n",
      "loss:  109.58911\n",
      "loss:  109.5891\n",
      "loss:  109.58911\n",
      "loss:  109.5891\n",
      "loss:  109.58911\n",
      "loss:  109.5891\n",
      "loss:  109.5891\n",
      "loss:  109.58909\n",
      "loss:  109.58909\n",
      "loss:  109.58909\n",
      "loss:  109.58909\n",
      "loss:  109.58908\n",
      "loss:  109.58907\n",
      "loss:  109.58908\n",
      "loss:  109.58908\n",
      "loss:  109.58908\n",
      "loss:  109.58907\n",
      "loss:  109.58907\n",
      "loss:  109.58907\n",
      "loss:  109.58907\n",
      "loss:  109.58907\n",
      "loss:  109.589066\n",
      "loss:  109.589066\n",
      "loss:  109.58907\n",
      "loss:  109.589066\n",
      "loss:  109.58905\n",
      "loss:  109.58905\n",
      "loss:  109.58905\n",
      "loss:  109.589066\n",
      "loss:  109.58905\n",
      "loss:  109.58904\n",
      "loss:  109.589066\n",
      "loss:  109.58904\n",
      "loss:  109.58904\n",
      "loss:  109.58905\n",
      "loss:  109.589035\n",
      "loss:  109.58902\n",
      "loss:  109.589035\n",
      "loss:  109.58902\n",
      "loss:  109.589035\n",
      "loss:  109.589035\n",
      "loss:  109.58902\n",
      "loss:  109.58902\n",
      "loss:  109.58901\n",
      "loss:  109.589035\n",
      "loss:  109.58901\n",
      "loss:  109.58901\n",
      "loss:  109.58902\n",
      "loss:  109.58901\n",
      "loss:  109.58902\n",
      "loss:  109.589005\n",
      "loss:  109.589005\n",
      "loss:  109.589\n",
      "loss:  109.589005\n",
      "loss:  109.589005\n",
      "loss:  109.589005\n",
      "loss:  109.58898\n",
      "loss:  109.58898\n",
      "loss:  109.589\n",
      "loss:  109.58898\n",
      "loss:  109.589\n",
      "loss:  109.58898\n",
      "loss:  109.58898\n",
      "loss:  109.588974\n",
      "loss:  109.58898\n",
      "loss:  109.58898\n",
      "loss:  109.588974\n",
      "loss:  109.58897\n",
      "loss:  109.588974\n",
      "loss:  109.58897\n",
      "loss:  109.58896\n",
      "loss:  109.58896\n",
      "loss:  109.58896\n",
      "loss:  109.58897\n",
      "loss:  109.58894\n",
      "loss:  109.58894\n",
      "loss:  109.58894\n",
      "loss:  109.58894\n",
      "loss:  109.58894\n",
      "loss:  109.58894\n",
      "loss:  109.58894\n",
      "loss:  109.588936\n",
      "loss:  109.58893\n",
      "loss:  109.588936\n",
      "loss:  109.588936\n",
      "loss:  109.588936\n",
      "loss:  109.588936\n",
      "loss:  109.58893\n",
      "loss:  109.588936\n",
      "loss:  109.58893\n",
      "loss:  109.588936\n",
      "loss:  109.58891\n",
      "loss:  109.58891\n",
      "loss:  109.58893\n",
      "loss:  109.58893\n",
      "loss:  109.588905\n",
      "loss:  109.588905\n",
      "loss:  109.58891\n",
      "loss:  109.58891\n",
      "loss:  109.588905\n",
      "loss:  109.58889\n",
      "loss:  109.5889\n",
      "loss:  109.5889\n",
      "loss:  109.588905\n",
      "loss:  109.5889\n",
      "loss:  109.58889\n",
      "loss:  109.5889\n",
      "loss:  109.58889\n",
      "loss:  109.58889\n",
      "loss:  109.588875\n",
      "loss:  109.58889\n",
      "loss:  109.5889\n",
      "loss:  109.588875\n",
      "loss:  109.588875\n",
      "loss:  109.58887\n",
      "loss:  109.58887\n",
      "loss:  109.588875\n",
      "loss:  109.58887\n",
      "loss:  109.58886\n",
      "loss:  109.58887\n",
      "loss:  109.588844\n",
      "loss:  109.58886\n",
      "loss:  109.58886\n",
      "loss:  109.588844\n",
      "loss:  109.588844\n",
      "loss:  109.58884\n",
      "loss:  109.58886\n",
      "loss:  109.58886\n",
      "loss:  109.588844\n",
      "loss:  109.58884\n",
      "loss:  109.58884\n",
      "loss:  109.588844\n",
      "loss:  109.588844\n",
      "loss:  109.58884\n",
      "loss:  109.58883\n",
      "loss:  109.58883\n",
      "loss:  109.58884\n",
      "loss:  109.58883\n",
      "loss:  109.58882\n",
      "loss:  109.58883\n",
      "loss:  109.58883\n",
      "loss:  109.588806\n",
      "loss:  109.58882\n",
      "loss:  109.588806\n",
      "loss:  109.58882\n",
      "loss:  109.588806\n",
      "loss:  109.5888\n",
      "loss:  109.588806\n",
      "loss:  109.588806\n",
      "loss:  109.588806\n",
      "loss:  109.5888\n",
      "loss:  109.5888\n",
      "loss:  109.5888\n",
      "loss:  109.58879\n",
      "loss:  109.58879\n",
      "loss:  109.58879\n",
      "loss:  109.58879\n",
      "loss:  109.58879\n",
      "loss:  109.58879\n",
      "loss:  109.588776\n",
      "loss:  109.588776\n",
      "loss:  109.58879\n",
      "loss:  109.58877\n",
      "loss:  109.58877\n",
      "loss:  109.58875\n",
      "loss:  109.58877\n",
      "loss:  109.58876\n",
      "loss:  109.58876\n",
      "loss:  109.58876\n",
      "loss:  109.58876\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "for i in range(1000):\n",
    "    if (a.grad is not None):\n",
    "        a.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    \n",
    "    predictions = a.expand_as(x) * x + b.expand_as(x)\n",
    "    loss = torch.mean( ( predictions - y) ** 2)\n",
    "    print( 'loss: ', loss.data.numpy() )\n",
    "    loss.backward()\n",
    "    a.data.add_( - learning_rate * a.grad.data )\n",
    "    b.data.add_( - learning_rate * b.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"235pt\" height=\"503pt\"\n viewBox=\"0.00 0.00 235.00 503.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 499)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-499 231,-499 231,4 -4,4\"/>\n<!-- 140323076569824 -->\n<g id=\"node1\" class=\"node\">\n<title>140323076569824</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"141.5,-31 87.5,-31 87.5,0 141.5,0 141.5,-31\"/>\n<text text-anchor=\"middle\" x=\"114.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n</g>\n<!-- 140323076552256 -->\n<g id=\"node2\" class=\"node\">\n<title>140323076552256</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"162,-86 67,-86 67,-67 162,-67 162,-86\"/>\n<text text-anchor=\"middle\" x=\"114.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MeanBackward0</text>\n</g>\n<!-- 140323076552256&#45;&gt;140323076569824 -->\n<g id=\"edge11\" class=\"edge\">\n<title>140323076552256&#45;&gt;140323076569824</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M114.5,-66.9688C114.5,-60.1289 114.5,-50.5621 114.5,-41.5298\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.0001,-41.3678 114.5,-31.3678 111.0001,-41.3678 118.0001,-41.3678\"/>\n</g>\n<!-- 140323076551968 -->\n<g id=\"node3\" class=\"node\">\n<title>140323076551968</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"159,-141 70,-141 70,-122 159,-122 159,-141\"/>\n<text text-anchor=\"middle\" x=\"114.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">PowBackward0</text>\n</g>\n<!-- 140323076551968&#45;&gt;140323076552256 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140323076551968&#45;&gt;140323076552256</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M114.5,-121.9197C114.5,-114.9083 114.5,-105.1442 114.5,-96.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.0001,-96.3408 114.5,-86.3408 111.0001,-96.3409 118.0001,-96.3408\"/>\n</g>\n<!-- 140323076551872 -->\n<g id=\"node4\" class=\"node\">\n<title>140323076551872</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"159,-196 70,-196 70,-177 159,-177 159,-196\"/>\n<text text-anchor=\"middle\" x=\"114.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 140323076551872&#45;&gt;140323076551968 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140323076551872&#45;&gt;140323076551968</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M114.5,-176.9197C114.5,-169.9083 114.5,-160.1442 114.5,-151.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.0001,-151.3408 114.5,-141.3408 111.0001,-151.3409 118.0001,-151.3408\"/>\n</g>\n<!-- 140323076551776 -->\n<g id=\"node5\" class=\"node\">\n<title>140323076551776</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"159,-251 70,-251 70,-232 159,-232 159,-251\"/>\n<text text-anchor=\"middle\" x=\"114.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 140323076551776&#45;&gt;140323076551872 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140323076551776&#45;&gt;140323076551872</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M114.5,-231.9197C114.5,-224.9083 114.5,-215.1442 114.5,-206.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"118.0001,-206.3408 114.5,-196.3408 111.0001,-206.3409 118.0001,-206.3408\"/>\n</g>\n<!-- 140323076551536 -->\n<g id=\"node6\" class=\"node\">\n<title>140323076551536</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-306 12,-306 12,-287 101,-287 101,-306\"/>\n<text text-anchor=\"middle\" x=\"56.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140323076551536&#45;&gt;140323076551776 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140323076551536&#45;&gt;140323076551776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M66.6028,-286.9197C74.9004,-279.0514 86.8537,-267.7164 96.7431,-258.3385\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.2745,-260.7615 104.1224,-251.3408 94.4578,-255.6821 99.2745,-260.7615\"/>\n</g>\n<!-- 140323076551584 -->\n<g id=\"node7\" class=\"node\">\n<title>140323076551584</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"107,-361 0,-361 0,-342 107,-342 107,-361\"/>\n<text text-anchor=\"middle\" x=\"53.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ExpandBackward0</text>\n</g>\n<!-- 140323076551584&#45;&gt;140323076551536 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140323076551584&#45;&gt;140323076551536</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M54.0226,-341.9197C54.405,-334.9083 54.9376,-325.1442 55.411,-316.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"58.9133,-316.5166 55.9632,-306.3408 51.9237,-316.1353 58.9133,-316.5166\"/>\n</g>\n<!-- 140323076554320 -->\n<g id=\"node8\" class=\"node\">\n<title>140323076554320</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"104,-422 3,-422 3,-403 104,-403 104,-422\"/>\n<text text-anchor=\"middle\" x=\"53.5\" y=\"-410\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140323076554320&#45;&gt;140323076551584 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140323076554320&#45;&gt;140323076551584</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M53.5,-402.9688C53.5,-394.5131 53.5,-381.8901 53.5,-371.2615\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"57.0001,-371.1656 53.5,-361.1656 50.0001,-371.1657 57.0001,-371.1656\"/>\n</g>\n<!-- 140323076889072 -->\n<g id=\"node9\" class=\"node\">\n<title>140323076889072</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"80.5,-495 26.5,-495 26.5,-464 80.5,-464 80.5,-495\"/>\n<text text-anchor=\"middle\" x=\"53.5\" y=\"-471\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140323076889072&#45;&gt;140323076554320 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140323076889072&#45;&gt;140323076554320</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M53.5,-463.9604C53.5,-454.6356 53.5,-442.6748 53.5,-432.6317\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"57.0001,-432.35 53.5,-422.3501 50.0001,-432.3501 57.0001,-432.35\"/>\n</g>\n<!-- 140323076551728 -->\n<g id=\"node10\" class=\"node\">\n<title>140323076551728</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"227,-306 120,-306 120,-287 227,-287 227,-306\"/>\n<text text-anchor=\"middle\" x=\"173.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">ExpandBackward0</text>\n</g>\n<!-- 140323076551728&#45;&gt;140323076551776 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140323076551728&#45;&gt;140323076551776</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M163.223,-286.9197C154.7824,-279.0514 142.623,-267.7164 132.5631,-258.3385\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"134.7578,-255.5995 125.0565,-251.3408 129.9846,-260.7198 134.7578,-255.5995\"/>\n</g>\n<!-- 140323076554464 -->\n<g id=\"node11\" class=\"node\">\n<title>140323076554464</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"226,-361 125,-361 125,-342 226,-342 226,-361\"/>\n<text text-anchor=\"middle\" x=\"175.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n</g>\n<!-- 140323076554464&#45;&gt;140323076551728 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140323076554464&#45;&gt;140323076551728</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M175.1516,-341.9197C174.8967,-334.9083 174.5416,-325.1442 174.226,-316.4652\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"177.719,-316.207 173.8578,-306.3408 170.7237,-316.4614 177.719,-316.207\"/>\n</g>\n<!-- 140323124903664 -->\n<g id=\"node12\" class=\"node\">\n<title>140323124903664</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"202.5,-428 148.5,-428 148.5,-397 202.5,-397 202.5,-428\"/>\n<text text-anchor=\"middle\" x=\"175.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 140323124903664&#45;&gt;140323076554464 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140323124903664&#45;&gt;140323076554464</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M175.5,-396.791C175.5,-389.0249 175.5,-379.5706 175.5,-371.3129\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"179.0001,-371.0647 175.5,-361.0648 172.0001,-371.0648 179.0001,-371.0647\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f9f8321b550>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从打印出的损失中我们可以观察到损失一直在下降。\n",
    "\n",
    "我们现在可以把直线绘制出来，看看这条直线是什么样子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "x_data = x.data.numpy() # 获得x包裹的数据\n",
    "plt.figure(figsize = (10, 7)) #设定绘图窗口大小\n",
    "xplot, = plt.plot(x_data, y.data.numpy(), 'o') # 绘制原始数据\n",
    "yplot, = plt.plot(x_data, a.data.numpy() * x_data + b.data.numpy())  #绘制拟合数据\n",
    "plt.xlabel('X') #更改坐标轴标注\n",
    "plt.ylabel('Y') #更改坐标轴标注\n",
    "str1 = str(a.data.numpy()[0]) + 'x +' + str(b.data.numpy()[0]) #图例信息\n",
    "plt.legend([xplot, yplot],['Data', str1]) #绘制图例\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = x.data.numpy()\n",
    "\n",
    "plt.figure( figsize = (10,7))\n",
    "\n",
    "xplot, = plt.plot(x_data, y.data.numpy(), 'o')\n",
    "yplot, = plt.plot(x_data, a.data.numpy() * x_data + b.data.numpy() )\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "str1 = str(a.data.numpy()[0]) + ' x + ' + str(b.data.numpy()[0])\n",
    "\n",
    "plt.legend([xplot, yplot], ['data', str1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以观察到直线已经拟合了点的走势。\n",
    "\n",
    "你可以修改训练次数，把训练次数改的很小，或者增加 10 倍，看看线性拟合会有什么变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.3 测试模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "虽然模型很简单，但我们仍要进行测试的流程。\n",
    "\n",
    "因为“准备数据”、“模型设计”、“训练”、“测试”是完成深度学习任务的基本套路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**▶ 示例代码：**\n",
    "\n",
    "```python\n",
    "x_test = Variable(torch.FloatTensor([1, 2, 10, 100, 1000])) #随便选择一些点1，2，……，1000\n",
    "predictions = a.expand_as(x_test) * x_test + b.expand_as(x_test) #计算模型的预测结果\n",
    "predictions #输出\n",
    "```\n",
    "\n",
    "**▶ 动手练习：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.2980,   2.2709,  10.0544,  97.6193, 973.2676],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.FloatTensor([1,2,10,100,1000])\n",
    "predictions = a.expand_as(x_test) * x_test + b.expand_as(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a href=\"#index\">回到目录</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 五、实验总结\n",
    "\n",
    "在本节实验中，我们熟悉了张量、自动微分变量的用法，认识了PyTorch中的计算图，并动手实践完成了一个线性回归模型。如果你亲自动手实践了每一行代码，我相信你对PyTorch的基本用法一定有了些许了解。在本节实验中积累的经验非常重要，因为你在以后的实验中会一直用到今天学习的知识点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 六、参考链接\n",
    "\n",
    "Tensor 支持的所有操作：\n",
    "\n",
    "http://pytorch.org/docs/0.3.0/tensors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div style=\"color: #999;font-size: 12px;\">©️ 本课程内容，由作者授权学堂在线发布，未经允许，禁止转载、下载及非法传播。</div>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61a57a4b5406d2de388e2f91097d4e4bcd7d5f4a46f53a795aa28a02eed27fc5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
