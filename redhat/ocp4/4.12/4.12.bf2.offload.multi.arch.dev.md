# openshift 4.12 BF2 offload

- [OVN/OVS offloading with OpenShift on NVIDIA BlueField-2 DPUs](https://access.redhat.com/articles/6804281)

bf2 network connection diagram:

![](./dia/bf2.offload.drawio.svg)

- https://multi.ocp.releases.ci.openshift.org/
  - [4.12.9](https://multi.ocp.releases.ci.openshift.org/releasestream/4-stable-multi/release/4.12.9)
  - quay.io/openshift-release-dev/ocp-release:4.12.9-multi

# fresh BF2 with official ubuntu os 

- [Installing Red Hat Enterprise Linux on NVIDIA BlueField-2 DPU](https://developers.redhat.com/articles/2021/10/18/sensitive-information-detection-using-nvidia-morpheus-ai-framework#setting_up_nvidia_netq_agent_on_nvidia_bluefield_2_dpu)

```bash


# https://bugzilla.redhat.com/show_bug.cgi?id=1814682
dnf install -y kernel-modules-extra psmisc

mkdir -p /data/down/
cd /data/down/

dnf install -y rshim expect wget minicom rpm-build lshw
systemctl enable --now rshim
systemctl status rshim --no-pager -l

dnf install -y openssl-devel mstflint

# nat router on host
# https://access.redhat.com/discussions/4642721
cat << EOF >> /etc/sysctl.d/99-wzh-sysctl.conf

net.ipv4.ip_forward = 1

EOF
sysctl --system

systemctl disable --now firewalld

# download bfb from here
# https://developer.nvidia.com/networking/doca
# and docs here
# https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/Deploying+BlueField+Software+Using+BFB+from+Host
wget -O bf2.bfb https://content.mellanox.com/BlueField/BFBs/Ubuntu20.04/DOCA_1.5.1_BSP_3.9.3_Ubuntu_20.04-4.2211-LTS.signed.bfb

cat bf2.bfb > /dev/rshim0/boot

# if you want to connect to bf2 through serial console
minicom --color on --baudrate 115200 --device /dev/rshim0/console

# on console of bf2
# login using ubuntu / ubuntu
# and change to mellanox

ip link | grep -A 1 enp3s0f1s0
# 11: enp3s0f1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether 02:7f:40:ae:d9:cc brd ff:ff:ff:ff:ff:ff

# upgrade fw
/opt/mellanox/mlnx-fw-updater/mlnx_fw_updater.pl

mlxconfig -d /dev/mst/mt41686_pciconf0 -y reset

mlxconfig -d /dev/mst/mt41686_pciconf0  s LINK_TYPE_P1=2 LINK_TYPE_P2=2

# power off and on machine.

grep -s -H "" /sys/class/net/*/phys_port_name
# /sys/class/net/en3f0pf0sf0/phys_port_name:pf0sf0
# /sys/class/net/en3f1pf1sf0/phys_port_name:pf1sf0
# /sys/class/net/enp3s0f0s0/phys_port_name:p0
# /sys/class/net/enp3s0f1s0/phys_port_name:p0
# /sys/class/net/p0/phys_port_name:p0
# /sys/class/net/p1/phys_port_name:p1
# /sys/class/net/pf0hpf/phys_port_name:pf0
# /sys/class/net/pf1hpf/phys_port_name:pf1

# on 101
nmcli con add type bridge-slave ifname enp7s0f1np1 master baremetal
nmcli con up baremetal

# on 103, bf2
cat /etc/netplan/60-mlnx.yaml
# network:
#   ethernets:
#     enp3s0f0s0:
#       renderer: networkd
#       dhcp4: 'true'
#     enp3s0f1s0:
#       renderer: networkd
#       dhcp4: 'true'
#   version: 2

cat << EOF > /etc/netplan/60-mlnx.yaml
network:
  ethernets:
    enp3s0f0s0:
      renderer: networkd
      dhcp4: 'true'
    enp3s0f1s0:
      renderer: networkd
      dhcp4: no
      addresses:
        - 192.168.7.113/24
      gateway4: 192.168.7.9
      nameservers:
          addresses: [114.114.114.114]
  version: 2
EOF

netplan apply

# login on 101
ssh ubuntu@192.168.7.113


```

# flash BF2 with rocky linux 8

we need to get ifname, so we have to flash it with rocky linux 8

```bash

# shell come frome https://github.com/wangzheng422/rhel-on-bf2
# download rhel8.6 install iso from  
# https://mirrors.nju.edu.cn/rocky/8/isos/aarch64/Rocky-8.7-aarch64-minimal.iso
export RHEL_ISO=Rocky-8.7-aarch64-minimal.iso
bash bluefield_provision.sh -s
# === STATUS === Checking usability of SRIOV for PCI 0000:05:00.0
# SRIOV enabled
# EMBEDDED_CPU mode enabled
# === STATUS === Checking usability of SRIOV for PCI 0000:05:00.1
# SRIOV enabled
# EMBEDDED_CPU mode enabled

setenforce 0

# bash bluefield_provision.sh -p

iptables -F
bash ./PXE_setup_RHEL_install_over_mlx.sh -i "${RHEL_ISO}" -p tmfifo -k RHEL8-bluefield.ks

uplink_interface="$(ip route |grep ^default | sed 's/.*dev \([^ ]\+\).*/\1/')"
test -n "${uplink_interface}" || die "need a default route"
    
iptables -t nat -A POSTROUTING -o "${uplink_interface}" -j MASQUERADE

# on a new terminal
minicom --color on --baudrate 115200 --device /dev/rshim0/console

# change back to original terminal
# to reboot the bf2
echo BOOT_MODE 1 > /dev/rshim0/misc
echo SW_RESET 1 > /dev/rshim0/misc

# sleep 10
nmcli conn up tmfifo_net0
systemctl restart dhcpd

iptables -t nat -A POSTROUTING -o "${uplink_interface}" -j MASQUERADE

# and see result on com console
# and press 'ESC' to see bios console
# and select 'Boot Manager'
# select EFI network with following content


```
see result on com console, press 'ESC' to see bios console

![](../4.10/imgs/2022-06-14-16-50-28.png)

select 'Boot Manager'

![](../4.10/imgs/2022-06-14-16-51-14.png)

select EFI network with following content

![](../4.10/imgs/2022-06-14-16-51-42.png)

```
MAC(001ACAFFFF01,0x1)/
IPv4(0.0.0.0)
```
in our env, it is 'EFI Network 8'
![](../4.10/imgs/2022-06-14-16-53-51.png)

select the item , and it will boot through pxe and install os

```bash
# [   21.067341] IPv6: ADDRCONF(NETDEV_CHANGE): enp3s0f1: link becomes ready

# on host, disable services
systemctl disable --now dhcpd httpd vsftpd tftp.socket tftp

# on bf2 console
# login using root / bluefield

nmcli con modify System\ eth0 con-name eth0
nmcli con modify eth0 ipv4.method manual ipv4.addresses 192.168.100.2/24 ipv4.gateway 192.168.100.1 ipv4.dns 172.21.1.1
nmcli con up eth0

nmcli con mod enp3s0f1 ipv4.method manual ipv4.addr 192.168.77.55/24 ipv4.gateway 192.168.77.9 ipv4.dns 192.168.77.11 
nmcli con up enp3s0f1

nmcli con down 'System eth0'

systemctl restart chronyd
chronyc sourcestats
chronyc makestep
hwclock -w

ethtool -i enp3s0f1
# driver: mlx5_core
# version: 4.18.0-425.3.1.el8.aarch64
# firmware-version: 24.35.2000 (MT_0000000539)
# expansion-rom-version:
# bus-info: 0000:03:00.1
# supports-statistics: yes
# supports-test: yes
# supports-eeprom-access: no
# supports-register-dump: no
# supports-priv-flags: yes

ethtool -i eth0
# driver: virtio_net
# version: 1.0.0
# firmware-version:
# expansion-rom-version:
# bus-info: virtio
# supports-statistics: yes
# supports-test: no
# supports-eeprom-access: no
# supports-register-dump: no
# supports-priv-flags: no

# login
ssh root@192.168.77.55
# bluefield

```

# install ocp cluster on kvm on 101 and 102

this is for hypershift host ocp

## create multi-arch release image

- https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.7/html/clusters/cluster_mce_overview#manual-release-image-cross-arch


```bash
# on vultr

# export BUILDNUMBER=4.12.9

# podman pull quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-x86_64
# podman pull quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-aarch64

# podman push quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-x86_64 quay.io/wangzheng422/ocp-release:$BUILDNUMBER-x86_64
# podman push quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-aarch64 quay.io/wangzheng422/ocp-release:$BUILDNUMBER-aarch64

# podman manifest create mymanifest
# podman manifest add mymanifest quay.io/wangzheng422/ocp-release:$BUILDNUMBER-x86_64
# podman manifest add mymanifest quay.io/wangzheng422/ocp-release:$BUILDNUMBER-aarch64

# podman manifest push mymanifest docker://quay.io/wangzheng422/ocp-release:$BUILDNUMBER

# on helper
export BUILDNUMBER=4.12.9

SEC_FILE='/data/pull-secret.json'

oc adm release mirror -a $SEC_FILE --keep-manifest-list=true \
  --from=quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi \
  --to=quaylab.infra.wzhlab.top:5443/ocp4/openshift4

# oc adm release mirror -a $SEC_FILE --keep-manifest-list=true \
#   --from=quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-x86_64 \
#   --to=quaylab.infra.wzhlab.top:5443/ocp4/openshift4

# oc adm release mirror -a $SEC_FILE --keep-manifest-list=true \
#   --from=quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-aarch64 \
#   --to=quaylab.infra.wzhlab.top:5443/ocp4/openshift4

# oc image mirror --keep-manifest-list=true quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi quaylab.infra.wzhlab.top:5443/ocp4/openshift4:$BUILDNUMBER-multi

# oc adm release info -a $SEC_FILE quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/arm64

# oc adm release info -a $SEC_FILE quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/amd64 --commit-urls

# get the binary
mkdir -p /data/ocp-$BUILDNUMBER-multi
cd /data/ocp-$BUILDNUMBER-multi

mkdir -p amd64
pushd amd64

# oc adm release extract -a $SEC_FILE --tools=true quay.io/openshift-release-dev/ocp-release:4.12.9-multi  --filter-by-os=linux/amd64

# try
mkdir -p ostree

VAR_IMAGE=`oc adm release info -a $SEC_FILE quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/amd64 --image-for=machine-os-images` 

oc image extract --path /:./ostree -a $SEC_FILE $VAR_IMAGE

/bin/cp -f ostree/coreos/coreos-*.iso ./
chmod +r *.iso

/bin/rm -rf ostree

oc adm release extract -a $SEC_FILE --command=oc quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/amd64
oc adm release extract -a $SEC_FILE --command=openshift-install quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/amd64
oc adm release extract -a $SEC_FILE --command=openshift-baremetal-install quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/amd64

popd

mkdir -p arm64
pushd arm64

mkdir -p ostree

VAR_IMAGE=`oc adm release info -a $SEC_FILE quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/arm64 --image-for=machine-os-images`

oc image extract --path /:./ostree -a $SEC_FILE $VAR_IMAGE --filter-by-os=linux/arm64 

/bin/cp -f ostree/coreos/coreos-*.iso ./
chmod +r *.iso

/bin/rm -rf ostree

oc adm release extract -a $SEC_FILE --command=oc  quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/arm64
oc adm release extract -a $SEC_FILE --command=openshift-install quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/arm64
# oc adm release extract -a $SEC_FILE --command=openshift-baremetal-install quay.io/openshift-release-dev/ocp-release:4.12.9-multi  --filter-by-os=linux/arm64

popd


# mkdir source

# oc adm release extract -a $SEC_FILE --git=source quay.io/openshift-release-dev/ocp-release:4.12.9-multi  --filter-by-os=linux/amd64

# on helper, as root
# get rootfs.img for arm
mount /data/ocp-$BUILDNUMBER-multi/arm64/coreos-aarch64.iso /mnt/
/bin/cp -f /mnt/images/pxeboot/rootfs.img  /data/ocp4/ocp-bf2-aarch64-rootfs.img
umount /mnt

```


## on helper node

终于所有的准备工作都做完了，我们开始在helper上面进行openshift的安装。在这之前，还有一个配置helper节点的步骤，主要是配置dns服务之类的，在这里就不重复了，有需要了解的，可以看[这里的文档](./4.11.helper.node.oc.mirror.md)

### get installer binary

我们先要从安装文件目录中，得到installer的二进制文件。

```bash

# switch to you install version

export BUILDNUMBER=4.12.9

pushd /data/ocp-${BUILDNUMBER}
tar -xzf openshift-client-linux-${BUILDNUMBER}.tar.gz -C /usr/local/bin/
tar -xzf openshift-install-linux-${BUILDNUMBER}.tar.gz -C /usr/local/bin/

install -m 755 /data/ocp4/clients/butane-amd64 /usr/local/bin/butane
install -m 755 /data/ocp4/clients/coreos-installer_amd64 /usr/local/bin/coreos-installer

popd

export BUILDNUMBER=4.12.9

pushd /data/ocp-${BUILDNUMBER}-multi/amd64

install oc /usr/local/bin/
install openshift-install /usr/local/bin/ 
install openshift-baremetal-install /usr/local/bin/

# install -m 755 /data/ocp4/clients/butane-amd64 /usr/local/bin/butane
# install -m 755 /data/ocp4/clients/coreos-installer_amd64 /usr/local/bin/coreos-installer

popd

openshift-install version
# openshift-install 4.12.9
# built from commit 15d8ffe8fa3a5ab2f36f7e0a54126c98733aaf25
# release image quay.io/openshift-release-dev/ocp-release@sha256:b79a218bdb20a80b0ae68eaf3e7c24ff8d5d2fecb71a84501e62d09f1095a705
# release architecture amd64

oc image info quay.io/openshift-release-dev/ocp-release@sha256:b79a218bdb20a80b0ae68eaf3e7c24ff8d5d2fecb71a84501e62d09f1095a705
  # OS            DIGEST
  # linux/amd64   sha256:a9497468740d289306e144ebc315164cfc457b9dbfb3d3b76c700149c3126866
  # linux/ppc64le sha256:5c3d9e3cfed3ec51159ea8b9e755cb8e9b3c27a06a0717ba753cb6a362b1fb5b
  # linux/s390x   sha256:ea2d15cfba1f27461783ef8b501685c0e6b2ddd7717f3720d34d3e4be1005860
  # linux/arm64   sha256:c1482f341d72097606f1e28c99159411d53891ccc63be24cdc14667990d6dac5

oc image info quay.io/openshift-release-dev/ocp-release:4.12.9-multi
  # OS            DIGEST
  # linux/amd64   sha256:a9497468740d289306e144ebc315164cfc457b9dbfb3d3b76c700149c3126866
  # linux/ppc64le sha256:5c3d9e3cfed3ec51159ea8b9e755cb8e9b3c27a06a0717ba753cb6a362b1fb5b
  # linux/s390x   sha256:ea2d15cfba1f27461783ef8b501685c0e6b2ddd7717f3720d34d3e4be1005860
  # linux/arm64   sha256:c1482f341d72097606f1e28c99159411d53891ccc63be24cdc14667990d6dac5


```

### create the install yaml

接下来我们创建安装配置文件。这里面最关键的就是那个yaml模板，我们在模板里面，启动IPI安装模式，并且配置3个master的redfish接口信息，并启用静态IP安装的方法，配置了静态IP信息。

安装配置yaml文件创建后，我们调用installer，把他们转化成ignition等真正的安装配置文件，并且和baremetal installer二进制文件一起，传递到物理机上。

这里面有2个二进制文件，一个是openshift installer，这个一般场景下，比如对接公有云，私有云，就够了，它会创建ignition文件，并且调用各种云的接口，创建虚拟机，开始安装。

但是如果是baremetal场景，有一个单独的baremetal installer二进制文件，它读取配置文件，调用物理机BMC接口信息，来开始安装，这个区别是目前openshift版本上的情况，不知道未来会不会有变化。

```bash
# create a user and create the cluster under the user

useradd -m 3nodeipi

usermod -aG wheel 3nodeipi

su - 3nodeipi

ssh-keygen

cat << EOF > ~/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

chmod 600 ~/.ssh/config

cat << 'EOF' >> ~/.bashrc

export BASE_DIR='/home/3nodeipi/'

EOF

export BASE_DIR='/home/3nodeipi/'
export BUILDNUMBER=4.12.9

mkdir -p ${BASE_DIR}/data/{sno/disconnected,install}

# set some parameter of you rcluster

NODE_SSH_KEY="$(cat ${BASE_DIR}/.ssh/id_rsa.pub)"
INSTALL_IMAGE_REGISTRY=quaylab.infra.wzhlab.top:5443

# PULL_SECRET='{"auths":{"registry.redhat.io": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"registry.ocp4.redhat.ren:5443": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"'${INSTALL_IMAGE_REGISTRY}'": {"auth": "'$( echo -n 'admin:redhatadmin' | openssl base64 )'","email": "noemail@localhost"}}}'
PULL_SECRET=$(cat /data/pull-secret.json)

NTP_SERVER=192.168.77.11
# HELP_SERVER=192.168.77.11
# KVM_HOST=192.168.77.11
# API_VIP=192.168.77.99
# INGRESS_VIP=192.168.77.98
# CLUSTER_PROVISION_IP=192.168.77.98
MACHINE_NETWORK='192.168.77.0/24'

# 定义单节点集群的节点信息
SNO_CLUSTER_NAME=acm-demo-one
SNO_BASE_DOMAIN=wzhlab.top

BOOTSTRAP_IP=192.168.77.22
MASTER_01_IP=192.168.77.23
# MASTER_02_IP=192.168.77.24
# MASTER_03_IP=192.168.77.25
# WORKER_01_IP=192.168.77.26

BOOTSTRAP_HOSTNAME=bootstrap-demo
MASTER_01_HOSTNAME=master-01-demo
# MASTER_02_HOSTNAME=master-02-demo
# MASTER_03_HOSTNAME=master-03-demo
# WORKER_01_HOSTNAME=worker-01-demo

BOOTSTRAP_INTERFACE=enp1s0
MASTER_01_INTERFACE=enp1s0
# MASTER_02_INTERFACE=enp1s0
# MASTER_03_INTERFACE=enp1s0
# WORKER_01_INTERFACE=enp1s0

# MASTER_01_INTERFACE_MAC=52:54:00:12:A1:01
# MASTER_02_INTERFACE_MAC=52:54:00:12:A1:02
# MASTER_03_INTERFACE_MAC=52:54:00:12:A1:03
# MASTER_03_INTERFACE_MAC=52:54:00:12:A1:03

BOOTSTRAP_DISK=/dev/vda
MASTER_01_DISK=/dev/vda
# MASTER_02_DISK=/dev/vda
# MASTER_03_DISK=/dev/vda
# WORKER_01_DISK=/dev/vda

OCP_GW=192.168.77.9
OCP_NETMASK=255.255.255.0
OCP_NETMASK_S=24
OCP_DNS=192.168.77.11

# echo ${SNO_IF_MAC} > /data/sno/sno.mac

mkdir -p ${BASE_DIR}/data/install
cd ${BASE_DIR}/data/install

/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap manifests master*[0-9] worker*[0-9] *

cat << EOF > ${BASE_DIR}/data/install/install-config.yaml 
apiVersion: v1
baseDomain: $SNO_BASE_DOMAIN
compute:
- name: worker
  replicas: 0 
controlPlane:
  name: master
  replicas: 1
metadata:
  name: $SNO_CLUSTER_NAME
networking:
  # OVNKubernetes , OpenShiftSDN
  clusterNetwork:
    - cidr: 172.21.0.0/16
      hostPrefix: 23
    # - cidr: fd02::/48
    #   hostPrefix: 64
  machineNetwork:
    - cidr: $MACHINE_NETWORK
    # - cidr: 2001:DB8::/32
  serviceNetwork:
    - 172.22.0.0/16
    # - fd03::/112
platform: 
  none: {}
pullSecret: '${PULL_SECRET}'
sshKey: |
$( cat ${BASE_DIR}/.ssh/id_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /etc/crts/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - ${INSTALL_IMAGE_REGISTRY}/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - ${INSTALL_IMAGE_REGISTRY}/ocp4/openshift4
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF


/bin/cp -f ${BASE_DIR}/data/install/install-config.yaml ${BASE_DIR}/data/install/install-config.yaml.bak

openshift-install --dir=${BASE_DIR}/data/install create manifests

# sudo bash -c "/bin/cp -f mirror/registries.conf /etc/containers/registries.conf.d/; chmod +r /etc/containers/registries.conf.d/*"

# /bin/cp -f  /data/ocp4/ansible-helper/files/* ${BASE_DIR}/data/install/openshift/

sudo bash -c "cd /data/ocp4 ; bash image.registries.conf.sh quaylab.infra.wzhlab.top:5443 ;"

/bin/cp -f /data/ocp4/99-worker-container-registries.yaml ${BASE_DIR}/data/install/openshift
/bin/cp -f /data/ocp4/99-master-container-registries.yaml ${BASE_DIR}/data/install/openshift

cd ${BASE_DIR}/data/install/

openshift-install --dir=${BASE_DIR}/data/install create ignition-configs 

# HTTP_PATH=http://192.168.7.11:8080/ignition


source /data/ocp4/acm.fn.sh

# 我们会创建一个wzh用户，密码是redhat，这个可以在第一次启动的是，从console/ssh直接用用户名口令登录
# 方便排错和研究
VAR_PWD_HASH="$(python3 -c 'import crypt,getpass; print(crypt.crypt("redhat"))')"

cat ${BASE_DIR}/data/install/bootstrap.ign \
  | jq --arg VAR "$VAR_PWD_HASH" --arg VAR_SSH "$NODE_SSH_KEY" '.passwd.users += [{ "name": "wzh", "system": true, "passwordHash": $VAR , "sshAuthorizedKeys": [ $VAR_SSH ], "groups": [ "adm", "wheel", "sudo", "systemd-journal"  ] }]' \
  | jq '. += { "kernel_arguments" : { "should_exist" : [ "systemd.debug-shell=1" ] } }' \
  | jq -c . \
  > ${BASE_DIR}/data/install/bootstrap-iso.ign

cat ${BASE_DIR}/data/install/master.ign \
  | jq --arg VAR "$VAR_PWD_HASH" --arg VAR_SSH "$NODE_SSH_KEY" '.passwd.users += [{ "name": "wzh", "system": true, "passwordHash": $VAR , "sshAuthorizedKeys": [ $VAR_SSH ], "groups": [ "adm", "wheel", "sudo", "systemd-journal"  ] }]' \
  | jq '. += { "kernel_arguments" : { "should_exist" : [ "systemd.debug-shell=1" ] } }' \
  | jq -c . \
  > ${BASE_DIR}/data/install/master-iso.ign



cd ${BASE_DIR}/data/install/
/bin/cp -f /data/ocp-$BUILDNUMBER-multi/amd64/coreos-x86_64.iso bootstrap.iso
# /bin/cp -f /data/ocp-4.10.43/rhcos-live.x86_64.iso bootstrap.iso
/bin/cp -f bootstrap.iso master01.iso
# /bin/cp -f bootstrap.iso master02.iso
# /bin/cp -f bootstrap.iso master03.iso
# sudo /bin/cp -f /data/work/ext-client/iso/rhcos-$VAR_IMAGE_VER-metal.x86_64.raw /data/dnf/
sudo /bin/cp -f ${BASE_DIR}/data/install/{bootstrap,master}-iso.ign /data/dnf/


# for ipv4 only
coreos-installer iso kargs modify -a "ip=$BOOTSTRAP_IP::$OCP_GW:$OCP_NETMASK:$BOOTSTRAP_HOSTNAME:$BOOTSTRAP_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=$BOOTSTRAP_DISK coreos.inst.ignition_url=http://192.168.77.11:5000/bootstrap-iso.ign  coreos.inst.insecure systemd.debug-shell=1 " bootstrap.iso

coreos-installer iso kargs modify -a "ip=$MASTER_01_IP::$OCP_GW:$OCP_NETMASK:$MASTER_01_HOSTNAME:$MASTER_01_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=$MASTER_01_DISK coreos.inst.ignition_url=http://192.168.77.11:5000/master-iso.ign  coreos.inst.insecure systemd.debug-shell=1 " master01.iso

# coreos-installer iso kargs modify -a "ip=$MASTER_02_IP::$OCP_GW:$OCP_NETMASK:$MASTER_02_HOSTNAME:$MASTER_02_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=$MASTER_02_DISK coreos.inst.ignition_url=http://192.168.77.11:5000/master-iso.ign  coreos.inst.insecure systemd.debug-shell=1 " master02.iso

# coreos-installer iso kargs modify -a "ip=$MASTER_03_IP::$OCP_GW:$OCP_NETMASK:$MASTER_03_HOSTNAME:$MASTER_03_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=$MASTER_03_DISK coreos.inst.ignition_url=http://192.168.77.11:5000/master-iso.ign  coreos.inst.insecure systemd.debug-shell=1 " master03.iso


```


### cleanup kvm ( 101 )

我们准备了脚本，来清理kvm，把物理机清理成一个干净的系统。

```bash

create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}

virsh destroy ocp4-acm-one-bootstrap
virsh undefine ocp4-acm-one-bootstrap

create_lv vgdata poolA lvacm-one-bootstrap 500G 
create_lv vgdata poolA lvacm-one-bootstrap-data 500G 

virsh destroy ocp4-acm-one-master-01
virsh undefine ocp4-acm-one-master-01

create_lv vgdata poolA lvacm-one-master-01 500G 
create_lv vgdata poolA lvacm-one-master-01-data 500G 

virsh destroy ocp4-acm-one-master-02
virsh undefine ocp4-acm-one-master-02

create_lv vgdata poolA lvacm-one-master-02 500G 
create_lv vgdata poolA lvacm-one-master-02-data 500G 

virsh destroy ocp4-acm-one-master-03
virsh undefine ocp4-acm-one-master-03

create_lv vgdata poolA lvacm-one-master-03 500G 
create_lv vgdata poolA lvacm-one-master-03-data 500G 



VAR_VM=`virsh list --all | grep bootstrap | awk '{print $2}'`
virsh destroy $VAR_VM
virsh undefine $VAR_VM
VAR_POOL=`virsh pool-list --all | grep bootstrap | awk '{print $1}'`
virsh pool-destroy $VAR_POOL
virsh pool-undefine $VAR_POOL
/bin/rm -rf /var/lib/libvirt/openshift-images/*
/bin/rm -rf /var/lib/libvirt/images/*



```

### start kvm ( 101 )


```bash

nmcli con mod baremetal +ipv4.address '192.168.77.101/24'
nmcli con mod baremetal ipv4.dns 192.168.77.11
nmcli con up baremetal

/bin/rm -rf /var/lib/libvirt/images/*


for var_file in bootstrap master01
do
  scp root@192.168.77.11:/home/3nodeipi/data/install/$var_file.iso /data/kvm/one/
done

create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}

virsh destroy ocp4-acm-one-bootstrap
virsh undefine ocp4-acm-one-bootstrap

create_lv vgdata poolA lvacm-one-bootstrap 500G recreate
create_lv vgdata poolA lvacm-one-bootstrap-data 500G recreate

SNO_MEM=50

virt-install --name=ocp4-acm-one-bootstrap --vcpus=16 --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --disk path=/dev/vgdata/lvacm-one-bootstrap,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lvacm-one-bootstrap-data,device=disk,bus=virtio,format=raw \
  --os-variant rhel8.3 --network bridge=baremetal,model=virtio \
  --graphics vnc,port=59002 --noautoconsole \
  --boot menu=on --cdrom /data/kvm/one/bootstrap.iso 

# --disk path=/dev/vgdata/lvacmhub-data,device=disk,bus=virtio,format=raw \
# --autoconsole text
# --graphics vnc,port=59002 \

# virsh console  ocp4-acm-one-bootstrap

virsh destroy ocp4-acm-one-master-01
virsh undefine ocp4-acm-one-master-01

create_lv vgdata poolA lvacm-one-master-01 500G recreate
create_lv vgdata poolA lvacm-one-master-01-data 500G recreate

virt-install --name=ocp4-acm-one-master-01 --vcpus=16 --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --disk path=/dev/vgdata/lvacm-one-master-01,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lvacm-one-master-01-data,device=disk,bus=virtio,format=raw \
  --os-variant rhel8.3 --network bridge=baremetal,model=virtio \
  --graphics vnc,port=59003 --noautoconsole \
  --boot menu=on --cdrom /data/kvm/one/master01.iso 

# virsh destroy ocp4-acm-one-master-02
# virsh undefine ocp4-acm-one-master-02

# create_lv vgdata poolA lvacm-one-master-02 500G recreate
# create_lv vgdata poolA lvacm-one-master-02-data 500G recreate

# virt-install --name=ocp4-acm-one-master-02 --vcpus=16 --ram=$(($SNO_MEM*1024)) \
#   --cpu=host-model \
#   --disk path=/dev/vgdata/lvacm-one-master-02,device=disk,bus=virtio,format=raw \
#   --disk path=/dev/vgdata/lvacm-one-master-02-data,device=disk,bus=virtio,format=raw \
#   --os-variant rhel8.3 --network bridge=baremetal,model=virtio \
#   --graphics vnc,port=59004 --noautoconsole \
#   --boot menu=on --cdrom /data/kvm/one/master02.iso 

# virsh destroy ocp4-acm-one-master-03
# virsh undefine ocp4-acm-one-master-03

# create_lv vgdata poolA lvacm-one-master-03 500G recreate
# create_lv vgdata poolA lvacm-one-master-03-data 500G recreate

# virt-install --name=ocp4-acm-one-master-03 --vcpus=16 --ram=$(($SNO_MEM*1024)) \
#   --cpu=host-model \
#   --disk path=/dev/vgdata/lvacm-one-master-03,device=disk,bus=virtio,format=raw \
#   --disk path=/dev/vgdata/lvacm-one-master-03-data,device=disk,bus=virtio,format=raw \
#   --os-variant rhel8.3 --network bridge=baremetal,model=virtio \
#   --graphics vnc,port=59005 --noautoconsole \
#   --boot menu=on --cdrom /data/kvm/one/master03.iso 


```



## on helper to see result

我们需要把物理机上的密钥文件等信息，传回helper节点。方便我们后续的操作。

```bash
# on helper node

cd ${BASE_DIR}/data/install
export KUBECONFIG=${BASE_DIR}/data/install/auth/kubeconfig
echo "export KUBECONFIG=${BASE_DIR}/data/install/auth/kubeconfig" >> ~/.bashrc
# oc completion bash | sudo tee /etc/bash_completion.d/openshift > /dev/null


cd ${BASE_DIR}/data/install
openshift-install wait-for install-complete --log-level debug
# INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/3nodeipi/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.acm-demo-one.wzhlab.top
# INFO Login to the console with user: "kubeadmin", and password: "dAdjK-grzQP-mgG2w-QLe29"
# INFO Time elapsed: 0s

# if you power off cluster for long time
# you will need to re-approve the csr
oc get csr | grep -v Approved
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve

# oc patch clusterversion/version --patch '{"spec":{"upstream":"https://multi.ocp.releases.ci.openshift.org/graph"}}' --type=merge

oc get clusterversions.config.openshift.io  -o yaml | grep Payload -A 2
      # message: Payload loaded version="4.12.9" image="quay.io/openshift-release-dev/ocp-release@sha256:a9497468740d289306e144ebc315164cfc457b9dbfb3d3b76c700149c3126866"
      #   architecture="multi"
      # reason: PayloadLoaded
      # status: "True"
      # type: ReleaseAccepted


```

## daily operation

### password login and oc config

```bash

# init setting for helper node
cat << EOF > ~/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF
chmod 600 ~/.ssh/config


cat > ${BASE_DIR}/data/install/crack.txt << EOF

echo redhat | sudo passwd --stdin root

sudo sed -i "s|^PasswordAuthentication no$|PasswordAuthentication yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^PermitRootLogin no$|PermitRootLogin yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^#ClientAliveInterval 180$|ClientAliveInterval 1800|g" /etc/ssh/sshd_config

sudo systemctl restart sshd

sudo sh -c 'echo "export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig" >> /root/.bashrc'

sudo sh -c 'echo "RET=\\\`oc config use-context system:admin\\\`" >> /root/.bashrc'

EOF

for i in 23
do
  ssh core@192.168.77.$i < ${BASE_DIR}/data/install/crack.txt
done

```

from other host

```bash
# https://unix.stackexchange.com/questions/230084/send-the-password-through-stdin-in-ssh-copy-id
dnf install -y sshpass

for i in 23
do
  sshpass -p 'redhat' ssh-copy-id root@192.168.77.$i
done

```

### power off / reboot

```bash

action=poweroff
# action=reboot
for i in 23 24 25
do
  ssh root@192.168.12.$i "$action"
done

virsh shutdown openwrt-factory
virsh shutdown openwrt-edge

```

### power on

```bash

virsh start openwrt-factory
virsh start openwrt-edge

for i in {1..3}
do
  virsh start ocp4-ipi-osp-master-0$i
done

```

### check info

我们日常还会有一些集群各个节点收集信息，脚本操作的工作，也提供脚本模板，帮助日常工作。

```bash

cat > ${BASE_DIR}/data/install/crack.txt << 'EOF'

for i in {3..8}
do
  nmcli con down enp${i}s0
  nmcli con del enp${i}s0
done

EOF

for i in 23 24 25
do
  ssh root@192.168.12.$i < ${BASE_DIR}/data/install/crack.txt
done

```


## deploy NFS on hub

```bash

# go to master-03, this is as storage node
# create the storage path
cat > ${BASE_DIR}/data/install/crack.txt << 'EOF'
mkdir -p /var/wzh-local-pv/
chcon -Rt container_file_t /var/wzh-local-pv/
EOF
ssh root@192.168.77.25 < ${BASE_DIR}/data/install/crack.txt

# on helper
cat << EOF > ${BASE_DIR}/data/install/local-pv.yaml
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-volume
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-local-pv
spec:
  capacity:
    storage: 400Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-volume
  local:
    path: /var/wzh-local-pv/
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - one-master-03.acm-demo-one.wzhlab.top
EOF
oc create --save-config -f ${BASE_DIR}/data/install/local-pv.yaml

# oc delete -f ${BASE_DIR}/data/install/local-pv.yaml

# move container image to quay.io
skopeo copy docker://k8s.gcr.io/sig-storage/nfs-provisioner:v3.0.0 docker://quay.io/wangzheng422/nfs-provisioner:v3.0.0

oc create ns nfs-system

# oc project nfs-system

cd ${BASE_DIR}/data/install

export http_proxy="http://127.0.0.1:18801"
export https_proxy=${http_proxy}

wget -O nfs.all.yaml https://raw.githubusercontent.com/wangzheng422/nfs-ganesha-server-and-external-provisioner/wzh/deploy/openshift/nfs.all.local.pv.wzhlab.yaml

unset http_proxy
unset https_proxy

/bin/cp -f nfs.all.yaml nfs.all.yaml.run

# sed -i 's/storageClassName: odf-lvm-vg1/storageClassName: local-volume/' nfs.all.yaml.run
sed -i 's/one-master-03.acm-demo-one.redhat.ren/one-master-03.acm-demo-one.wzhlab.top/' nfs.all.yaml.run
sed -i 's/storage: 5Gi/storage: 400Gi/' nfs.all.yaml.run
sed -i 's#k8s.gcr.io/sig-storage/nfs-provisioner:v3.0.0#quay.io/wangzheng422/nfs-provisioner:v3.0.0#' nfs.all.yaml.run

oc create --save-config -n nfs-system -f nfs.all.yaml.run

# oc delete -n nfs-system -f nfs.all.yaml.run

oc patch storageclass wzhlab-top-nfs -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

```

## add bf2 to sno cluster

### build bf2 iso

```bash
# on helper
export BUILDNUMBER=4.12.9
cd ${BASE_DIR}/data/install/

# /bin/cp -f /data/ocp-$BUILDNUMBER-multi/arm64/coreos-aarch64.iso ./ocp-bf2-aarch64.iso

oc extract -n openshift-machine-api secret/worker-user-data --keys=userData --to=- > bf2.ign

source /data/ocp4/acm.fn.sh

# 我们会创建一个wzh用户，密码是redhat，这个可以在第一次启动的是，从console/ssh直接用用户名口令登录
# 方便排错和研究
VAR_PWD_HASH="$(python3 -c 'import crypt,getpass; print(crypt.crypt("redhat"))')"

cat ${BASE_DIR}/data/install/bf2.ign \
  | jq --arg VAR "$VAR_PWD_HASH" --arg VAR_SSH "$NODE_SSH_KEY" '.passwd.users += [{ "name": "wzh", "system": true, "passwordHash": $VAR , "sshAuthorizedKeys": [ $VAR_SSH ], "groups": [ "adm", "wheel", "sudo", "systemd-journal"  ] }]' \
  | jq '. += { "kernel_arguments" : { "should_exist" : [ "systemd.debug-shell=1" ] } }' \
  | jq -c . \
  > ${BASE_DIR}/data/install/bf2-iso.ign

sudo bash -c "/bin/cp -f ${BASE_DIR}/data/install/bf2-iso.ign /data/dnf/bf2-iso.ign"
# coreos-installer iso ignition embed ocp-bf2-aarch64.iso --ignition-file bf2-iso.ign -f

# try to run on bf2
# on helper
# scp /data/ocp-$BUILDNUMBER-multi/arm64/coreos-aarch64.iso root@192.168.77.55:/root/bf2/ocp-bf2-aarch64.iso
# scp bf2.ign root@192.168.77.55:/root/bf2/

# on bf2
# sync the clock to hw, otherwise, the https cert will invalid
hwclock -w

# cd /root/bf2
# wget -O coreos-installer https://mirror.openshift.com/pub/openshift-v4/clients/coreos-installer/latest/coreos-installer_arm64
# chmod +x coreos-installer
# ./coreos-installer iso ignition embed ocp-bf2-aarch64.iso --ignition-file bf2.ign -f


# try x86
# cd ${BASE_DIR}/data/install/

# /bin/cp -f /data/ocp-$BUILDNUMBER/rhcos-live.x86_64.iso ./worker.iso

# oc extract -n openshift-machine-api secret/worker-user-data --keys=userData --to=- > bf2.ign

# sudo bash -c "/bin/cp -f bf2.ign /data/dnf/"

# coreos-installer iso kargs modify -a "ip=$WORKER_01_IP::$OCP_GW:$OCP_NETMASK:$WORKER_01_HOSTNAME:$WORKER_01_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=$WORKER_01_DISK coreos.inst.ignition_url=http://192.168.77.11:5000/bf2.ign  coreos.inst.insecure systemd.debug-shell=1 " worker.iso

```

### boot the bf2 with the iso

```bash
# copy iso to 103
# on 103
cd /data/down/
# scp root@192.168.77.11:/home/3nodeipi/data/install/ocp-bf2-aarch64.iso  ./
# scp root@192.168.77.11:/home/3nodeipi/down/ocp-bf2-aarch64-rootfs.img  ./
# scp root@192.168.77.11:/home/3nodeipi/data/install/worker.iso  ./

scp root@192.168.77.11:/data/ocp-4.12.9-multi/arm64/coreos-aarch64.iso ./ocp-bf2-aarch64.iso

# get the iso from bf2
# cd /data/down/
# scp root@192.168.77.55:/root/bf2/ocp-bf2-aarch64.iso ./

# python3 -m http.server 8080

export RHEL_ISO=ocp-bf2-aarch64.iso
# export BASE_ISO=Rocky-8.7-aarch64-minimal.iso
# shell come frome https://github.com/wangzheng422/rhel-on-bf2
bash bluefield_provision.sh -s
# === STATUS === Checking usability of SRIOV for PCI 0000:05:00.0
# SRIOV enabled
# EMBEDDED_CPU mode enabled
# === STATUS === Checking usability of SRIOV for PCI 0000:05:00.1
# SRIOV enabled
# EMBEDDED_CPU mode enabled

setenforce 0

# bash bluefield_provision.sh -p

iptables -F
bash ./PXE_setup_RHEL_install_over_mlx.aarch64.sh -i "${RHEL_ISO}" -p tmfifo 


# uplink_interface="$(ip route |grep ^default | sed 's/.*dev \([^ ]\+\).*/\1/')"
# test -n "${uplink_interface}" || die "need a default route"
    
# iptables -t nat -A POSTROUTING -o "${uplink_interface}" -j MASQUERADE

# on a new terminal
minicom --wrap --color on --baudrate 115200 --device /dev/rshim0/console

# change back to original terminal
# to reboot the bf2
echo BOOT_MODE 1 > /dev/rshim0/misc
echo SW_RESET 1 > /dev/rshim0/misc


# sleep 10
# ESC during bf2 boot, to go into bios config
# select boot from network 8
# before continue, restart nic and dhcpd on host
nmcli conn up tmfifo_net0
systemctl restart dhcpd

# iptables -t nat -A POSTROUTING -o "${uplink_interface}" -j MASQUERADE

# after booting
ssh core@192.168.77.55

# on helper
oc get csr | grep -v Approved
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve

oc get node -o wide
# NAME             STATUS     ROLES                         AGE     VERSION           INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                                                        KERNEL-VERSION                  CONTAINER-RUNTIME
# bf2-dpu          NotReady   worker                        2m15s   v1.25.7+eab9cc9   192.168.77.55   <none>        Red Hat Enterprise Linux CoreOS 412.86.202303211731-0 (Ootpa)   4.18.0-372.49.1.el8_6.aarch64   cri-o://1.25.2-10.rhaos4.12.git0a083f9.el8
# master-01-demo   Ready      control-plane,master,worker   28h     v1.25.7+eab9cc9   192.168.77.23   <none>        Red Hat Enterprise Linux CoreOS 412.86.202303211731-0 (Ootpa)   4.18.0-372.49.1.el8_6.x86_64    cri-o://1.25.2-10.rhaos4.12.git0a083f9.el8

oc get node -o wide --show-labels
# NAME             STATUS     ROLES                         AGE   VERSION           INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                                                        KERNEL-VERSION                  CONTAINER-RUNTIME                            LABELS
# bf2-dpu          NotReady   worker                        56s   v1.25.7+eab9cc9   192.168.77.55   <none>        Red Hat Enterprise Linux CoreOS 412.86.202303211731-0 (Ootpa)   4.18.0-372.49.1.el8_6.aarch64   cri-o://1.25.2-10.rhaos4.12.git0a083f9.el8   beta.kubernetes.io/arch=arm64,beta.kubernetes.io/os=linux,kubernetes.io/arch=arm64,kubernetes.io/hostname=bf2-dpu,kubernetes.io/os=linux,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
# master-01-demo   Ready      control-plane,master,worker   28h   v1.25.7+eab9cc9   192.168.77.23   <none>        Red Hat Enterprise Linux CoreOS 412.86.202303211731-0 (Ootpa)   4.18.0-372.49.1.el8_6.x86_64    cri-o://1.25.2-10.rhaos4.12.git0a083f9.el8   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master-01-demo,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos

lsblk
# NAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
# mmcblk0      179:0    0 58.3G  0 disk
# ├─mmcblk0p1  179:1    0    1M  0 part
# ├─mmcblk0p2  179:2    0  127M  0 part
# ├─mmcblk0p3  179:3    0  384M  0 part /boot
# └─mmcblk0p4  179:4    0 57.8G  0 part /sysroot
# mmcblk0boot0 179:8    0    4M  1 disk
# mmcblk0boot1 179:16   0    4M  1 disk

lscpu
# Architecture:        aarch64
# Byte Order:          Little Endian
# CPU(s):              8
# On-line CPU(s) list: 0-7
# Thread(s) per core:  1
# Core(s) per socket:  8
# Socket(s):           1
# NUMA node(s):        1
# Vendor ID:           ARM
# BIOS Vendor ID:      https://www.mellanox.com
# Model:               0
# Model name:          Cortex-A72
# BIOS Model name:     Mellanox BlueField-2 [A1] A72(D08) r1p0
# Stepping:            r1p0
# BogoMIPS:            400.00
# L1d cache:           32K
# L1i cache:           48K
# L2 cache:            1024K
# L3 cache:            6144K
# NUMA node0 CPU(s):   0-7
# Flags:               fp asimd evtstrm crc32 cpuid

free -h
#               total        used        free      shared  buff/cache   available
# Mem:           15Gi       1.6Gi        10Gi        80Mi       3.9Gi        11Gi
# Swap:            0B          0B          0B


```

![](imgs/2023-04-01-23-53-59.png)

![](imgs/2023-04-01-23-54-25.png)


# end

```bash


create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}

SNO_MEM=20
export KVM_DIRECTORY=/data/kvm

virsh destroy ocp4-ipi-osp-worker-01
virsh undefine ocp4-ipi-osp-worker-01

create_lv vgdata poolA lv-ocp4-ipi-osp-worker-01 500G recreate
create_lv vgdata poolA lv-ocp4-ipi-osp-worker-01-data 500G recreate


virt-install --name=ocp4-ipi-osp-worker-01 --vcpus=16 --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --disk path=/dev/vgdata/lv-ocp4-ipi-osp-worker-01,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lv-ocp4-ipi-osp-worker-01-data,device=disk,bus=virtio,format=raw \
  --os-variant rhel8.4 \
  --network bridge=baremetal,model=virtio,mac.address=52:54:00:12:A1:02 \
  --graphics vnc,port=59003 --noautoconsole \
  --boot menu=on --cdrom /data/down/worker.iso


```
