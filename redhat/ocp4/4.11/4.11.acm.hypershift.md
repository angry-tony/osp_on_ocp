# openshift4.11 acm with hypershift

本文介绍，在openshift4.11上，装 ACM 组件以后，然后通过hypershift的方式，来部署一个单节点openshift4.10的集群（SNO），在部署的过程中，我们模拟离线的网络环境，并且禁止DHCP，只用静态IP。

ZTP(zero touch provision)模式之所以诱人，是因为他只需要baremetal的bmc信息，以及网卡的mac地址，就可以完成集群的部署。MCE会创建一个iso，并调用bmc的api，去挂载这个iso并启动。

本次实验，整个流程如下：
1. 在openshift4上安装ACM组件，
2. 在ACM上配置cluster, infra env等配置。
3. MCE通过网络 redfish 协议启动kvm
4. kvm自动开始集群安装，但是由于kvm+redfish的限制，安装过程中的重启，需要手动停止kvm，配置由硬盘启动，然后再手动启动kvm。
5. 集群安装完成，保存集群登录信息

本次实验的部署架构图：

![](../4.10/dia/4.10.bm.ipi.sno.static.ip.drawio.svg)

本次实验的网络架构，和服务器,kvm部属架构，是依托之前的一个未完成的实验，[工厂模式](../4.10/4.10.factory.md)，虽然工厂模式实验的网络模型比较复杂，但是我们就不重复配置环境了。

参考资料：
- https://cloud.redhat.com/blog/how-to-build-bare-metal-hosted-clusters-on-red-hat-advanced-cluster-management-for-kubernetes
- https://cloud.redhat.com/blog/a-guide-to-red-hat-hypershift-on-bare-metal

<!-- 视频讲解

[<kbd><img src="../4.10/imgs/20220412112651.png" width="600"></kbd>](https://www.bilibili.com/video/bv1F3411n7tT)

- [bilibili](https://www.bilibili.com/video/bv1F3411n7tT)
- [youtube](https://youtu.be/tX2iozE2Rn0) -->

# 静态变量和 kvm 配置

根据factory的安装过程，我们弄了一个 3 node IPI 模式安装的 openshift， 是一个 ipi 的 compact cluster. 我们把这个集群作为hub集群，里面要装ACM组件。

以下的参数，是我们用这个hub集群，通过hypershift创建出来新集群的参数，新集群只有1个worker节点。

```bash
# on helper

# 做一些配置参数定义
INSTALL_IMAGE_REGISTRY=quaylab.infra.wzhlab.top:8443
# PULL_SECRET='{"auths":{"registry.redhat.io": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"registry.ocp4.redhat.ren:5443": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"'${INSTALL_IMAGE_REGISTRY}'": {"auth": "'$( echo -n 'admin:redhatadmin' | openssl base64 )'","email": "noemail@localhost"}}}'
PULL_SECRET=$(cat /data/pull-secret.json)

ACM_DEMO_CLUSTER=edge01

SNO_BASE_DOMAIN=wzhlab.top
SNO_IP=192.168.12.33
SNO_GW=192.168.12.1
SNO_NETMAST=255.255.255.0
SNO_NETMAST_S=24
SNO_HOSTNAME=edge-worker-01
SNO_IF=enp1s0
SNO_IF_MAC=52:54:00:20:a2:01
SNO_DNS=192.168.77.11
SNO_DISK=/dev/vda
SNO_CORE_PWD=redhat

```

另外，要说明的是，我们发现参考材料里面，对dns的配置不太对，至少对于单一worker节点来说，api, apps都指向这个worker节点就可以。

# 部署ACM

接下来，我们就部署ACM，我们用最简单的部署模式。

```bash
# install operator Advanced Cluster Management for Kubernetes

# https://docs.openshift.com/container-platform/4.9/scalability_and_performance/ztp-deploying-disconnected.html#enabling-assisted-installer-service-on-bare-metal_ztp-deploying-disconnected
# https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html/install/installing#installing-from-the-cli


cat << EOF > ${BASE_DIR}/data/install/acm.subscript.ns.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: open-cluster-management
EOF
oc create -f ${BASE_DIR}/data/install/acm.subscript.ns.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.subscript.yaml
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name:  open-cluster-management-wzh
  namespace: open-cluster-management
spec:
  targetNamespaces:
    - open-cluster-management
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: advanced-cluster-management
  namespace: open-cluster-management
spec:
  sourceNamespace: openshift-marketplace
  source: redhat-operators
  channel: release-2.6
  installPlanApproval: Automatic
  name: advanced-cluster-management
EOF

oc create -f ${BASE_DIR}/data/install/acm.subscript.yaml

# RHACM create the MultiClusterHub resource

cat << EOF > ${BASE_DIR}/data/install/acm.mch.mch.yaml
apiVersion: operator.open-cluster-management.io/v1
kind: MultiClusterHub
metadata:
  name: multiclusterhub
  namespace: open-cluster-management
spec: {}
EOF
oc create -f ${BASE_DIR}/data/install/acm.mch.mch.yaml

oc patch mce multiclusterengine --type=merge -p '{"spec":{"overrides":{"components":[{"name":"hypershift-preview","enabled": true}]}}}'

# wait here until you can see the local-cluster
oc get ManagedCluster -A
# NAME            HUB ACCEPTED   MANAGED CLUSTER URLS                  JOINED   AVAILABLE   AGE
# local-cluster   true           https://api.factory.wzhlab.top:6443   True     True        5h22m

# cat << EOF > ${BASE_DIR}/data/install/import-hub-kni20.yaml
# apiVersion: cluster.open-cluster-management.io/v1
# kind: ManagedCluster
# metadata:
#   labels:
#     local-cluster: "true"
#   name: local-cluster
# spec:
#   hubAcceptsClient: true
#   leaseDurationSeconds: 60
# EOF
# oc apply -f ${BASE_DIR}/data/install/import-hub-kni20.yaml

cat << EOF > ${BASE_DIR}/data/install/managed-cluster-addon.yaml
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ManagedClusterAddOn
metadata:
  name: hypershift-addon
  namespace: local-cluster
spec:
  installNamespace: open-cluster-management-agent-addon
EOF
oc create --save-config -f ${BASE_DIR}/data/install/managed-cluster-addon.yaml
# oc delete -f ${BASE_DIR}/data/install/managed-cluster-addon.yaml

oc get managedclusteraddons -A
# NAMESPACE       NAME                          AVAILABLE   DEGRADED   PROGRESSING
# local-cluster   application-manager           True
# local-cluster   cert-policy-controller        True
# local-cluster   cluster-proxy                 True
# local-cluster   config-policy-controller      True
# local-cluster   governance-policy-framework   True
# local-cluster   hypershift-addon              True
# local-cluster   iam-policy-controller         True
# local-cluster   work-manager                  True


```

装好了是这样：

![](imgs/2023-01-16-19-12-47.png)

我们可以通过webUI访问ACM： https://console-openshift-console.apps.factory.wzhlab.top/multicloud/infrastructure/clusters/managed

![](imgs/2023-01-16-19-55-49.png)

![](imgs/2023-01-16-19-56-31.png)

<!-- ![](imgs/2023-01-16-20-01-07.png) -->

![](imgs/2023-01-16-20-01-35.png)

![](imgs/2023-01-16-20-02-50.png)

![](imgs/2023-01-16-20-03-59.png)

![](imgs/2023-01-16-20-04-49.png)

其他的链接，都没有内容，页面是空的。

# 用hypershift模式部署只有一个worker的集群

有过部署assisted install service，并通过AIS来部署SNO的经验，那么通过ACM，用hypershift的模式来部署，就容易理解了，整个过程一样，都是配置ACM里面的assisted install service，然后定义infr env，调用BMC API，来直接挂载iso，并启动主机。不通的地方，之前是定义一个 ClusterDeployment, 现在定义一个 HostedCluster 。

## 命令行配置新集群

ACM 2.6 UI 是完全支持hypershift的，但是，我们现在的实验，是为了项目上能定制，所以有些配置要用命令行完成。

红帽官方文档上，还有另外一个做法，定义一个配置文件，然后使用工具，创建本实验里面手动创建的各种yaml。

本文就是手动创建yaml，然后一步一步的做，更深入的理解以下hypershift的过程。

```bash

oc project open-cluster-management
# oc project multicluster-engine

# # do not need, because now, it is acm 2.4.2
# # but it seems doesn't matter, if you enable it
# oc patch hiveconfig hive --type merge -p '{"spec":{"targetNamespace":"hive","logLevel":"debug","featureGates":{"custom":{"enabled":["AlphaAgentInstallStrategy"]},"featureSet":"Custom"}}}'

oc get hiveconfig hive -n multicluster-engine -o yaml
# ......
# spec: {}
# status:
#   aggregatorClientCAHash: b30ffa769079a2ac0e37e40172084089
#   conditions:
#   - lastProbeTime: "2023-01-13T09:10:10Z"
#     lastTransitionTime: "2023-01-13T09:10:10Z"
#     message: Hive is deployed successfully
#     reason: DeploymentSuccess
#     status: "True"
#     type: Ready
#   configApplied: true
#   observedGeneration: 1

oc patch provisioning provisioning-configuration --type merge -p '{"spec":{"watchAllNamespaces": true }}'

oc get provisioning provisioning-configuration -o yaml
# ......
# spec:
#   preProvisioningOSDownloadURLs: {}
#   provisioningMacAddresses:
#   - 52:54:00:20:a1:01
#   - 52:54:00:20:a1:02
#   - 52:54:00:20:a1:03
#   provisioningNetwork: Disabled
#   provisioningOSDownloadURL: http://192.168.77.11:8080/rhcos-openstack.x86_64.qcow2.gz?sha256=506bb66f8cb407c74061a8201f13e7b1edd44000d944be85eb7a4df7058dcb79
#   watchAllNamespaces: true
# ......

cat << EOF > ${BASE_DIR}/data/install/acm.ocp.release.yaml
apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  name: openshift-v4.11.21
  namespace: multicluster-engine
spec:
  releaseImage: ${INSTALL_IMAGE_REGISTRY}/openshift/release-images:4.11.21-x86_64
EOF
oc create -f ${BASE_DIR}/data/install/acm.ocp.release.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.ocp.release.yaml

oc get ClusterImageSet
# NAME                 RELEASE
# openshift-v4.11.21   quaylab.infra.wzhlab.top:8443/openshift/release-images:4.11.21-x86_64

cat << EOF > ${BASE_DIR}/data/install/acm.cm.asc.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: assisted-service-config
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  LOG_LEVEL: "debug"
EOF
oc create -f ${BASE_DIR}/data/install/acm.cm.asc.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.cm.asc.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: assisted-deployment-pull-secret
  namespace: multicluster-engine
stringData:
  .dockerconfigjson: '$PULL_SECRET'
EOF
oc create -f ${BASE_DIR}/data/install/acm.secret.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.secret.yaml

# oc get pod -A | grep metal3
# if the result is empty, then we will go in manual way
oc get pod -A | grep metal3
# openshift-machine-api                              metal3-8666f4cf4d-2bkfb                                           5/5     Running             0               5m21s
# openshift-machine-api                              metal3-image-cache-8jhtr                                          1/1     Running             0               74m
# openshift-machine-api                              metal3-image-cache-9jfs7                                          1/1     Running             0               74m
# openshift-machine-api                              metal3-image-cache-fl545                                          1/1     Running             0               74m
# openshift-machine-api                              metal3-image-customization-868d87999b-x2mnw                       1/1     Running             0               74m

openshift-install version
# openshift-install 4.11.21
# built from commit d3fb15afdbf1558344ea88a1e134c8e9a011440f
# release image quay.io/openshift-release-dev/ocp-release@sha256:860cc37824074671c4cf76e02d224d243e670d2298e6dab8923ee391fbd0ae1c
# release architecture amd64

openshift-install coreos print-stream-json | jq .architectures.x86_64.artifacts.metal.release -r
# 411.86.202210041459-0

VAR_COREOS_VERSION=`openshift-install coreos print-stream-json | jq .architectures.x86_64.artifacts.metal.release -r`

# the config of CA is important here.
# assisted service will not use cluster's CA config
cat << EOF > ${BASE_DIR}/data/install/acm.mirror.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hyper1-mirror-config
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  ca-bundle.crt: |
$( cat /etc/crts/infra.wzhlab.top.crt | sed 's/^/    /g' )
  registries.conf: |
    unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

    [[registry]]
      prefix = ""
      location = "quay.io/openshift-release-dev/ocp-release"
      mirror-by-digest-only = true

      [[registry.mirror]]
        location = "${INSTALL_IMAGE_REGISTRY}/openshift/release-images"

    [[registry]]
      prefix = ""
      location = "quay.io/openshift-release-dev/ocp-v4.0-art-dev"
      mirror-by-digest-only = true

      [[registry.mirror]]
        location = "${INSTALL_IMAGE_REGISTRY}/openshift/release"

---
EOF
oc create -f ${BASE_DIR}/data/install/acm.mirror.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.mirror.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.agentservicecofnig.yaml
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
  name: agent
  namespace: multicluster-engine
  ### This is the annotation that injects modifications in the Assisted Service pod
  annotations:
    unsupported.agent-install.openshift.io/assisted-service-configmap: "assisted-service-config"
###
spec:
  databaseStorage:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 40Gi
  filesystemStorage:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 40Gi
  ### This is a ConfigMap that only will make sense on Disconnected environments
  mirrorRegistryRef:
    name: "hyper1-mirror-config"
  ###
  osImages:
    - openshiftVersion: "4.11"
      version: "$VAR_COREOS_VERSION"
      url: "http://192.168.77.11:8080/rhcos-live.x86_64.iso"
      rootFSUrl: "http://192.168.77.11:8080/rhcos-live-rootfs.x86_64.img"
      cpuArchitecture: x86_64
EOF
oc create -f ${BASE_DIR}/data/install/acm.agentservicecofnig.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.agentservicecofnig.yaml

# oc get pod -n multicluster-engine -o json | jq .items[].metadata.name -r | xargs -I DEMO oc logs -n multicluster-engine --prefix=true DEMO | grep 'failed to add release image '

# wait here to see all the status is True
oc get AgentServiceConfig/agent -n multicluster-engine -o yaml  
# ......
# status:
#   conditions:
#   - lastTransitionTime: "2023-01-13T01:38:25Z"
#     message: AgentServiceConfig reconcile completed without error.
#     reason: ReconcileSucceeded
#     status: "True"
#     type: ReconcileCompleted
#   - lastTransitionTime: "2023-01-13T01:40:25Z"
#     message: All the deployments managed by Infrastructure-operator are healthy.
#     reason: DeploymentSucceeded
#     status: "True"
#     type: DeploymentsHealthy

# logs in infrastructure-operator 

# stop here, and wait the assisted-service pod run into ok status
oc get pod -n multicluster-engine | grep assisted
# assisted-image-service-0                               1/1     Running   0               4m38s
# assisted-service-764cd98cf7-2r2db                      2/2     Running   1 (2m59s ago)   4m40s

# begin to create new cluster

oc create ns ${ACM_DEMO_CLUSTER}
oc project ${ACM_DEMO_CLUSTER}

cat << EOF > ${BASE_DIR}/data/install/capi-role-${ACM_DEMO_CLUSTER}.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: capi-provider-role
  namespace: ${ACM_DEMO_CLUSTER}
rules:
- apiGroups:
  - agent-install.openshift.io
  resources:
  - agents
  verbs:
  - '*'
EOF
oc create --save-config -f ${BASE_DIR}/data/install/capi-role-${ACM_DEMO_CLUSTER}.yaml



# nodepool -> config -> config map -> machine config
cat << EOF > ${BASE_DIR}/data/install/hyper.mirror.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hyper-mirror-config
  namespace: ${ACM_DEMO_CLUSTER}
data:
  config: |
$( cat /data/ocp4/99-worker-container-registries.yaml | sed 's/^/    /g' )

---
EOF
oc create -f ${BASE_DIR}/data/install/hyper.mirror.yaml
# oc delete -f ${BASE_DIR}/data/install/hyper.mirror.yaml



cat << EOF > ${BASE_DIR}/data/install/hosted-cluster-${ACM_DEMO_CLUSTER}.yaml 
---
apiVersion: hypershift.openshift.io/v1alpha1
kind: HostedCluster
metadata:
  name: ${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
  labels:
    "cluster.open-cluster-management.io/clusterset": 'default'
spec:
  release:
    image: ${INSTALL_IMAGE_REGISTRY}/openshift/release-images:4.11.21-x86_64
  pullSecret:
    name: pullsecret-cluster-${ACM_DEMO_CLUSTER}
  sshKey:
    name: sshkey-cluster-${ACM_DEMO_CLUSTER}
  networking:
    podCIDR: 10.132.0.0/14
    serviceCIDR: 172.31.0.0/16
    machineCIDR: 192.168.12.0/24
    networkType: OpenShiftSDN
  platform:
    type: Agent
    agent:
      agentNamespace: ${ACM_DEMO_CLUSTER}
  infraID: ${ACM_DEMO_CLUSTER}
  dns:
    baseDomain: '$SNO_BASE_DOMAIN'
  services:
  - service: APIServer
    servicePublishingStrategy:
        nodePort:
          address: 192.168.12.23
          port: 30000
        type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      type: Route
  - service: OIDC
    servicePublishingStrategy:
      type: Route
  - service: Konnectivity
    servicePublishingStrategy:
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
---
apiVersion: v1
kind: Secret
metadata:
  name: pullsecret-cluster-${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
stringData:
  '.dockerconfigjson': '$PULL_SECRET'
type: kubernetes.io/dockerconfigjson
---
apiVersion: v1
kind: Secret
metadata:
  name: sshkey-cluster-${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
stringData:
  id_rsa.pub: '$(< ~/.ssh/id_rsa.pub)'
---
apiVersion: hypershift.openshift.io/v1alpha1
kind: NodePool
metadata:
  name: 'nodepool-${ACM_DEMO_CLUSTER}-01'
  namespace: ${ACM_DEMO_CLUSTER}
spec:
  clusterName: ${ACM_DEMO_CLUSTER}
  config:
    - name: hyper-mirror-config
  replicas: 1
  management:
    autoRepair: false
    upgradeType: InPlace
  platform:
    type: Agent
    agent:
      agentLabelSelector:
        matchLabels: {}
  release:
    image: ${INSTALL_IMAGE_REGISTRY}/openshift/release-images:4.11.21-x86_64
---
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  labels:
    cloud: hypershift
    name: ${ACM_DEMO_CLUSTER}
    cluster.open-cluster-management.io/clusterset: 'default'
  name: ${ACM_DEMO_CLUSTER}
spec:
  hubAcceptsClient: true
---
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: ${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
spec:
  clusterName: ${ACM_DEMO_CLUSTER}
  clusterNamespace: ${ACM_DEMO_CLUSTER}
  clusterLabels:
    cloud: ai-hypershift
  applicationManager:
    enabled: true
  policyController:
    enabled: true
  searchCollector:
    enabled: true
  certPolicyController:
    enabled: true
  iamPolicyController:
    enabled: true
EOF

oc create --save-config -f ${BASE_DIR}/data/install/hosted-cluster-${ACM_DEMO_CLUSTER}.yaml 
# oc delete -f ${BASE_DIR}/data/install/hosted-cluster-${ACM_DEMO_CLUSTER}.yaml 




cat << EOF > ${BASE_DIR}/data/install/acm.managed.secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: assisted-deployment-pull-secret
  namespace: ${ACM_DEMO_CLUSTER}
stringData:
  .dockerconfigjson: '$PULL_SECRET'
EOF
oc create -f ${BASE_DIR}/data/install/acm.managed.secret.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.managed.secret.yaml


cat << EOF > ${BASE_DIR}/data/install/acm.nmsc.yaml
apiVersion: agent-install.openshift.io/v1beta1
kind: NMStateConfig
metadata:
 name: ${ACM_DEMO_CLUSTER}
 namespace: ${ACM_DEMO_CLUSTER}
 labels:
   nmstate-conf-cluster-name: ${ACM_DEMO_CLUSTER}
spec:
 config:
   interfaces:
     - name: ${SNO_IF}
       type: ethernet
       state: up
       ipv4:
         enabled: true
         address:
           - ip: ${SNO_IP}
             prefix-length: ${SNO_NETMAST_S}
         dhcp: false
   dns-resolver:
     config:
       server:
         - ${SNO_DNS}
   routes:
     config:
       - destination: 0.0.0.0/0
         next-hop-address: ${SNO_GW}
         next-hop-interface: ${SNO_IF}
         table-id: 254
 interfaces:
   - name: "${SNO_IF}" 
     macAddress: ${SNO_IF_MAC}
EOF
oc create -f ${BASE_DIR}/data/install/acm.nmsc.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.nmsc.yaml

oc get NMStateConfig/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER}
# NAME     AGE
# edge01   3h30m





# cat << EOF > ${BASE_DIR}/data/install/acm.clusterdeployment.yaml
# apiVersion: hive.openshift.io/v1
# kind: ClusterDeployment
# metadata:
#   name: ${ACM_DEMO_CLUSTER}
#   namespace: ${ACM_DEMO_CLUSTER}
# spec:
#   baseDomain: ${SNO_BASE_DOMAIN}
#   clusterName: ${ACM_DEMO_CLUSTER}
#   controlPlaneConfig:
#     servingCertificates: {}
#   installed: false
#   clusterInstallRef:
#     group: extensions.hive.openshift.io
#     kind: AgentClusterInstall
#     name: ${ACM_DEMO_CLUSTER}
#     version: v1beta1
#   platform:
#     agentBareMetal:
#       agentSelector:
#         matchLabels:
#           cluster-name: "${ACM_DEMO_CLUSTER}"
#   pullSecretRef:
#     name: assisted-deployment-pull-secret
# EOF
# oc create -f ${BASE_DIR}/data/install/acm.clusterdeployment.yaml
# # oc delete -f ${BASE_DIR}/data/install/acm.clusterdeployment.yaml

# oc get ClusterDeployment/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o json | jq .status | head
# # {
# #   "conditions": [
# #     {
# #       "lastProbeTime": "2023-01-13T01:53:10Z",
# #       "lastTransitionTime": "2023-01-13T01:53:10Z",
# #       "message": "Platform credentials passed authentication check",
# #       "reason": "PlatformAuthSuccess",
# #       "status": "False",
# #       "type": "AuthenticationFailure"
# #     },

# # oc create configmap registry-config --from-file=quaylab.infra.wzhlab.top..8443=/etc/crts/infra.wzhlab.top.crt -n openshift-config

# # oc edit image.config.openshift.io cluster
# # spec:
# #   additionalTrustedCA:
# #     name: registry-config

# cat << EOF > ${BASE_DIR}/data/install/acm.agentclusterinstall.yaml
# apiVersion: extensions.hive.openshift.io/v1beta1
# kind: AgentClusterInstall
# metadata:
#   name: ${ACM_DEMO_CLUSTER}
#   namespace: ${ACM_DEMO_CLUSTER}
#   # Only include the annotation if using OVN, otherwise omit the annotation
# #   annotations:
# #     agent-install.openshift.io/install-config-overrides: '{"networking":{"networkType":"OVNKubernetes"}}'
# spec:
#   clusterDeploymentRef:
#     name: ${ACM_DEMO_CLUSTER}
#   imageSetRef:
#     name: openshift-v4.11.21
#   networking:
#     clusterNetwork:
#       - cidr: "10.128.0.0/14"
#         hostPrefix: 23
#     serviceNetwork:
#       - "172.30.0.0/16"
#     machineNetwork:
#       - cidr: "192.168.12.0/24"
#   provisionRequirements:
#     controlPlaneAgents: 1
#   sshPublicKey: "$(< ~/.ssh/id_rsa.pub)"
# EOF
# oc create -f ${BASE_DIR}/data/install/acm.agentclusterinstall.yaml
# # oc delete -f ${BASE_DIR}/data/install/acm.agentclusterinstall.yaml

# # wait a moment, and this will be ok
# oc get AgentClusterInstall/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o json | jq .status | head
# # {
# #   "conditions": [
# #     {
# #       "lastProbeTime": "2023-01-13T03:04:50Z",
# #       "lastTransitionTime": "2023-01-13T03:04:50Z",
# #       "message": "SyncOK",
# #       "reason": "SyncOK",
# #       "status": "True",
# #       "type": "SpecSynced"
# #     },

# cat << EOF > ${BASE_DIR}/data/install/acm.managedcluster.yaml
# apiVersion: cluster.open-cluster-management.io/v1
# kind: ManagedCluster
# metadata:
#   name: ${ACM_DEMO_CLUSTER}
# spec:
#   hubAcceptsClient: true
# EOF
# oc create -f ${BASE_DIR}/data/install/acm.managedcluster.yaml
# # oc delete -f ${BASE_DIR}/data/install/acm.managedcluster.yaml

# oc get ManagedCluster/${ACM_DEMO_CLUSTER} -o yaml
# # ......
# # status:
# #   capacity:
# #     core_worker: "0"
# #     socket_worker: "0"
# #   conditions:
# #   - lastTransitionTime: "2023-01-13T15:24:23Z"
# #     message: Accepted by hub cluster admin
# #     reason: HubClusterAdminAccepted
# #     status: "True"
# #     type: HubAcceptedManagedCluster
# #   version: {}

# # 我们是离线安装，所以要定制一下启动配置文件
# # generate the ignition





# cat << EOF > ${BASE_DIR}/data/sno/ign.base.json
# {
#   "ignition": {
#     "version": "3.1.0"
#   }
# }
# EOF

# cat << EOF > ${BASE_DIR}/data/sno/install.images.bu
# variant: openshift
# version: 4.9.0
# metadata:
#   labels:
#     machineconfiguration.openshift.io/role: master
#   name: 99-zzz-master-install-images
# storage:
#   files:
#     - path: /etc/containers/registries.conf.d/base.registries.conf
#       overwrite: true
#       contents:
#         inline: |
#           unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]
#           short-name-mode = ""

#           [[registry]]
#             prefix = ""
#             location = "quay.io/openshift-release-dev/ocp-release"
#             mirror-by-digest-only = true

#             [[registry.mirror]]
#               location = "${INSTALL_IMAGE_REGISTRY}/openshift/release-images"

#           [[registry]]
#             prefix = ""
#             location = "quay.io/openshift-release-dev/ocp-v4.0-art-dev"
#             mirror-by-digest-only = true

#             [[registry.mirror]]
#               location = "${INSTALL_IMAGE_REGISTRY}/openshift/release"

# EOF

# cat << EOF > ${BASE_DIR}/data/sno/install.crts.bu
# variant: openshift
# version: 4.9.0
# metadata:
#   labels:
#     machineconfiguration.openshift.io/role: master
#   name: 99-zzz-master-install-crts
# storage:
#   files:
#     - path: /etc/pki/ca-trust/source/anchors/quaylab.crt
#       overwrite: true
#       contents:
#         inline: |
# $( cat /etc/crts/infra.wzhlab.top.crt | sed 's/^/          /g' )

# EOF

# mkdir -p ${BASE_DIR}/data/sno/disconnected/
# # copy ntp related config
# /bin/cp -f  /data/ocp4/ansible-helper/files/* ${BASE_DIR}/data/sno/disconnected/

# # copy image registry proxy related config
# # cd /data/ocp4
# # bash image.registries.conf.sh nexus.infra.redhat.ren:8083
# sudo bash -c "cd /data/ocp4 ; bash image.registries.conf.quay.sh quaylab.infra.wzhlab.top:8443 ;"

# /bin/cp -f /data/ocp4/99-worker-container-registries.yaml ${BASE_DIR}/data/sno/disconnected/
# /bin/cp -f /data/ocp4/99-master-container-registries.yaml ${BASE_DIR}/data/sno/disconnected/

# cd ${BASE_DIR}/data/sno/
# # load ignition file generation function
# source /data/ocp4/acm.fn.sh


# get_file_content_for_ignition "/opt/openshift/openshift/99-master-chrony-configuration.yaml" "${BASE_DIR}/data/sno/disconnected/99-master-chrony-configuration.yaml"
# VAR_99_master_chrony=$RET_VAL
# VAR_99_master_chrony_2=$RET_VAL_2

# get_file_content_for_ignition "/opt/openshift/openshift/99-worker-chrony-configuration.yaml" "${BASE_DIR}/data/sno/disconnected/99-worker-chrony-configuration.yaml"
# VAR_99_worker_chrony=$RET_VAL
# VAR_99_worker_chrony_2=$RET_VAL_2

# get_file_content_for_ignition "/opt/openshift/openshift/99-master-container-registries.yaml" "${BASE_DIR}/data/sno/disconnected/99-master-container-registries.yaml"
# VAR_99_master_container_registries=$RET_VAL
# VAR_99_master_container_registries_2=$RET_VAL_2

# get_file_content_for_ignition "/opt/openshift/openshift/99-worker-container-registries.yaml" "${BASE_DIR}/data/sno/disconnected/99-worker-container-registries.yaml"
# VAR_99_worker_container_registries=$RET_VAL
# VAR_99_worker_container_registries_2=$RET_VAL_2

# butane ${BASE_DIR}/data/sno/install.images.bu > ${BASE_DIR}/data/sno/disconnected/99-zzz-master-install-images.yaml
# get_file_content_for_ignition "/opt/openshift/openshift/99-zzz-master-install-images.yaml" "${BASE_DIR}/data/sno/disconnected/99-zzz-master-install-images.yaml"
# VAR_99_master_install_images=$RET_VAL
# VAR_99_master_install_images_2=$RET_VAL_2

# butane ${BASE_DIR}/data/sno/install.crts.bu > ${BASE_DIR}/data/sno/disconnected/99-zzz-master-install-crts.yaml
# get_file_content_for_ignition "/opt/openshift/openshift/99-zzz-master-install-crts.yaml" "${BASE_DIR}/data/sno/disconnected/99-zzz-master-install-crts.yaml"
# VAR_99_master_install_crts=$RET_VAL
# VAR_99_master_install_crts_2=$RET_VAL_2

# # https://access.redhat.com/solutions/6194821
# # butane /data/sno/static.ip.bu | python3 -c 'import json, yaml, sys; print(json.dumps(yaml.load(sys.stdin)))'

# # https://stackoverflow.com/questions/2854655/command-to-escape-a-string-in-bash
# # VAR_PULL_SEC=`printf "%q" $(cat  /data/pull-secret.json)`

# # https://access.redhat.com/solutions/221403
# # VAR_PWD_HASH="$(openssl passwd -1 -salt 'openshift' 'redhat')"
# VAR_PWD_HASH="$(python3 -c 'import crypt,getpass; print(crypt.crypt("redhat"))')"

# tmppath=$(mktemp)
# cat ${BASE_DIR}/data/sno/ign.base.json \
#   | jq --arg VAR "$VAR_PWD_HASH" --arg VAR_SSH "$NODE_SSH_KEY" '.passwd.users += [{ "name": "wzh", "system": true, "passwordHash": $VAR , "sshAuthorizedKeys": [ $VAR_SSH ], "groups": [ "adm", "wheel", "sudo", "systemd-journal"  ] }]' \
#   | jq --argjson VAR "$VAR_99_master_chrony" '.storage.files += [$VAR] ' \
#   | jq --argjson VAR "$VAR_99_worker_chrony" '.storage.files += [$VAR] ' \
#   | jq --argjson VAR "$VAR_99_master_container_registries" '.storage.files += [$VAR] ' \
#   | jq --argjson VAR "$VAR_99_worker_container_registries" '.storage.files += [$VAR] ' \
#   | jq --argjson VAR "$VAR_99_master_chrony_2" '.storage.files += [$VAR] ' \
#   | jq --argjson VAR "$VAR_99_master_container_registries_2" '.storage.files += [$VAR] ' \
#   | jq --argjson VAR "$VAR_99_master_install_images_2" '.storage.files += [$VAR] ' \
#   | jq --argjson VAR "$VAR_99_master_install_crts_2" '.storage.files += [$VAR] ' \
#   | jq -c . \
#   > ${tmppath}
# VAR_IGNITION=$(cat ${tmppath})
# rm -f ${tmppath}


cat << EOF > ${BASE_DIR}/data/install/acm.infraenv.yaml
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: ${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
spec:
  additionalNTPSources:
    - 192.168.77.11
  # clusterRef:
  #   name: ${ACM_DEMO_CLUSTER}
  #   namespace: ${ACM_DEMO_CLUSTER}
  sshAuthorizedKey: "$(< ~/.ssh/id_rsa.pub)"
  pullSecretRef:
    name: assisted-deployment-pull-secret
  # ignitionConfigOverride: '${VAR_IGNITION}'
  nmStateConfigLabelSelector:
    matchLabels:
      nmstate-conf-cluster-name: ${ACM_DEMO_CLUSTER}
  # imageType: "full-iso"
EOF
oc create -f ${BASE_DIR}/data/install/acm.infraenv.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.infraenv.yaml

oc get infraenv/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o json | jq .status
# {
#   "agentLabelSelector": {
#     "matchLabels": {
#       "infraenvs.agent-install.openshift.io": "edge01"
#     }
#   },
#   "bootArtifacts": {
#     "initrd": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/images/c70485f3-0b12-437f-9efe-85b17f0c627f/pxe-initrd?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.rrkRFxLVcMjEw16W3brxl_YCxHtJtUu-h0KMHcvj3DO701_ZPUM6cDg765Q02CviGSNcSTmu0ic5g06AkU0Zzg&arch=x86_64&version=4.11",
#     "ipxeScript": "https://assisted-service-multicluster-engine.apps.factory.wzhlab.top/api/assisted-install/v2/infra-envs/c70485f3-0b12-437f-9efe-85b17f0c627f/downloads/files?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.3j_oKrmfOVQn85v2S3laLojUKaCTRqgkv_aSBPo-z_7k8-n2swb2m9aNT3uPr3CEstV4UVurkYwShtawFed0Cg&file_name=ipxe-script",
#     "kernel": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/boot-artifacts/kernel?arch=x86_64&version=4.11",
#     "rootfs": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/boot-artifacts/rootfs?arch=x86_64&version=4.11"
#   },
#   "conditions": [
#     {
#       "lastTransitionTime": "2023-01-13T03:15:17Z",
#       "message": "Image has been created",
#       "reason": "ImageCreated",
#       "status": "True",
#       "type": "ImageCreated"
#     }
#   ],
#   "createdTime": "2023-01-13T03:15:16Z",
#   "debugInfo": {
#     "eventsURL": "https://assisted-service-multicluster-engine.apps.factory.wzhlab.top/api/assisted-install/v2/events?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.W_KCQgx4SwgbErK6eiyh7EmxPb9L8KKawXLOWPgBoPxVPH79QXq5wb-X5DT48b6qBlk3xk-F7MCT_bEG1f30Ww&infra_env_id=c70485f3-0b12-437f-9efe-85b17f0c627f"
#   },
#   "isoDownloadURL": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/images/c70485f3-0b12-437f-9efe-85b17f0c627f?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.4FqFWSqfYijmGGWAKopqHIiKghDZBZ2NAqTY1hmUhwNfTzuKlFLZ2pDZAevAxtmf7aN96-6UCeNewIfqoLzPVQ&arch=x86_64&type=minimal-iso&version=4.11"
# }


# VAR_ISO=`oc get infraenv ${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o jsonpath={.status.isoDownloadURL}`

# cd /data/install/
# wget --no-check-certificate -O acm.demo1.iso $VAR_ISO

oc get pod -A | grep metal3
# openshift-machine-api                              metal3-8666f4cf4d-2bkfb                                           5/5     Running     5               12h
# openshift-machine-api                              metal3-image-cache-8jhtr                                          1/1     Running     1               13h
# openshift-machine-api                              metal3-image-cache-9jfs7                                          1/1     Running     1               13h
# openshift-machine-api                              metal3-image-cache-fl545                                          1/1     Running     1               13h
# openshift-machine-api                              metal3-image-customization-868d87999b-x2mnw                       1/1     Running     1               13h

# cd /data/ocp4/
# cat << 'EOF' > redfish.sh
# #!/usr/bin/env bash

# curl -k -s https://192.168.7.1:8000/redfish/v1/Systems/ | jq -r '.Members[]."@odata.id"' >  list

# while read -r line; do
#     curl -k -s https://192.168.7.1:8000/$line | jq -j '.Id, " ", .Name, "\n" '
# done < list

# EOF
# bash redfish.sh > /data/install/vm.list
# cat /data/install/vm.list
# # 075b17f7-9be9-4576-8d72-2ddd99909e19 ocp4-acm-demo1-master0
# # c991312a-26de-438d-8c2d-6aa6cd586bca ocp4-master0
# # e70f66bc-7878-4617-811d-89cdaf62cc8c ocp4-Helper

# oc patch provisioning provisioning-configuration --type merge -p '{"spec":{"watchAllNamespaces": true}}'

cat << EOF > ${BASE_DIR}/data/install/acm.demo.secret.bmc.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ${ACM_DEMO_CLUSTER}-bmc-master-01
  namespace: ${ACM_DEMO_CLUSTER}
data:
  password: $(echo password | base64)
  username: $(echo admin | base64)
type: Opaque
EOF
oc create -f ${BASE_DIR}/data/install/acm.demo.secret.bmc.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.demo.secret.bmc.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.demo.bmh.master.yaml
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: ${ACM_DEMO_CLUSTER}-master-01
  namespace: ${ACM_DEMO_CLUSTER}
  labels:
    infraenvs.agent-install.openshift.io: "${ACM_DEMO_CLUSTER}"
  annotations:
    ## Disable the Introspection
    inspect.metal3.io: disabled
    ## Set Static Hostname
    bmac.agent-install.openshift.io/hostname: "${SNO_HOSTNAME}"
    ## Set Static Role
    bmac.agent-install.openshift.io/role: "master"
spec:
  online: true
  bmc:
    address: redfish-virtualmedia://192.168.77.101:8000/redfish/v1/Systems/$(cat /data/install/vm.list.* | grep ocp4-ipi-edge-master-01 | awk '{print $1}')
    credentialsName: ${ACM_DEMO_CLUSTER}-bmc-master-01
    disableCertificateVerification: true
  bootMACAddress: $(cat /data/install/mac.list.* | grep ocp4-ipi-edge-master-01 | awk '{print $2}')
  automatedCleaningMode: disabled
EOF
oc create -f ${BASE_DIR}/data/install/acm.demo.bmh.master.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.demo.bmh.master.yaml

oc get agent
# NAME                                   CLUSTER   APPROVED   ROLE     STAGE
# a176e428-fea7-43ff-95c7-a927514227ed             true       master

oc get agent/a176e428-fea7-43ff-95c7-a927514227ed -o yaml
# ......
# spec:
spec:
  approved: true
  hostname: edge-worker-01
  role: master
# ......


oc extract -n ${ACM_DEMO_CLUSTER} secret/${ACM_DEMO_CLUSTER}-admin-kubeconfig --to=- > ${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER}

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get co
# NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
# console                                    4.11.21   True        False         False      6h22m
# csi-snapshot-controller                    4.11.21   True        False         False      6h24m
# dns                                        4.11.21   True        False         False      6h23m
# image-registry                             4.11.21   True        False         False      6h23m
# ingress                                    4.11.21   True        False         False      6h39m
# insights                                   4.11.21   True        False         False      6h25m
# kube-apiserver                             4.11.21   True        False         False      6h40m
# kube-controller-manager                    4.11.21   True        False         False      6h40m
# kube-scheduler                             4.11.21   True        False         False      6h40m
# kube-storage-version-migrator              4.11.21   True        False         False      6h24m
# monitoring                                 4.11.21   True        False         False      6h20m
# network                                    4.11.21   True        False         False      6h24m
# openshift-apiserver                        4.11.21   True        False         False      6h40m
# openshift-controller-manager               4.11.21   True        False         False      6h40m
# openshift-samples                          4.11.21   True        False         False      6h23m
# operator-lifecycle-manager                 4.11.21   True        False         False      6h40m
# operator-lifecycle-manager-catalog         4.11.21   True        False         False      6h40m
# operator-lifecycle-manager-packageserver   4.11.21   True        False         False      6h40m
# service-ca                                 4.11.21   True        False         False      6h25m
# storage                                    4.11.21   True        False         False      6h25m

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get node
# NAME             STATUS   ROLES    AGE     VERSION
# edge-master-01   Ready    worker   6h28m   v1.24.6+5658434

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get mcp
# error: the server doesn't have a resource type "mcp"

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get mc
# error: the server doesn't have a resource type "mc"

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get all -o wide -n openshift-ingress
# NAME                                 READY   STATUS    RESTARTS   AGE     IP              NODE             NOMINATED NODE   READINESS GATES
# pod/router-default-bb569f544-cknjw   1/1     Running   0          6h41m   192.168.12.33   edge-master-01   <none>           <none>

# NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE     SELECTOR
# service/router-internal-default   ClusterIP   172.31.152.115   <none>        80/TCP,443/TCP,1936/TCP   6h41m   ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default

# NAME                             READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                                                                                                   SELECTOR
# deployment.apps/router-default   1/1     1            1           6h41m   router       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0dc935b7825a800e32eac69fafa2d238e1d6eb2f344cdf29345cb1123c26a22   ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default

# NAME                                       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                                                                                                   SELECTOR
# replicaset.apps/router-default-bb569f544   1         1         1       6h41m   router       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0dc935b7825a800e32eac69fafa2d238e1d6eb2f344cdf29345cb1123c26a22   ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default,pod-template-hash=bb569f544


oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get pod -A | wc -l
# 56

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get clusterversion
# NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
# version   4.11.21   True        False         6h35m   Cluster version is 4.11.21


# to delete cluster


```

![](imgs/2023-01-16-11-40-30.png)

![](imgs/2023-01-16-11-43-31.png)

![](imgs/2023-01-16-11-42-55.png)

![](imgs/2023-01-16-11-44-58.png)

![](imgs/2023-01-16-12-33-28.png)

![](imgs/2023-01-16-12-33-45.png)

![](imgs/2023-01-16-12-34-02.png)



我们回到MCE的界面中，能从基础架构中，看到我们新创建的HOST了，能看到MCE正在通过redfish配置这个kvm

![](../4.11/imgs/2023-01-13-23-38-41.png)

这个bare metal host其实是调用的openshift4平台上的服务创建的，所以从openshift4的console上也能看得到：

![](../4.11/imgs/2023-01-13-23-31-32.png)

能从openshift4 console上看到这个bare metal host的详细信息：

![](../4.11/imgs/2023-01-13-23-32-56.png)

![](../4.11/imgs/2023-01-13-23-32-27.png)

回到ACM的界面中，我们能看到安装正在继续：

![](../4.11/imgs/2023-01-13-23-33-40.png)

从ACM的cluster界面中，我们能看到安装的详细进展情况：

![](../4.11/imgs/2023-01-13-23-34-09.png)

但是安装的中途，提示我们需要动手操作一下。这是因为我们是用kvm模拟的物理机，并且模拟了一个redfish，这个redfish功能比较简单，在安装ocp的过程中，kvm会重启，但是远程挂载的光盘没有卸载，所以我们需要卸载掉这个光驱，然后继续安装：

![](../4.11/imgs/2023-01-13-20-19-01.png)

![](../4.11/imgs/2023-01-13-20-19-34.png)

进入kvm的界面，调整一下启动顺序：
![](../4.10/imgs/20220408230353.png)  

然后重启kvm，等待一段时间，infra env就安装完成了。

![](../4.11/imgs/2023-01-13-20-22-49.png)

不过，cluster还在继续安装，我们安心等待安装过程完成。

![](../4.11/imgs/2023-01-13-20-21-47.png) 

终于完成啦。

## 安装完成

装好了以后，我们在MCE里面就能看到如下景象： https://console-openshift-console.apps.factory.wzhlab.top/multicloud/infrastructure/clusters/managed

![](../4.11/imgs/2023-01-13-21-19-58.png)


看cluster的详细信息，也正常了：

![](../4.11/imgs/2023-01-13-21-16-29.png)

⚠️一定记得，下载kubeconfig文件，还有密码

cluster的node tab也有内容了：

![](../4.11/imgs/2023-01-13-21-52-50.png)

cluster的add-on也有了组件，这两个组件是干什么用的呢？作者暂时说不确切，以后查到资料再分析吧。

![](imgs/2023-01-13-21-53-25.png)

infra env也绿色状态了 https://console-openshift-console.apps.factory.wzhlab.top/multicloud/infrastructure/environments

![](../4.11/imgs/2023-01-13-22-45-47.png)

详细信息和原来一样：

![](../4.11/imgs/2023-01-13-22-46-13.png)

hosts tab 也完成了

![](../4.11/imgs/2023-01-13-22-46-41.png)

## post operation

装完了，我们为了方便做实验，我们对集群节点做点配置。虽然减低了集群的安全性，但是做实验吗，无所谓了。

```bash

# on helper

# VAR_CLUSTER=edge01
# oc get secret/$VAR_CLUSTER-keypair -n $VAR_CLUSTER --template='{{index .data "id_rsa.key" | base64decode}}' > ${BASE_DIR}/data/install/edge.key

# chmod 600 ${BASE_DIR}/data/install/edge.key

# ssh -i ${BASE_DIR}/data/install/edge.key core@192.168.12.33

cat > ${BASE_DIR}/data/install/crack.txt << EOF

echo redhat | sudo passwd --stdin root

sudo sed -i "s|^PasswordAuthentication no$|PasswordAuthentication yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^PermitRootLogin no$|PermitRootLogin yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^#ClientAliveInterval 180$|ClientAliveInterval 1800|g" /etc/ssh/sshd_config

sudo systemctl restart sshd

sudo sh -c 'echo "export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig" >> /root/.bashrc'

sudo sh -c 'echo "RET=\\\`oc config use-context system:admin\\\`" >> /root/.bashrc'

EOF

for i in 33
do
  ssh core@192.168.12.$i < ${BASE_DIR}/data/install/crack.txt
done


for i in 33
do
  sshpass -p 'redhat' ssh-copy-id root@192.168.12.$i
done


ssh root@192.168.12.33

```

# check dhcp existed

我们是静态IP安装，那么就要确认一下环境里面是不是真的 DHCP 给关了，检查的方法如下。

https://superuser.com/questions/750359/check-if-a-dhcp-server-existing-in-my-network-using-bash

```bash
dnf install nmap -y
nmap --script broadcast-dhcp6-discover -e enp1s0

```

# end

```bash

# revert the order
tac << EOF 
oc delete -f /data/install/acm.ocp.release.yaml
oc delete -f /data/install/acm.cm.asc.yaml
oc delete -f /data/install/acm.secret.yaml
oc delete -f /data/install/acm.mirror.yaml
oc delete -f /data/install/acm.agentservicecofnig.yaml
oc delete -f /data/install/acm.managed.secret.yaml
oc delete -f /data/install/acm.agentclusterinstall.yaml
oc delete -f /data/install/acm.nmsc.yaml
oc delete -f /data/install/acm.clusterdeployment.yaml
oc delete -f /data/install/acm.klusterletaddonconfig.yaml
oc delete -f /data/install/acm.managedcluster.yaml
oc delete -f /data/install/acm.infraenv.yaml
EOF
oc delete -f /data/install/acm.infraenv.yaml
oc delete -f /data/install/acm.managedcluster.yaml
oc delete -f /data/install/acm.klusterletaddonconfig.yaml
oc delete -f /data/install/acm.clusterdeployment.yaml
oc delete -f /data/install/acm.nmsc.yaml
oc delete -f /data/install/acm.agentclusterinstall.yaml
oc delete -f /data/install/acm.managed.secret.yaml
oc delete -f /data/install/acm.agentservicecofnig.yaml
oc delete -f /data/install/acm.mirror.yaml
oc delete -f /data/install/acm.secret.yaml
oc delete -f /data/install/acm.cm.asc.yaml
oc delete -f /data/install/acm.ocp.release.yaml


```
