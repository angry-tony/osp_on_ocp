# openshift4.11 acm with hypershift

本文介绍，在openshift4.11上，装 ACM 组件以后，然后通过hypershift的方式，来部署一个单worker节点openshift4.11的控制面托管的集群，在部署的过程中，我们模拟离线的网络环境，并且禁止DHCP，只用静态IP。

控制面托管（hypershift)模式，之所以诱人，是因为他能够让控制面变成一个namespace，然后托管到中心控制面集群上，这样就能把多个集群的控制面集中到一个中心集群上，能大大提高master节点的计算密度，节约master节点的成本。并且能够把集群master节点的运行维护工作，交给专业团队运维的控制面集群，作为最终用户，只要关心worker节点的运行和维护，而worker节点的运行维护相对来说，是非常简单的。

对比SNO，compact cluster这种master/worker混合部署的方案，hypershift通过剥离控制面业务负载，到中心集群，防止work load对master的不利影响，比如用户部署了一个UPF这种极度消耗CPU的应用，就会无意间影响master，从而让整个集群垮掉。而hypershift就从方案层面，避免了这种情况。而从中心集群的角度来说，他的业务负载种类比较单一，就能刚好的有针对性的优化和运维。

本次实验，整个流程如下：
1. 在openshift4上安装ACM组件，
2. 在ACM上配置cluster, infra env等配置。
3. MCE通过网络 redfish 协议启动kvm
4. kvm自动开始集群安装，但是由于kvm+redfish的限制，安装过程中的重启，需要手动停止kvm，配置由硬盘启动，然后再手动启动kvm。
5. 集群安装完成，保存集群登录信息

本次实验的部署架构图：

![](../4.10/dia/4.10.bm.ipi.sno.static.ip.drawio.svg)

本次实验的网络架构，和服务器,kvm部属架构，是依托之前的一个未完成的实验，[工厂模式](../4.10/4.10.factory.md)，虽然工厂模式实验的网络模型比较复杂，但是我们就不重复配置环境了。

参考资料：
- https://cloud.redhat.com/blog/how-to-build-bare-metal-hosted-clusters-on-red-hat-advanced-cluster-management-for-kubernetes
- https://cloud.redhat.com/blog/a-guide-to-red-hat-hypershift-on-bare-metal


<!-- 视频讲解

[<kbd><img src="../4.10/imgs/20220412112651.png" width="600"></kbd>](https://www.bilibili.com/video/bv1F3411n7tT)

- [bilibili](https://www.bilibili.com/video/bv1F3411n7tT)
- [youtube](https://youtu.be/tX2iozE2Rn0) -->


```bash

function ocgetall {
  for i in $(oc api-resources --verbs=list --namespaced -o name | grep -v "events.events.k8s.io" | grep -v "events" | sort | uniq); do
    echo "Resource:" $i
    oc -n ${1} get --ignore-not-found ${i}
  done
}

ocgetall ${ACM_DEMO_CLUSTER}

ocgetall ${ACM_DEMO_CLUSTER}-${ACM_DEMO_CLUSTER}


oc get infraenv/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o yaml 
# metadata:
#   labels:
#     agentclusterinstalls.extensions.hive.openshift.io/location: edge01
#     networkType: static
# spec:
#   additionalNTPSources:
#   - 192.168.77.11
#   agentLabels:
#     agentclusterinstalls.extensions.hive.openshift.io/location: edge01
#   cpuArchitecture: x86_64
#   ipxeScriptType: DiscoveryImageAlways
#   nmStateConfigLabelSelector:
#     matchLabels:
#       infraenvs.agent-install.openshift.io: edge01
#   pullSecretRef:
#     name: pullsecret-edge01
#   sshAuthorizedKey: ssh-rsa ....



apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: edge01
  namespace: edge01
  labels:
    agentclusterinstalls.extensions.hive.openshift.io/location: edge01
    networkType: static
spec:
  agentLabels:
    'agentclusterinstalls.extensions.hive.openshift.io/location': edge01
  pullSecretRef:
    name: pullsecret-edge01
  sshAuthorizedKey: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzQKhV/F8VIXufYgrYg335P019IExW1IKgh3TRyOxh89dmQ6GAJOMLm8oI5Gdx0/4LrYxICuLxXykk321Lf/K8bunXmchEYciO3L2KObq4e8CNVSQoEd6YmUe80roj/mbvbR6LvFGOcv+bdxrq8KO4+nXUpE/+oTFD2hYpEoBcoO8POKY+y8/6ZUh9bIf+fso+mtxlrxVBFYbD48FawQEX2M/h0TxBsCw12165XMGlnWVVWc0ynmgU74eELeWGisBe6jRXeLUPHQC7SM1RR5++4cKVAZqWiXGnF2g/1J/JkU9wc+puT0tDhXtC6XENxBa4/02wJDbH9xRCPB9cZmxqS/PPOkNqFrQGcKpAx2bP4dsL8cn2d/ignXAhvxqaPiyv4Qkz/aAxmGnpT87Bs+V4ib/CKeaF+UXMGizj6ng5TOr3+IzRH6bJYpqoCu6OSPKMHnVHqgsGkEXvNXDH/SoMDb+vGkSMp7FoEz96MCD+REZ5eqvlYQ/Z2+x6N5pffaM= 3nodeipi@ocp4-helper
  additionalNTPSources:
    - 192.168.77.11
  nmStateConfigLabelSelector:	
      matchLabels:	
        infraenvs.agent-install.openshift.io: edge01
status:
  agentLabelSelector:
    matchLabels:
      'agentclusterinstalls.extensions.hive.openshift.io/location': edge01
---
kind: Secret
apiVersion: v1
metadata:
  name: pullsecret-edge01
  namespace: edge01
data:
  '.dockerconfigjson': '....'
type: 'kubernetes.io/dockerconfigjson'
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: capi-provider-role
  namespace: edge01
rules:
  - verbs:
      - '*'
    apiGroups:
      - agent-install.openshift.io
    resources:
      - agents




---
apiVersion: hypershift.openshift.io/v1alpha1
kind: HostedCluster
metadata:
  name: 'edge01'
  namespace: 'edge01'
  labels:
    "cluster.open-cluster-management.io/clusterset": 'default'
spec:
  release:
    image: quaylab.infra.wzhlab.top:8443/openshift/release-images:4.11.21-x86_64
  pullSecret:
    name: pullsecret-cluster-edge01
  sshKey:
    name: sshkey-cluster-edge01
  networking:
    podCIDR: 10.132.0.0/14
    serviceCIDR: 172.31.0.0/16
    machineCIDR: 192.168.12.0/24
  platform:
    type: Agent
    agent:
      agentNamespace: 'edge01'
  infraID: 'edge01'
  dns:
    baseDomain: 'wzhlab.top'
  services:
  - service: APIServer
    servicePublishingStrategy:
      type: NodePort
      nodePort:
        address: 192.168.12.23
        port: 30000
  - service: OAuthServer
    servicePublishingStrategy:
      type: Route
  - service: OIDC
    servicePublishingStrategy:
      type: Route
  - service: Konnectivity
    servicePublishingStrategy:
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
---
apiVersion: v1
kind: Secret
metadata:
  name: pullsecret-cluster-edge01
  namespace: edge01
data:
  '.dockerconfigjson': ....
type: kubernetes.io/dockerconfigjson
---
apiVersion: v1
kind: Secret
metadata:
  name: sshkey-cluster-edge01
  namespace: 'edge01'
stringData:
  id_rsa.pub: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCzQKhV/F8VIXufYgrYg335P019IExW1IKgh3TRyOxh89dmQ6GAJOMLm8oI5Gdx0/4LrYxICuLxXykk321Lf/K8bunXmchEYciO3L2KObq4e8CNVSQoEd6YmUe80roj/mbvbR6LvFGOcv+bdxrq8KO4+nXUpE/+oTFD2hYpEoBcoO8POKY+y8/6ZUh9bIf+fso+mtxlrxVBFYbD48FawQEX2M/h0TxBsCw12165XMGlnWVVWc0ynmgU74eELeWGisBe6jRXeLUPHQC7SM1RR5++4cKVAZqWiXGnF2g/1J/JkU9wc+puT0tDhXtC6XENxBa4/02wJDbH9xRCPB9cZmxqS/PPOkNqFrQGcKpAx2bP4dsL8cn2d/ignXAhvxqaPiyv4Qkz/aAxmGnpT87Bs+V4ib/CKeaF+UXMGizj6ng5TOr3+IzRH6bJYpqoCu6OSPKMHnVHqgsGkEXvNXDH/SoMDb+vGkSMp7FoEz96MCD+REZ5eqvlYQ/Z2+x6N5pffaM= 3nodeipi@ocp4-helper
---
apiVersion: hypershift.openshift.io/v1alpha1
kind: NodePool
metadata:
  name: 'nodepool-edge01-1'
  namespace: 'edge01'
spec:
  clusterName: 'edge01'
  replicas: 1
  management:
    autoRepair: false
    upgradeType: InPlace
  platform:
    type: Agent
    agent:
      agentLabelSelector:
        matchLabels: {}
  release:
    image: quaylab.infra.wzhlab.top:8443/openshift/release-images:4.11.21-x86_64
---
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  labels:
    cloud: hypershift
    name: 'edge01'
    cluster.open-cluster-management.io/clusterset: 'default'
  name: 'edge01'
spec:
  hubAcceptsClient: true
---
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: 'edge01'
  namespace: 'edge01'
spec:
  clusterName: 'edge01'
  clusterNamespace: 'edge01'
  clusterLabels:
    cloud: ai-hypershift
  applicationManager:
    enabled: true
  policyController:
    enabled: true
  searchCollector:
    enabled: true
  certPolicyController:
    enabled: true
  iamPolicyController:
    enabled: true
---

```





# 静态变量和 kvm 配置

根据factory的安装过程，我们弄了一个 3 node IPI 模式安装的 openshift， 是一个 ipi 的 compact cluster. 我们把这个集群作为hub集群，里面要装ACM组件。

以下的参数，是我们用这个hub集群，通过hypershift创建出来新集群的参数，新集群只有1个worker节点。

```bash
# on helper

# 做一些配置参数定义
INSTALL_IMAGE_REGISTRY=quaylab.infra.wzhlab.top:8443
# PULL_SECRET='{"auths":{"registry.redhat.io": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"registry.ocp4.redhat.ren:5443": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"'${INSTALL_IMAGE_REGISTRY}'": {"auth": "'$( echo -n 'admin:redhatadmin' | openssl base64 )'","email": "noemail@localhost"}}}'
PULL_SECRET=$(cat /data/pull-secret.json)

ACM_DEMO_CLUSTER=edge01

SNO_BASE_DOMAIN=wzhlab.top
SNO_IP=192.168.12.33
SNO_GW=192.168.12.1
SNO_NETMAST=255.255.255.0
SNO_NETMAST_S=24
SNO_HOSTNAME=edge-worker-01
SNO_IF=enp1s0
SNO_IF_MAC=52:54:00:20:a2:01
SNO_DNS=192.168.77.11
SNO_DISK=/dev/vda
SNO_CORE_PWD=redhat

```

另外，要说明的是，我们发现参考材料里面，对dns的配置不太对，至少对于单一worker节点来说，api, apps都指向这个worker节点就可以。

# 部署ACM

接下来，我们就部署ACM，我们用最简单的部署模式。

```bash
# install operator Advanced Cluster Management for Kubernetes

cat << EOF > ${BASE_DIR}/data/install/acm.subscript.ns.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: open-cluster-management
EOF
oc create -f ${BASE_DIR}/data/install/acm.subscript.ns.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.subscript.yaml
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name:  open-cluster-management-wzh
  namespace: open-cluster-management
spec:
  targetNamespaces:
    - open-cluster-management
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: advanced-cluster-management
  namespace: open-cluster-management
spec:
  sourceNamespace: openshift-marketplace
  source: redhat-operators
  channel: release-2.6
  installPlanApproval: Automatic
  name: advanced-cluster-management
EOF

oc create -f ${BASE_DIR}/data/install/acm.subscript.yaml

# RHACM create the MultiClusterHub resource

cat << EOF > ${BASE_DIR}/data/install/acm.mch.mch.yaml
apiVersion: operator.open-cluster-management.io/v1
kind: MultiClusterHub
metadata:
  name: multiclusterhub
  namespace: open-cluster-management
spec: {}
EOF
oc create -f ${BASE_DIR}/data/install/acm.mch.mch.yaml

oc patch mce multiclusterengine --type=merge -p '{"spec":{"overrides":{"components":[{"name":"hypershift-preview","enabled": true}]}}}'

# wait here until you can see the local-cluster
oc get ManagedCluster -A
# NAME            HUB ACCEPTED   MANAGED CLUSTER URLS                  JOINED   AVAILABLE   AGE
# local-cluster   true           https://api.factory.wzhlab.top:6443   True     True        5h22m

cat << EOF > ${BASE_DIR}/data/install/managed-cluster-addon.yaml
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ManagedClusterAddOn
metadata:
  name: hypershift-addon
  namespace: local-cluster
spec:
  installNamespace: open-cluster-management-agent-addon
EOF
oc create --save-config -f ${BASE_DIR}/data/install/managed-cluster-addon.yaml
# oc delete -f ${BASE_DIR}/data/install/managed-cluster-addon.yaml

oc get managedclusteraddons -A
# NAMESPACE       NAME                          AVAILABLE   DEGRADED   PROGRESSING
# local-cluster   application-manager           True
# local-cluster   cert-policy-controller        True
# local-cluster   cluster-proxy                 True
# local-cluster   config-policy-controller      True
# local-cluster   governance-policy-framework   True
# local-cluster   hypershift-addon              True
# local-cluster   iam-policy-controller         True
# local-cluster   work-manager                  True


```

装好了是这样：

![](imgs/2023-01-16-19-12-47.png)

我们可以通过webUI访问ACM： https://console-openshift-console.apps.factory.wzhlab.top/multicloud/infrastructure/clusters/managed

![](imgs/2023-01-16-19-55-49.png)

![](imgs/2023-01-16-19-56-31.png)

<!-- ![](imgs/2023-01-16-20-01-07.png) -->

![](imgs/2023-01-16-20-01-35.png)

![](imgs/2023-01-16-20-02-50.png)

![](imgs/2023-01-16-20-03-59.png)

![](imgs/2023-01-16-20-04-49.png)

其他的链接，都没有内容，页面是空的。

# 用hypershift模式部署只有一个worker的集群

有过部署assisted install service，并通过AIS来部署SNO的经验，那么通过ACM，用hypershift的模式来部署，就容易理解了，整个过程一样，都是配置ACM里面的assisted install service，然后定义infr env，调用BMC API，来直接挂载iso，并启动主机。不同的地方，之前是定义一个 ClusterDeployment, 现在定义一个 HostedCluster 。

## setup ACM for cluster deploy

ACM 2.6 UI 是完全支持hypershift的，但是，我们现在的实验，是为了项目上能定制，所以有些配置要用命令行完成。

红帽官方文档上，还有另外一个做法，定义一个配置文件，然后使用工具，创建本实验里面手动创建的各种yaml。

本文就是手动创建yaml，然后一步一步的做，更深入的理解以下hypershift的过程。

```bash

oc project open-cluster-management

oc get hiveconfig hive -n multicluster-engine -o yaml
# ......
# spec: {}
# status:
#   aggregatorClientCAHash: b30ffa769079a2ac0e37e40172084089
#   conditions:
#   - lastProbeTime: "2023-01-13T09:10:10Z"
#     lastTransitionTime: "2023-01-13T09:10:10Z"
#     message: Hive is deployed successfully
#     reason: DeploymentSuccess
#     status: "True"
#     type: Ready
#   configApplied: true
#   observedGeneration: 1

oc patch provisioning provisioning-configuration --type merge -p '{"spec":{"watchAllNamespaces": true }}'

oc get provisioning provisioning-configuration -o yaml
# ......
# spec:
#   preProvisioningOSDownloadURLs: {}
#   provisioningMacAddresses:
#   - 52:54:00:20:a1:01
#   - 52:54:00:20:a1:02
#   - 52:54:00:20:a1:03
#   provisioningNetwork: Disabled
#   provisioningOSDownloadURL: http://192.168.77.11:8080/rhcos-openstack.x86_64.qcow2.gz?sha256=506bb66f8cb407c74061a8201f13e7b1edd44000d944be85eb7a4df7058dcb79
#   watchAllNamespaces: true
# ......

cat << EOF > ${BASE_DIR}/data/install/acm.ocp.release.yaml
apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  name: openshift-v4.11.21
  namespace: multicluster-engine
spec:
  releaseImage: ${INSTALL_IMAGE_REGISTRY}/openshift/release-images:4.11.21-x86_64
EOF
oc create -f ${BASE_DIR}/data/install/acm.ocp.release.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.ocp.release.yaml

oc get ClusterImageSet
# NAME                 RELEASE
# openshift-v4.11.21   quaylab.infra.wzhlab.top:8443/openshift/release-images:4.11.21-x86_64

cat << EOF > ${BASE_DIR}/data/install/acm.cm.asc.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: assisted-service-config
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  LOG_LEVEL: "debug"
EOF
oc create -f ${BASE_DIR}/data/install/acm.cm.asc.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.cm.asc.yaml

# cat << EOF > ${BASE_DIR}/data/install/acm.secret.yaml
# apiVersion: v1
# kind: Secret
# metadata:
#   name: assisted-deployment-pull-secret
#   namespace: multicluster-engine
# stringData:
#   .dockerconfigjson: '$PULL_SECRET'
# EOF
# oc create -f ${BASE_DIR}/data/install/acm.secret.yaml
# # oc delete -f ${BASE_DIR}/data/install/acm.secret.yaml

openshift-install version
# openshift-install 4.11.21
# built from commit d3fb15afdbf1558344ea88a1e134c8e9a011440f
# release image quay.io/openshift-release-dev/ocp-release@sha256:860cc37824074671c4cf76e02d224d243e670d2298e6dab8923ee391fbd0ae1c
# release architecture amd64

openshift-install coreos print-stream-json | jq .architectures.x86_64.artifacts.metal.release -r
# 411.86.202210041459-0

VAR_COREOS_VERSION=`openshift-install coreos print-stream-json | jq .architectures.x86_64.artifacts.metal.release -r`

# # the config of CA is important here.
# # assisted service will not use cluster's CA config
cat << EOF > ${BASE_DIR}/data/install/acm.mirror.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hyper1-mirror-config
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  ca-bundle.crt: |
$( cat /etc/crts/infra.wzhlab.top.crt | sed 's/^/    /g' )
  registries.conf: |
    unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

    [[registry]]
      prefix = ""
      location = "quay.io/openshift-release-dev/ocp-release"
      mirror-by-digest-only = true

      [[registry.mirror]]
        location = "${INSTALL_IMAGE_REGISTRY}/openshift/release-images"

    [[registry]]
      prefix = ""
      location = "quay.io/openshift-release-dev/ocp-v4.0-art-dev"
      mirror-by-digest-only = true

      [[registry.mirror]]
        location = "${INSTALL_IMAGE_REGISTRY}/openshift/release"

---
EOF
oc create -f ${BASE_DIR}/data/install/acm.mirror.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.mirror.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.agentservicecofnig.yaml
apiVersion: agent-install.openshift.io/v1beta1
kind: AgentServiceConfig
metadata:
  name: agent
  namespace: multicluster-engine
  ### This is the annotation that injects modifications in the Assisted Service pod
  annotations:
    unsupported.agent-install.openshift.io/assisted-service-configmap: "assisted-service-config"
###
spec:
  databaseStorage:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 40Gi
  filesystemStorage:
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 40Gi
  ### This is a ConfigMap that only will make sense on Disconnected environments
  mirrorRegistryRef:
    name: "hyper1-mirror-config"
  ###
  osImages:
    - openshiftVersion: "4.11"
      version: "$VAR_COREOS_VERSION"
      url: "http://192.168.77.11:8080/rhcos-live.x86_64.iso"
      rootFSUrl: "http://192.168.77.11:8080/rhcos-live-rootfs.x86_64.img"
      cpuArchitecture: x86_64
EOF
oc create -f ${BASE_DIR}/data/install/acm.agentservicecofnig.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.agentservicecofnig.yaml

# oc get pod -n multicluster-engine -o json | jq .items[].metadata.name -r | xargs -I DEMO oc logs -n multicluster-engine --prefix=true DEMO | grep 'failed to add release image '

# wait here to see all the status is True
oc get AgentServiceConfig/agent -n multicluster-engine -o yaml  
# ......
# status:
#   conditions:
#   - lastTransitionTime: "2023-01-13T01:38:25Z"
#     message: AgentServiceConfig reconcile completed without error.
#     reason: ReconcileSucceeded
#     status: "True"
#     type: ReconcileCompleted
#   - lastTransitionTime: "2023-01-13T01:40:25Z"
#     message: All the deployments managed by Infrastructure-operator are healthy.
#     reason: DeploymentSucceeded
#     status: "True"
#     type: DeploymentsHealthy

# stop here, and wait the assisted-service pod run into ok status
oc get pod -n multicluster-engine | grep assisted
# assisted-image-service-0                               1/1     Running   0               4m38s
# assisted-service-764cd98cf7-2r2db                      2/2     Running   1 (2m59s ago)   4m40s
```

## begin to create new cluster - control plan

准备工作都做好了，我们开始创建一个hypershift管理的，托管控制面的新集群。

```bash
oc create ns ${ACM_DEMO_CLUSTER}
oc project ${ACM_DEMO_CLUSTER}

cat << EOF > ${BASE_DIR}/data/install/capi-role-${ACM_DEMO_CLUSTER}.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: capi-provider-role
  namespace: ${ACM_DEMO_CLUSTER}
rules:
- apiGroups:
  - agent-install.openshift.io
  resources:
  - agents
  verbs:
  - '*'
EOF
oc create --save-config -f ${BASE_DIR}/data/install/capi-role-${ACM_DEMO_CLUSTER}.yaml



# nodepool -> config -> config map -> machine config
# we have container image cache, so we add customize config through machine config
cat << EOF > ${BASE_DIR}/data/install/hyper.mirror.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hyper-mirror-config
  namespace: ${ACM_DEMO_CLUSTER}
data:
  config: |
$( cat /data/ocp4/99-worker-container-registries.yaml | sed 's/^/    /g' )

---
EOF
oc create -f ${BASE_DIR}/data/install/hyper.mirror.yaml
# oc delete -f ${BASE_DIR}/data/install/hyper.mirror.yaml



cat << EOF > ${BASE_DIR}/data/install/hosted-cluster-${ACM_DEMO_CLUSTER}.yaml 
---
apiVersion: hypershift.openshift.io/v1alpha1
kind: HostedCluster
metadata:
  name: ${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
  labels:
    "cluster.open-cluster-management.io/clusterset": 'default'
spec:
  release:
    image: ${INSTALL_IMAGE_REGISTRY}/openshift/release-images:4.11.21-x86_64
  pullSecret:
    name: pullsecret-cluster-${ACM_DEMO_CLUSTER}
  sshKey:
    name: sshkey-cluster-${ACM_DEMO_CLUSTER}
  networking:
    podCIDR: 10.132.0.0/14
    serviceCIDR: 172.31.0.0/16
    machineCIDR: 192.168.12.0/24
    networkType: OpenShiftSDN
  platform:
    type: Agent
    agent:
      agentNamespace: ${ACM_DEMO_CLUSTER}
  infraID: ${ACM_DEMO_CLUSTER}
  dns:
    baseDomain: '$SNO_BASE_DOMAIN'
  services:
  - service: APIServer
    servicePublishingStrategy:
        nodePort:
          address: 192.168.12.23
          port: 30000
        type: NodePort
  - service: OAuthServer
    servicePublishingStrategy:
      type: Route
  - service: OIDC
    servicePublishingStrategy:
      type: Route
  - service: Konnectivity
    servicePublishingStrategy:
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
---
apiVersion: v1
kind: Secret
metadata:
  name: pullsecret-cluster-${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
stringData:
  '.dockerconfigjson': '$PULL_SECRET'
type: kubernetes.io/dockerconfigjson
---
apiVersion: v1
kind: Secret
metadata:
  name: sshkey-cluster-${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
stringData:
  id_rsa.pub: '$(< ~/.ssh/id_rsa.pub)'
---
apiVersion: hypershift.openshift.io/v1alpha1
kind: NodePool
metadata:
  name: 'nodepool-${ACM_DEMO_CLUSTER}-01'
  namespace: ${ACM_DEMO_CLUSTER}
spec:
  clusterName: ${ACM_DEMO_CLUSTER}
  config:
    - name: hyper-mirror-config
  replicas: 1
  management:
    autoRepair: false
    upgradeType: InPlace
  platform:
    type: Agent
    agent:
      agentLabelSelector:
        matchLabels: {}
  release:
    image: ${INSTALL_IMAGE_REGISTRY}/openshift/release-images:4.11.21-x86_64
---
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  labels:
    cloud: hypershift
    name: ${ACM_DEMO_CLUSTER}
    cluster.open-cluster-management.io/clusterset: 'default'
  name: ${ACM_DEMO_CLUSTER}
spec:
  hubAcceptsClient: true
---
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: ${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
spec:
  clusterName: ${ACM_DEMO_CLUSTER}
  clusterNamespace: ${ACM_DEMO_CLUSTER}
  clusterLabels:
    cloud: ai-hypershift
  applicationManager:
    enabled: true
  policyController:
    enabled: true
  searchCollector:
    enabled: true
  certPolicyController:
    enabled: true
  iamPolicyController:
    enabled: true
EOF

oc create --save-config -f ${BASE_DIR}/data/install/hosted-cluster-${ACM_DEMO_CLUSTER}.yaml 
# oc delete -f ${BASE_DIR}/data/install/hosted-cluster-${ACM_DEMO_CLUSTER}.yaml 

oc get HostedCluster -A
# NAMESPACE   NAME     VERSION   KUBECONFIG                PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
# edge01      edge01             edge01-admin-kubeconfig   Partial    True        False         The hosted control plane is available

oc get HostedCluster/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o yaml

# wait here, and check the control plan creation.
oc get pod -n ${ACM_DEMO_CLUSTER}-${ACM_DEMO_CLUSTER}
# NAME                                             READY   STATUS    RESTARTS   AGE
# capi-provider-87b88465c-zgrx2                    1/1     Running   0          10m
# catalog-operator-7dcf86576f-vffl6                2/2     Running   0          7m33s
# certified-operators-catalog-7b4bdcb679-25gls     1/1     Running   0          7m39s
# cluster-api-5984dc678b-46ms7                     1/1     Running   0          10m
# cluster-autoscaler-5cd6b96d55-nzw4x              1/1     Running   0          9m33s
# cluster-network-operator-547f6988f4-6q2f2        1/1     Running   0          7m49s
# cluster-policy-controller-857bf8594f-9dhhj       1/1     Running   0          7m56s
# cluster-version-operator-85f5fd968f-rhchm        1/1     Running   0          7m55s
# community-operators-catalog-f6d797bc-87f9k       1/1     Running   0          7m38s
# control-plane-operator-65444fdff8-fzhvb          1/1     Running   0          10m
# etcd-0                                           1/1     Running   0          9m36s
# hosted-cluster-config-operator-cb8bd76f7-wvtfl   1/1     Running   0          7m41s
# ignition-server-57fbf98b8b-wvkv2                 1/1     Running   0          9m26s
# ingress-operator-594bdd5d6d-2t6kw                2/2     Running   0          7m46s
# konnectivity-agent-67bd878b88-bwxcp              1/1     Running   0          9m35s
# konnectivity-server-764ffdb8fd-xgxqq             1/1     Running   0          9m36s
# kube-apiserver-7f85bd5d7f-cvd7r                  3/3     Running   0          9m34s
# kube-controller-manager-7bd7ff884f-2c4jr         1/1     Running   0          6m35s
# kube-scheduler-68858b678d-jlpmx                  1/1     Running   0          8m30s
# machine-approver-c6b6f6ff8-jh445                 1/1     Running   0          9m33s
# oauth-openshift-5bb59d5596-55mtw                 2/2     Running   0          6m15s
# olm-operator-949f6f76b-r8kkz                     2/2     Running   0          7m32s
# openshift-apiserver-5ddbbd9847-n2824             2/2     Running   0          6m35s
# openshift-controller-manager-7cdd5bcc7b-p7kfb    1/1     Running   0          7m56s
# openshift-oauth-apiserver-8c76cb9b9-t9nts        1/1     Running   0          7m58s
# packageserver-58d5b997b9-wdn58                   2/2     Running   0          7m32s
# redhat-marketplace-catalog-85748dc79-tl8sr       1/1     Running   0          7m38s
# redhat-operators-catalog-74849cb9d6-9bg49        1/1     Running   0          7m38s


oc get pod -n ${ACM_DEMO_CLUSTER}-${ACM_DEMO_CLUSTER} | tail -n +2 | wc -l
# 28

```

![](imgs/2023-01-17-19-25-03.png)

![](imgs/2023-01-17-19-24-12.png)

## add worker node to control plan


### create the infra env

```bash

cat << EOF > ${BASE_DIR}/data/install/acm.managed.secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: assisted-deployment-pull-secret
  namespace: ${ACM_DEMO_CLUSTER}
stringData:
  .dockerconfigjson: '$PULL_SECRET'
EOF
oc create -f ${BASE_DIR}/data/install/acm.managed.secret.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.managed.secret.yaml


cat << EOF > ${BASE_DIR}/data/install/acm.nmsc.yaml
apiVersion: agent-install.openshift.io/v1beta1
kind: NMStateConfig
metadata:
 name: ${ACM_DEMO_CLUSTER}
 namespace: ${ACM_DEMO_CLUSTER}
 labels:
   nmstate-conf-cluster-name: ${ACM_DEMO_CLUSTER}
spec:
 config:
   interfaces:
     - name: ${SNO_IF}
       type: ethernet
       state: up
       ipv4:
         enabled: true
         address:
           - ip: ${SNO_IP}
             prefix-length: ${SNO_NETMAST_S}
         dhcp: false
   dns-resolver:
     config:
       server:
         - ${SNO_DNS}
   routes:
     config:
       - destination: 0.0.0.0/0
         next-hop-address: ${SNO_GW}
         next-hop-interface: ${SNO_IF}
         table-id: 254
 interfaces:
   - name: "${SNO_IF}" 
     macAddress: ${SNO_IF_MAC}
EOF
oc create -f ${BASE_DIR}/data/install/acm.nmsc.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.nmsc.yaml

oc get NMStateConfig/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER}
# NAME     AGE
# edge01   3h30m

oc get clusterdeployment -A
# NAMESPACE       NAME     INFRAID                                PLATFORM          REGION   VERSION   CLUSTERTYPE   PROVISIONSTATUS   POWERSTATE   AGE
# edge01-edge01   edge01   39d863f0-57f8-4ff4-a2b5-61e3e654c4db   agent-baremetal            4.11.21                 Provisioned                    122m

oc get clusterdeployment/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER}-${ACM_DEMO_CLUSTER} -o yaml 

oc get AgentClusterInstall -A
# NAMESPACE       NAME     CLUSTER   STATE
# edge01-edge01   edge01   edge01    adding-hosts

oc get AgentClusterInstall/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER}-${ACM_DEMO_CLUSTER} -o yaml 

cat << EOF > ${BASE_DIR}/data/install/acm.infraenv.yaml
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: ${ACM_DEMO_CLUSTER}
  namespace: ${ACM_DEMO_CLUSTER}
spec:
  additionalNTPSources:
    - 192.168.77.11
  # clusterRef:
  #   name: ${ACM_DEMO_CLUSTER}
  #   namespace: ${ACM_DEMO_CLUSTER}-${ACM_DEMO_CLUSTER}
  sshAuthorizedKey: "$(< ~/.ssh/id_rsa.pub)"
  pullSecretRef:
    name: assisted-deployment-pull-secret
  # ignitionConfigOverride: '${VAR_IGNITION}'
  nmStateConfigLabelSelector:
    matchLabels:
      nmstate-conf-cluster-name: ${ACM_DEMO_CLUSTER}
  # imageType: "full-iso"
EOF
oc create -f ${BASE_DIR}/data/install/acm.infraenv.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.infraenv.yaml

oc get infraenv/${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o json | jq .status
# {
#   "agentLabelSelector": {
#     "matchLabels": {
#       "infraenvs.agent-install.openshift.io": "edge01"
#     }
#   },
#   "bootArtifacts": {
#     "initrd": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/images/c70485f3-0b12-437f-9efe-85b17f0c627f/pxe-initrd?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.rrkRFxLVcMjEw16W3brxl_YCxHtJtUu-h0KMHcvj3DO701_ZPUM6cDg765Q02CviGSNcSTmu0ic5g06AkU0Zzg&arch=x86_64&version=4.11",
#     "ipxeScript": "https://assisted-service-multicluster-engine.apps.factory.wzhlab.top/api/assisted-install/v2/infra-envs/c70485f3-0b12-437f-9efe-85b17f0c627f/downloads/files?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.3j_oKrmfOVQn85v2S3laLojUKaCTRqgkv_aSBPo-z_7k8-n2swb2m9aNT3uPr3CEstV4UVurkYwShtawFed0Cg&file_name=ipxe-script",
#     "kernel": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/boot-artifacts/kernel?arch=x86_64&version=4.11",
#     "rootfs": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/boot-artifacts/rootfs?arch=x86_64&version=4.11"
#   },
#   "conditions": [
#     {
#       "lastTransitionTime": "2023-01-13T03:15:17Z",
#       "message": "Image has been created",
#       "reason": "ImageCreated",
#       "status": "True",
#       "type": "ImageCreated"
#     }
#   ],
#   "createdTime": "2023-01-13T03:15:16Z",
#   "debugInfo": {
#     "eventsURL": "https://assisted-service-multicluster-engine.apps.factory.wzhlab.top/api/assisted-install/v2/events?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.W_KCQgx4SwgbErK6eiyh7EmxPb9L8KKawXLOWPgBoPxVPH79QXq5wb-X5DT48b6qBlk3xk-F7MCT_bEG1f30Ww&infra_env_id=c70485f3-0b12-437f-9efe-85b17f0c627f"
#   },
#   "isoDownloadURL": "https://assisted-image-service-multicluster-engine.apps.factory.wzhlab.top/images/c70485f3-0b12-437f-9efe-85b17f0c627f?api_key=eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbmZyYV9lbnZfaWQiOiJjNzA0ODVmMy0wYjEyLTQzN2YtOWVmZS04NWIxN2YwYzYyN2YifQ.4FqFWSqfYijmGGWAKopqHIiKghDZBZ2NAqTY1hmUhwNfTzuKlFLZ2pDZAevAxtmf7aN96-6UCeNewIfqoLzPVQ&arch=x86_64&type=minimal-iso&version=4.11"
# }

# VAR_ISO=`oc get infraenv ${ACM_DEMO_CLUSTER} -n ${ACM_DEMO_CLUSTER} -o jsonpath={.status.isoDownloadURL}`

# cd /data/install/
# wget --no-check-certificate -O acm.demo1.iso $VAR_ISO

```

![](imgs/2023-01-17-19-28-12.png)

![](imgs/2023-01-17-19-28-40.png)

![](imgs/2023-01-17-19-29-01.png)

### add host to infra env


![](imgs/2023-01-17-19-30-49.png)

```bash
# lets confirm that the metal3 component is ready
# then we can use ocp to manage the baremetal
oc get pod -A | grep metal3
# openshift-machine-api                              metal3-8666f4cf4d-2bkfb                                           5/5     Running     5               12h
# openshift-machine-api                              metal3-image-cache-8jhtr                                          1/1     Running     1               13h
# openshift-machine-api                              metal3-image-cache-9jfs7                                          1/1     Running     1               13h
# openshift-machine-api                              metal3-image-cache-fl545                                          1/1     Running     1               13h
# openshift-machine-api                              metal3-image-customization-868d87999b-x2mnw                       1/1     Running     1               13h


cat << EOF > ${BASE_DIR}/data/install/acm.demo.secret.bmc.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ${ACM_DEMO_CLUSTER}-bmc-master-01
  namespace: ${ACM_DEMO_CLUSTER}
data:
  password: $(echo password | base64)
  username: $(echo admin | base64)
type: Opaque
EOF
oc create -f ${BASE_DIR}/data/install/acm.demo.secret.bmc.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.demo.secret.bmc.yaml

cat << EOF > ${BASE_DIR}/data/install/acm.demo.bmh.master.yaml
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: ${ACM_DEMO_CLUSTER}-${SNO_HOSTNAME}
  namespace: ${ACM_DEMO_CLUSTER}
  labels:
    infraenvs.agent-install.openshift.io: "${ACM_DEMO_CLUSTER}"
  annotations:
    ## Disable the Introspection
    inspect.metal3.io: disabled
    ## Set Static Hostname
    bmac.agent-install.openshift.io/hostname: "${SNO_HOSTNAME}"
    ## Set Static Role, auto-assign?
    # bmac.agent-install.openshift.io/role: "worker"
spec:
  online: true
  bmc:
    address: redfish-virtualmedia://192.168.77.101:8000/redfish/v1/Systems/$(cat /data/install/vm.list.* | grep ocp4-ipi-edge-master-01 | awk '{print $1}')
    credentialsName: ${ACM_DEMO_CLUSTER}-bmc-master-01
    disableCertificateVerification: true
  bootMACAddress: $(cat /data/install/mac.list.* | grep ocp4-ipi-edge-master-01 | awk '{print $2}')
  automatedCleaningMode: disabled
EOF
oc create -f ${BASE_DIR}/data/install/acm.demo.bmh.master.yaml
# oc delete -f ${BASE_DIR}/data/install/acm.demo.bmh.master.yaml



```

![](imgs/2023-01-17-19-33-16.png)

![](imgs/2023-01-17-19-37-05.png)

```bash

oc get agent -n ${ACM_DEMO_CLUSTER}
# NAME                                   CLUSTER   APPROVED   ROLE     STAGE
# a176e428-fea7-43ff-95c7-a927514227ed             true       worker

oc get agent/a176e428-fea7-43ff-95c7-a927514227ed -n ${ACM_DEMO_CLUSTER} -o yaml
# ......
# spec:
#   approved: true
#   hostname: edge-worker-01
#   role: worker
# ......

```

## import the hosted cluster

复制页面上的命令，并到helper上，运行这2个命令，他们是登录到hosted control plan，然后配置一些CR进去

![](imgs/2023-01-17-14-11-16.png)

```bash
# on helper

# copy past the 1st command
oc login https://192.168.12.23:30000 -u kubeadmin -p z2I9i-BZF8L-sYvUC-47c7x

# copy past the 2nd command
# it is too large, we will omit most of them
echo "Ci0tLQphc............" | base64 -d | oc create -f - || test $? -eq 0 && sleep 2 && echo "Ci0tLQphcGlWZ............" | base64 -d | oc apply -f - || echo "VGhlIGNsdXN..............." | base64 -d

# lets decode the first 2 base64 content, the 3rd one is just a message.
```

第一个导入hosted control plan的yaml

```yaml
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: klusterlets.operator.open-cluster-management.io
spec:
  conversion:
    strategy: None
  group: operator.open-cluster-management.io
  names:
    kind: Klusterlet
    listKind: KlusterletList
    plural: klusterlets
    singular: klusterlet
  scope: Cluster
  preserveUnknownFields: false
  versions:
    - name: v1
      schema:
        openAPIV3Schema:
          description: Klusterlet represents controllers to install the resources for a managed cluster. When configured, the Klusterlet requires a secret named bootstrap-hub-kubeconfig in the agent namespace to allow API requests to the hub for the registration protocol. In Hosted mode, the Klusterlet requires an additional secret named external-managed-kubeconfig in the agent namespace to allow API requests to the managed cluster for resources installation.
          type: object
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              description: Spec represents the desired deployment configuration of Klusterlet agent.
              type: object
              properties:
                clusterName:
                  description: ClusterName is the name of the managed cluster to be created on hub. The Klusterlet agent generates a random name if it is not set, or discovers the appropriate cluster name on OpenShift.
                  type: string
                deployOption:
                  description: DeployOption contains the options of deploying a klusterlet
                  type: object
                  properties:
                    mode:
                      description: 'Mode can be Default or Hosted. It is Default mode if not specified In Default mode, all klusterlet related resources are deployed on the managed cluster. In Hosted mode, only crd and configurations are installed on the spoke/managed cluster. Controllers run in another cluster (defined as management-cluster) and connect to the mangaged cluster with the kubeconfig in secret of "external-managed-kubeconfig"(a kubeconfig of managed-cluster with cluster-admin permission). Note: Do not modify the Mode field once it''s applied.'
                      type: string
                externalServerURLs:
                  description: ExternalServerURLs represents the a list of apiserver urls and ca bundles that is accessible externally If it is set empty, managed cluster has no externally accessible url that hub cluster can visit.
                  type: array
                  items:
                    description: ServerURL represents the apiserver url and ca bundle that is accessible externally
                    type: object
                    properties:
                      caBundle:
                        description: CABundle is the ca bundle to connect to apiserver of the managed cluster. System certs are used if it is not set.
                        type: string
                        format: byte
                      url:
                        description: URL is the url of apiserver endpoint of the managed cluster.
                        type: string
                namespace:
                  description: 'Namespace is the namespace to deploy the agent. The namespace must have a prefix of "open-cluster-management-", and if it is not set, the namespace of "open-cluster-management-agent" is used to deploy agent. Note: in Detach mode, this field will be **ignored**, the agent will be deployed to the namespace with the same name as klusterlet.'
                  type: string
                nodePlacement:
                  description: NodePlacement enables explicit control over the scheduling of the deployed pods.
                  type: object
                  properties:
                    nodeSelector:
                      description: NodeSelector defines which Nodes the Pods are scheduled on. The default is an empty list.
                      type: object
                      additionalProperties:
                        type: string
                    tolerations:
                      description: Tolerations is attached by pods to tolerate any taint that matches the triple <key,value,effect> using the matching operator <operator>. The default is an empty list.
                      type: array
                      items:
                        description: The pod this Toleration is attached to tolerates any taint that matches the triple <key,value,effect> using the matching operator <operator>.
                        type: object
                        properties:
                          effect:
                            description: Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute.
                            type: string
                          key:
                            description: Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys.
                            type: string
                          operator:
                            description: Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category.
                            type: string
                          tolerationSeconds:
                            description: TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system.
                            type: integer
                            format: int64
                          value:
                            description: Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string.
                            type: string
                registrationImagePullSpec:
                  description: RegistrationImagePullSpec represents the desired image configuration of registration agent. quay.io/open-cluster-management.io/registration:latest will be used if unspecified.
                  type: string
                workImagePullSpec:
                  description: WorkImagePullSpec represents the desired image configuration of work agent. quay.io/open-cluster-management.io/work:latest will be used if unspecified.
                  type: string
            status:
              description: Status represents the current status of Klusterlet agent.
              type: object
              properties:
                conditions:
                  description: 'Conditions contain the different condition statuses for this Klusterlet. Valid condition types are: Applied: Components have been applied in the managed cluster. Available: Components in the managed cluster are available and ready to serve. Progressing: Components in the managed cluster are in a transitioning state. Degraded: Components in the managed cluster do not match the desired configuration and only provide degraded service.'
                  type: array
                  items:
                    description: "Condition contains details for one aspect of the current state of this API Resource. --- This struct is intended for direct use as an array at the field path .status.conditions.  For example, type FooStatus struct{     // Represents the observations of a foo's current state.     // Known .status.conditions.type are: \"Available\", \"Progressing\", and \"Degraded\"     // +patchMergeKey=type     // +patchStrategy=merge     // +listType=map     // +listMapKey=type     Conditions []metav1.Condition `json:\"conditions,omitempty\" patchStrategy:\"merge\" patchMergeKey:\"type\" protobuf:\"bytes,1,rep,name=conditions\"` \n     // other fields }"
                    type: object
                    required:
                      - lastTransitionTime
                      - message
                      - reason
                      - status
                      - type
                    properties:
                      lastTransitionTime:
                        description: lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable.
                        type: string
                        format: date-time
                      message:
                        description: message is a human readable message indicating details about the transition. This may be an empty string.
                        type: string
                        maxLength: 32768
                      observedGeneration:
                        description: observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance.
                        type: integer
                        format: int64
                        minimum: 0
                      reason:
                        description: reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty.
                        type: string
                        maxLength: 1024
                        minLength: 1
                        pattern: ^[A-Za-z]([A-Za-z0-9_,:]*[A-Za-z0-9_])?$
                      status:
                        description: status of the condition, one of True, False, Unknown.
                        type: string
                        enum:
                          - "True"
                          - "False"
                          - Unknown
                      type:
                        description: type of condition in CamelCase or in foo.example.com/CamelCase. --- Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt)
                        type: string
                        maxLength: 316
                        pattern: ^([a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$
                generations:
                  description: Generations are used to determine when an item needs to be reconciled or has changed in a way that needs a reaction.
                  type: array
                  items:
                    description: GenerationStatus keeps track of the generation for a given resource so that decisions about forced updates can be made. The definition matches the GenerationStatus defined in github.com/openshift/api/v1
                    type: object
                    properties:
                      group:
                        description: group is the group of the resource that you're tracking
                        type: string
                      lastGeneration:
                        description: lastGeneration is the last generation of the resource that controller applies
                        type: integer
                        format: int64
                      name:
                        description: name is the name of the resource that you're tracking
                        type: string
                      namespace:
                        description: namespace is where the resource that you're tracking is
                        type: string
                      resource:
                        description: resource is the resource type of the resource that you're tracking
                        type: string
                      version:
                        description: version is the version of the resource that you're tracking
                        type: string
                observedGeneration:
                  description: ObservedGeneration is the last generation change you've dealt with
                  type: integer
                  format: int64
                relatedResources:
                  description: RelatedResources are used to track the resources that are related to this Klusterlet.
                  type: array
                  items:
                    description: RelatedResourceMeta represents the resource that is managed by an operator
                    type: object
                    properties:
                      group:
                        description: group is the group of the resource that you're tracking
                        type: string
                      name:
                        description: name is the name of the resource that you're tracking
                        type: string
                      namespace:
                        description: namespace is where the thing you're tracking is
                        type: string
                      resource:
                        description: resource is the resource type of the resource that you're tracking
                        type: string
                      version:
                        description: version is the version of the thing you're tracking
                        type: string
      served: true
      storage: true
      subresources:
        status: {}
status:
  acceptedNames:
    kind: ""
    plural: ""
  conditions: []
  storedVersions: []

```
第二个yaml是这样的。
```yaml

---
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    workload.openshift.io/allowed: "management"
  name: "open-cluster-management-agent"

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: klusterlet
  namespace: "open-cluster-management-agent"

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: klusterlet
rules:
- apiGroups: [""]
  resources: ["secrets", "configmaps", "serviceaccounts"]
  verbs: ["create", "get", "list", "update", "watch", "patch", "delete"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["create", "get", "list", "update", "watch", "patch"]
- apiGroups: ["authorization.k8s.io"]
  resources: ["subjectaccessreviews"]
  verbs: ["create"]
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["create", "get", "list", "watch","delete"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["", "events.k8s.io"]
  resources: ["events"]
  verbs: ["create", "patch", "update"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["create", "get", "list", "update", "watch", "patch", "delete"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["clusterrolebindings", "rolebindings"]
  verbs: ["create", "get", "list", "update", "watch", "patch", "delete"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["clusterroles", "roles"]
  verbs: ["create", "get", "list", "update", "watch", "patch", "delete", "escalate", "bind"]
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: ["create", "get", "list", "update", "watch", "patch", "delete"]
- apiGroups: ["operator.open-cluster-management.io"]
  resources: ["klusterlets"]
  verbs: ["get", "list", "watch", "update", "patch", "delete"]
- apiGroups: ["operator.open-cluster-management.io"]
  resources: ["klusterlets/status"]
  verbs: ["update", "patch"]
- apiGroups: ["work.open-cluster-management.io"]
  resources: ["appliedmanifestworks"]
  verbs: ["list", "update", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: open-cluster-management:klusterlet-admin-aggregate-clusterrole
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
rules:
- apiGroups: ["operator.open-cluster-management.io"]
  resources: ["klusterlets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: klusterlet
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: klusterlet
subjects:
- kind: ServiceAccount
  name: klusterlet
  namespace: "open-cluster-management-agent"

---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: klusterlet
  namespace: "open-cluster-management-agent"
  labels:
    app: klusterlet
spec:
  replicas: 1
  selector:
    matchLabels:
      app: klusterlet
  template:
    metadata:
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
      labels:
        app: klusterlet
    spec:
      serviceAccountName: klusterlet
      tolerations:
      - key: "node-role.kubernetes.io/infra"
        value: ""
        effect: "NoSchedule"
        operator: "Exists"
      containers:
      - name: klusterlet
        image: registry.redhat.io/multicluster-engine/registration-operator-rhel8@sha256:183dc28f1991ad2aa2fcb987d217fc63863909497ae9291b14a96079640463d3
        imagePullPolicy: IfNotPresent
        args:
          - "/registration-operator"
          - "klusterlet"
          - "--disable-leader-election"
        livenessProbe:
          httpGet:
            path: /healthz
            scheme: HTTPS
            port: 8443
          initialDelaySeconds: 2
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /healthz
            scheme: HTTPS
            port: 8443
          initialDelaySeconds: 2

---
apiVersion: v1
kind: Secret
metadata:
  name: "bootstrap-hub-kubeconfig"

  namespace: "open-cluster-management-agent"

type: Opaque
data:
  kubeconfig: "YXBpVmVyc2............"

---
apiVersion: operator.open-cluster-management.io/v1
kind: Klusterlet
metadata:
  name: klusterlet
spec:
  deployOption:
    mode: Default
  registrationImagePullSpec: "registry.redhat.io/multicluster-engine/registration-rhel8@sha256:52efbbbd9deef8517ea2c96b1d4756c154ebf342a6331603c6942cf0a64ee133"
  workImagePullSpec: "registry.redhat.io/multicluster-engine/work-rhel8@sha256:3e1a592361dc8176dae1eb5d2bc82bd3aabb6e370add47ae84325ddeb00d661c"
  clusterName: "edge01"
  namespace: "open-cluster-management-agent"
  nodePlacement:
    tolerations:
    - key: "node-role.kubernetes.io/infra"
      value: ""
      effect: "NoSchedule"
      operator: "Exists"

```




```bash
oc extract -n ${ACM_DEMO_CLUSTER} secret/${ACM_DEMO_CLUSTER}-admin-kubeconfig --to=- > ${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER}

# approve the worker node, if the node can't import
# under normal situation, this is no needed.
oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get csr | grep -v Approved
oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} adm certificate approve


oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get co
# NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
# console                                    4.11.21   True        False         False      6h22m
# csi-snapshot-controller                    4.11.21   True        False         False      6h24m
# dns                                        4.11.21   True        False         False      6h23m
# image-registry                             4.11.21   True        False         False      6h23m
# ingress                                    4.11.21   True        False         False      6h39m
# insights                                   4.11.21   True        False         False      6h25m
# kube-apiserver                             4.11.21   True        False         False      6h40m
# kube-controller-manager                    4.11.21   True        False         False      6h40m
# kube-scheduler                             4.11.21   True        False         False      6h40m
# kube-storage-version-migrator              4.11.21   True        False         False      6h24m
# monitoring                                 4.11.21   True        False         False      6h20m
# network                                    4.11.21   True        False         False      6h24m
# openshift-apiserver                        4.11.21   True        False         False      6h40m
# openshift-controller-manager               4.11.21   True        False         False      6h40m
# openshift-samples                          4.11.21   True        False         False      6h23m
# operator-lifecycle-manager                 4.11.21   True        False         False      6h40m
# operator-lifecycle-manager-catalog         4.11.21   True        False         False      6h40m
# operator-lifecycle-manager-packageserver   4.11.21   True        False         False      6h40m
# service-ca                                 4.11.21   True        False         False      6h25m
# storage                                    4.11.21   True        False         False      6h25m

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get node
# NAME             STATUS   ROLES    AGE     VERSION
# edge-master-01   Ready    worker   6h28m   v1.24.6+5658434

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get mcp
# error: the server doesn't have a resource type "mcp"

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get mc
# error: the server doesn't have a resource type "mc"

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get all -o wide -n openshift-ingress
# NAME                                 READY   STATUS    RESTARTS   AGE     IP              NODE             NOMINATED NODE   READINESS GATES
# pod/router-default-bb569f544-cknjw   1/1     Running   0          6h41m   192.168.12.33   edge-master-01   <none>           <none>

# NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                   AGE     SELECTOR
# service/router-internal-default   ClusterIP   172.31.152.115   <none>        80/TCP,443/TCP,1936/TCP   6h41m   ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default

# NAME                             READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES                                                                                                                   SELECTOR
# deployment.apps/router-default   1/1     1            1           6h41m   router       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0dc935b7825a800e32eac69fafa2d238e1d6eb2f344cdf29345cb1123c26a22   ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default

# NAME                                       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                                                                                                                   SELECTOR
# replicaset.apps/router-default-bb569f544   1         1         1       6h41m   router       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:e0dc935b7825a800e32eac69fafa2d238e1d6eb2f344cdf29345cb1123c26a22   ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default,pod-template-hash=bb569f544


oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get pod -A | wc -l
# 56

oc --kubeconfig=${BASE_DIR}/data/install/kubeconfig-${ACM_DEMO_CLUSTER} get clusterversion
# NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
# version   4.11.21   True        False         6h35m   Cluster version is 4.11.21


# to delete cluster


```



![](imgs/2023-01-16-11-43-31.png)


![](imgs/2023-01-16-11-40-30.png)



![](imgs/2023-01-16-11-42-55.png)

![](imgs/2023-01-16-11-44-58.png)

<!-- ![](imgs/2023-01-16-12-33-28.png) -->

<!-- ![](imgs/2023-01-16-12-33-45.png)

![](imgs/2023-01-16-12-34-02.png) -->



<!-- 我们回到MCE的界面中，能从基础架构中，看到我们新创建的HOST了，能看到MCE正在通过redfish配置这个kvm

![](../4.11/imgs/2023-01-13-23-38-41.png)

这个bare metal host其实是调用的openshift4平台上的服务创建的，所以从openshift4的console上也能看得到：

![](../4.11/imgs/2023-01-13-23-31-32.png)

能从openshift4 console上看到这个bare metal host的详细信息：

![](../4.11/imgs/2023-01-13-23-32-56.png)

![](../4.11/imgs/2023-01-13-23-32-27.png)

回到ACM的界面中，我们能看到安装正在继续：

![](../4.11/imgs/2023-01-13-23-33-40.png)

从ACM的cluster界面中，我们能看到安装的详细进展情况：

![](../4.11/imgs/2023-01-13-23-34-09.png)

但是安装的中途，提示我们需要动手操作一下。这是因为我们是用kvm模拟的物理机，并且模拟了一个redfish，这个redfish功能比较简单，在安装ocp的过程中，kvm会重启，但是远程挂载的光盘没有卸载，所以我们需要卸载掉这个光驱，然后继续安装：

![](../4.11/imgs/2023-01-13-20-19-01.png)

![](../4.11/imgs/2023-01-13-20-19-34.png)

进入kvm的界面，调整一下启动顺序：
![](../4.10/imgs/20220408230353.png)  

然后重启kvm，等待一段时间，infra env就安装完成了。

![](../4.11/imgs/2023-01-13-20-22-49.png)

不过，cluster还在继续安装，我们安心等待安装过程完成。

![](../4.11/imgs/2023-01-13-20-21-47.png) 

终于完成啦。

## 安装完成

装好了以后，我们在MCE里面就能看到如下景象： https://console-openshift-console.apps.factory.wzhlab.top/multicloud/infrastructure/clusters/managed

![](../4.11/imgs/2023-01-13-21-19-58.png)


看cluster的详细信息，也正常了：

![](../4.11/imgs/2023-01-13-21-16-29.png)

⚠️一定记得，下载kubeconfig文件，还有密码

cluster的node tab也有内容了：

![](../4.11/imgs/2023-01-13-21-52-50.png)

cluster的add-on也有了组件，这两个组件是干什么用的呢？作者暂时说不确切，以后查到资料再分析吧。

![](imgs/2023-01-13-21-53-25.png)

infra env也绿色状态了 https://console-openshift-console.apps.factory.wzhlab.top/multicloud/infrastructure/environments

![](../4.11/imgs/2023-01-13-22-45-47.png)

详细信息和原来一样：

![](../4.11/imgs/2023-01-13-22-46-13.png)

hosts tab 也完成了

![](../4.11/imgs/2023-01-13-22-46-41.png) -->

## post operation

装完了，我们为了方便做实验，我们对集群节点做点配置。虽然减低了集群的安全性，但是做实验吗，无所谓了。

```bash

# on helper

# VAR_CLUSTER=edge01
# oc get secret/$VAR_CLUSTER-keypair -n $VAR_CLUSTER --template='{{index .data "id_rsa.key" | base64decode}}' > ${BASE_DIR}/data/install/edge.key

# chmod 600 ${BASE_DIR}/data/install/edge.key

# ssh -i ${BASE_DIR}/data/install/edge.key core@192.168.12.33

cat > ${BASE_DIR}/data/install/crack.txt << EOF

echo redhat | sudo passwd --stdin root

sudo sed -i "s|^PasswordAuthentication no$|PasswordAuthentication yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^PermitRootLogin no$|PermitRootLogin yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^#ClientAliveInterval 180$|ClientAliveInterval 1800|g" /etc/ssh/sshd_config

sudo systemctl restart sshd

sudo sh -c 'echo "export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig" >> /root/.bashrc'

sudo sh -c 'echo "RET=\\\`oc config use-context system:admin\\\`" >> /root/.bashrc'

EOF

for i in 33
do
  ssh core@192.168.12.$i < ${BASE_DIR}/data/install/crack.txt
done


for i in 33
do
  sshpass -p 'redhat' ssh-copy-id root@192.168.12.$i
done


ssh root@192.168.12.33

```

# check dhcp existed

我们是静态IP安装，那么就要确认一下环境里面是不是真的 DHCP 给关了，检查的方法如下。

https://superuser.com/questions/750359/check-if-a-dhcp-server-existing-in-my-network-using-bash

```bash
dnf install nmap -y
nmap --script broadcast-dhcp6-discover -e enp1s0

```

# end

```bash

# revert the order
tac << EOF 
oc delete -f /data/install/acm.ocp.release.yaml
oc delete -f /data/install/acm.cm.asc.yaml
oc delete -f /data/install/acm.secret.yaml
oc delete -f /data/install/acm.mirror.yaml
oc delete -f /data/install/acm.agentservicecofnig.yaml
oc delete -f /data/install/acm.managed.secret.yaml
oc delete -f /data/install/acm.agentclusterinstall.yaml
oc delete -f /data/install/acm.nmsc.yaml
oc delete -f /data/install/acm.clusterdeployment.yaml
oc delete -f /data/install/acm.klusterletaddonconfig.yaml
oc delete -f /data/install/acm.managedcluster.yaml
oc delete -f /data/install/acm.infraenv.yaml
EOF
oc delete -f /data/install/acm.infraenv.yaml
oc delete -f /data/install/acm.managedcluster.yaml
oc delete -f /data/install/acm.klusterletaddonconfig.yaml
oc delete -f /data/install/acm.clusterdeployment.yaml
oc delete -f /data/install/acm.nmsc.yaml
oc delete -f /data/install/acm.agentclusterinstall.yaml
oc delete -f /data/install/acm.managed.secret.yaml
oc delete -f /data/install/acm.agentservicecofnig.yaml
oc delete -f /data/install/acm.mirror.yaml
oc delete -f /data/install/acm.secret.yaml
oc delete -f /data/install/acm.cm.asc.yaml
oc delete -f /data/install/acm.ocp.release.yaml


```
