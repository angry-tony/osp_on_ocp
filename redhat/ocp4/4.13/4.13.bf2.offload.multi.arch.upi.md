# openshift 4.12 BF2 offload

- [OVN/OVS offloading with OpenShift on NVIDIA BlueField-2 DPUs](https://access.redhat.com/articles/6804281)

bf2 network connection diagram:

![](./dia/bf2.offload.drawio.svg)

- https://multi.ocp.releases.ci.openshift.org/
  - [4.12.9](https://multi.ocp.releases.ci.openshift.org/releasestream/4-stable-multi/release/4.12.9)
  - quay.io/openshift-release-dev/ocp-release:4.12.9-multi

# fresh BF2 with official ubuntu os 

- [Installing Red Hat Enterprise Linux on NVIDIA BlueField-2 DPU](https://developers.redhat.com/articles/2021/10/18/sensitive-information-detection-using-nvidia-morpheus-ai-framework#setting_up_nvidia_netq_agent_on_nvidia_bluefield_2_dpu)

```bash


# https://bugzilla.redhat.com/show_bug.cgi?id=1814682
dnf install -y kernel-modules-extra psmisc

mkdir -p /data/down/
cd /data/down/

dnf install -y rshim expect wget minicom rpm-build lshw
systemctl enable --now rshim
systemctl status rshim --no-pager -l

dnf install -y openssl-devel mstflint

# nat router on host
# https://access.redhat.com/discussions/4642721
cat << EOF >> /etc/sysctl.d/99-wzh-sysctl.conf

net.ipv4.ip_forward = 1

EOF
sysctl --system

systemctl disable --now firewalld

# download bfb from here
# https://developer.nvidia.com/networking/doca
# and docs here
# https://docs.nvidia.com/networking/display/BlueFieldDPUOSLatest/Deploying+BlueField+Software+Using+BFB+from+Host
wget -O bf2.bfb https://content.mellanox.com/BlueField/BFBs/Ubuntu20.04/DOCA_1.5.1_BSP_3.9.3_Ubuntu_20.04-4.2211-LTS.signed.bfb

cat bf2.bfb > /dev/rshim0/boot

# if you want to connect to bf2 through serial console
minicom --color on --baudrate 115200 --device /dev/rshim0/console

# on console of bf2
# login using ubuntu / ubuntu
# and change to mellanox

ip link | grep -A 1 enp3s0f1s0
# 11: enp3s0f1s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
#     link/ether 02:7f:40:ae:d9:cc brd ff:ff:ff:ff:ff:ff

# upgrade fw
/opt/mellanox/mlnx-fw-updater/mlnx_fw_updater.pl

mlxconfig -d /dev/mst/mt41686_pciconf0 -y reset

mlxconfig -d /dev/mst/mt41686_pciconf0  s LINK_TYPE_P1=2 LINK_TYPE_P2=2

# power off and on machine.

grep -s -H "" /sys/class/net/*/phys_port_name
# /sys/class/net/en3f0pf0sf0/phys_port_name:pf0sf0
# /sys/class/net/en3f1pf1sf0/phys_port_name:pf1sf0
# /sys/class/net/enp3s0f0s0/phys_port_name:p0
# /sys/class/net/enp3s0f1s0/phys_port_name:p0
# /sys/class/net/p0/phys_port_name:p0
# /sys/class/net/p1/phys_port_name:p1
# /sys/class/net/pf0hpf/phys_port_name:pf0
# /sys/class/net/pf1hpf/phys_port_name:pf1

# on 101
nmcli con add type bridge-slave ifname enp7s0f1np1 master baremetal
nmcli con up baremetal

# on 103, bf2
cat /etc/netplan/60-mlnx.yaml
# network:
#   ethernets:
#     enp3s0f0s0:
#       renderer: networkd
#       dhcp4: 'true'
#     enp3s0f1s0:
#       renderer: networkd
#       dhcp4: 'true'
#   version: 2

cat << EOF > /etc/netplan/60-mlnx.yaml
network:
  ethernets:
    enp3s0f0s0:
      renderer: networkd
      dhcp4: 'true'
    enp3s0f1s0:
      renderer: networkd
      dhcp4: no
      addresses:
        - 192.168.7.113/24
      gateway4: 192.168.7.9
      nameservers:
          addresses: [114.114.114.114]
  version: 2
EOF

netplan apply

# login on 101
ssh ubuntu@192.168.7.113


```

# flash BF2 with rocky linux 8

we need to get ifname, so we have to flash it with rocky linux 8

```bash

# shell come frome https://github.com/wangzheng422/rhel-on-bf2
# download rhel8.6 install iso from  
# https://mirrors.nju.edu.cn/rocky/8/isos/aarch64/Rocky-8.7-aarch64-minimal.iso
export RHEL_ISO=Rocky-8.7-aarch64-minimal.iso
bash bluefield_provision.sh -s
# === STATUS === Checking usability of SRIOV for PCI 0000:05:00.0
# SRIOV enabled
# EMBEDDED_CPU mode enabled
# === STATUS === Checking usability of SRIOV for PCI 0000:05:00.1
# SRIOV enabled
# EMBEDDED_CPU mode enabled

setenforce 0

# bash bluefield_provision.sh -p

iptables -F
bash ./PXE_setup_RHEL_install_over_mlx.sh -i "${RHEL_ISO}" -p tmfifo -k RHEL8-bluefield.ks

uplink_interface="$(ip route |grep ^default | sed 's/.*dev \([^ ]\+\).*/\1/')"
test -n "${uplink_interface}" || die "need a default route"
    
iptables -t nat -A POSTROUTING -o "${uplink_interface}" -j MASQUERADE

# on a new terminal
minicom --color on --baudrate 115200 --device /dev/rshim0/console

# change back to original terminal
# to reboot the bf2
echo BOOT_MODE 1 > /dev/rshim0/misc
echo SW_RESET 1 > /dev/rshim0/misc

sleep 10
nmcli conn up tmfifo_net0
systemctl restart dhcpd

iptables -t nat -A POSTROUTING -o "${uplink_interface}" -j MASQUERADE

# and see result on com console
# and press 'ESC' to see bios console
# and select 'Boot Manager'
# select EFI network with following content


```
see result on com console, press 'ESC' to see bios console

![](../4.10/imgs/2022-06-14-16-50-28.png)

select 'Boot Manager'

![](../4.10/imgs/2022-06-14-16-51-14.png)

select EFI network with following content

![](../4.10/imgs/2022-06-14-16-51-42.png)

```
MAC(001ACAFFFF01,0x1)/
IPv4(0.0.0.0)
```
in our env, it is 'EFI Network 8'
![](../4.10/imgs/2022-06-14-16-53-51.png)

select the item , and it will boot through pxe and install os

```bash
# [   21.067341] IPv6: ADDRCONF(NETDEV_CHANGE): enp3s0f1: link becomes ready

# on host, disable services
systemctl disable --now dhcpd httpd vsftpd tftp.socket tftp

# on bf2 console
# login using root / bluefield

nmcli con modify System\ eth0 con-name eth0
nmcli con modify eth0 ipv4.method manual ipv4.addresses 192.168.100.2/24 ipv4.gateway 192.168.100.1 ipv4.dns 172.21.1.1
nmcli con up eth0

nmcli con mod enp3s0f1 ipv4.method manual ipv4.addr 192.168.77.55/24 ipv4.gateway 192.168.77.9 ipv4.dns 192.168.77.11 
nmcli con up enp3s0f1

ethtool -i enp3s0f1
# driver: mlx5_core
# version: 4.18.0-425.3.1.el8.aarch64
# firmware-version: 24.35.2000 (MT_0000000539)
# expansion-rom-version:
# bus-info: 0000:03:00.1
# supports-statistics: yes
# supports-test: yes
# supports-eeprom-access: no
# supports-register-dump: no
# supports-priv-flags: yes

ethtool -i eth0
# driver: virtio_net
# version: 1.0.0
# firmware-version:
# expansion-rom-version:
# bus-info: virtio
# supports-statistics: yes
# supports-test: no
# supports-eeprom-access: no
# supports-register-dump: no
# supports-priv-flags: no

# login
ssh root@192.168.77.55
# bluefield

```

# install ocp cluster on kvm on 101 and 102

this is for hypershift host ocp

## create multi-arch release image

- https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.7/html/clusters/cluster_mce_overview#manual-release-image-cross-arch


```bash
# on vultr

# export BUILDNUMBER=4.12.9

# podman pull quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-x86_64
# podman pull quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-aarch64

# podman push quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-x86_64 quay.io/wangzheng422/ocp-release:$BUILDNUMBER-x86_64
# podman push quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-aarch64 quay.io/wangzheng422/ocp-release:$BUILDNUMBER-aarch64

# podman manifest create mymanifest
# podman manifest add mymanifest quay.io/wangzheng422/ocp-release:$BUILDNUMBER-x86_64
# podman manifest add mymanifest quay.io/wangzheng422/ocp-release:$BUILDNUMBER-aarch64

# podman manifest push mymanifest docker://quay.io/wangzheng422/ocp-release:$BUILDNUMBER

# on helper
export BUILDNUMBER=4.13.0-rc.2

SEC_FILE='/data/pull-secret.json'

oc adm release mirror -a $SEC_FILE --keep-manifest-list=true \
  --from=quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi \
  --to=quaylab.infra.wzhlab.top:5443/ocp4/openshift4

oc adm release mirror -a $SEC_FILE --keep-manifest-list=true \
  --from=quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-x86_64 \
  --to=quaylab.infra.wzhlab.top:5443/ocp4/openshift4

oc adm release mirror -a $SEC_FILE --keep-manifest-list=true \
  --from=quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-aarch64 \
  --to=quaylab.infra.wzhlab.top:5443/ocp4/openshift4

oc image mirror --keep-manifest-list=true quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi quaylab.infra.wzhlab.top:5443/ocp4/openshift4:$BUILDNUMBER-multi

oc adm release info -a $SEC_FILE quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/arm64

oc adm release info -a $SEC_FILE quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/amd64 --commit-urls

# get the binary
mkdir -p /data/ocp-$BUILDNUMBER-multi
cd /data/ocp-$BUILDNUMBER-multi

mkdir -p amd64
pushd amd64

# oc adm release extract -a $SEC_FILE --tools=true quay.io/openshift-release-dev/ocp-release:4.12.9-multi  --filter-by-os=linux/amd64

# try
mkdir -p ostree

VAR_IMAGE=`oc adm release info -a $SEC_FILE quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/amd64 --image-for=machine-os-images` 

oc image extract --path /:./ostree -a $SEC_FILE $VAR_IMAGE

/bin/cp -f ostree/coreos/coreos-*.iso ./
chmod +r *.iso

/bin/rm -rf ostree

oc adm release extract -a $SEC_FILE --command=oc quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/amd64
oc adm release extract -a $SEC_FILE --command=openshift-install quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/amd64
oc adm release extract -a $SEC_FILE --command=openshift-baremetal-install quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/amd64

popd

mkdir -p arm64
pushd arm64

mkdir -p ostree

VAR_IMAGE=`oc adm release info -a $SEC_FILE quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/arm64 --image-for=machine-os-images`

oc image extract --path /:./ostree -a $SEC_FILE $VAR_IMAGE --filter-by-os=linux/arm64 

/bin/cp -f ostree/coreos/coreos-*.iso ./
chmod +r *.iso

/bin/rm -rf ostree

oc adm release extract -a $SEC_FILE --command=oc  quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/arm64
oc adm release extract -a $SEC_FILE --command=openshift-install quay.io/openshift-release-dev/ocp-release:$BUILDNUMBER-multi  --filter-by-os=linux/arm64
# oc adm release extract -a $SEC_FILE --command=openshift-baremetal-install quay.io/openshift-release-dev/ocp-release:4.12.9-multi  --filter-by-os=linux/arm64

popd


# mkdir source

# oc adm release extract -a $SEC_FILE --git=source quay.io/openshift-release-dev/ocp-release:4.12.9-multi  --filter-by-os=linux/amd64

```


## on helper node

终于所有的准备工作都做完了，我们开始在helper上面进行openshift的安装。在这之前，还有一个配置helper节点的步骤，主要是配置dns服务之类的，在这里就不重复了，有需要了解的，可以看[这里的文档](./4.11.helper.node.oc.mirror.md)

### get installer binary

我们先要从安装文件目录中，得到installer的二进制文件。

```bash

# switch to you install version

export BUILDNUMBER=4.13.0-rc.2

pushd /data/ocp-${BUILDNUMBER}
tar -xzf openshift-client-linux-${BUILDNUMBER}.tar.gz -C /usr/local/bin/
tar -xzf openshift-install-linux-${BUILDNUMBER}.tar.gz -C /usr/local/bin/

install -m 755 /data/ocp4/clients/butane-amd64 /usr/local/bin/butane
install -m 755 /data/ocp4/clients/coreos-installer_amd64 /usr/local/bin/coreos-installer

popd

pushd /data/ocp-${BUILDNUMBER}-multi/amd64

install oc /usr/local/bin/
install openshift-install /usr/local/bin/ 
install openshift-baremetal-install /usr/local/bin/

# install -m 755 /data/ocp4/clients/butane-amd64 /usr/local/bin/butane
# install -m 755 /data/ocp4/clients/coreos-installer_amd64 /usr/local/bin/coreos-installer

popd

openshift-install version
# openshift-install 4.13.0-rc.2
# built from commit aaf8b7c5b0426a45728e58a1282d1300f8d37b77
# release image quay.io/openshift-release-dev/ocp-release@sha256:feafe0e47cc476efe748bf2fb5cc4ae0960ccec5b8279c6d81f0b37af75fc197
# release architecture amd64

oc image info quay.io/openshift-release-dev/ocp-release@sha256:feafe0e47cc476efe748bf2fb5cc4ae0960ccec5b8279c6d81f0b37af75fc197
  # OS            DIGEST
  # linux/amd64   sha256:b5a2b03775d62aded0d2350e376bd214a0c6a93b2367b2b1748ef5799ca3ca40
  # linux/ppc64le sha256:09178ffe61123dbb6df7b91bea11cbdb0bb1168c4150fca712b170dbe4ad13e9
  # linux/s390x   sha256:6a6f7331073434858f86a14c455cbfb7c4bddbf8a77fdd786d6a627e3e6b6e02
  # linux/arm64   sha256:da5ed207535e6a3b6320cac188ddf06e3270944b0ec9b37a9483861c6a11f1d3

oc image info quay.io/openshift-release-dev/ocp-release:${BUILDNUMBER}-multi
  # OS            DIGEST
  # linux/amd64   sha256:b5a2b03775d62aded0d2350e376bd214a0c6a93b2367b2b1748ef5799ca3ca40
  # linux/ppc64le sha256:09178ffe61123dbb6df7b91bea11cbdb0bb1168c4150fca712b170dbe4ad13e9
  # linux/s390x   sha256:6a6f7331073434858f86a14c455cbfb7c4bddbf8a77fdd786d6a627e3e6b6e02
  # linux/arm64   sha256:da5ed207535e6a3b6320cac188ddf06e3270944b0ec9b37a9483861c6a11f1d3


```

### create the install yaml

接下来我们创建安装配置文件。这里面最关键的就是那个yaml模板，我们在模板里面，启动IPI安装模式，并且配置3个master的redfish接口信息，并启用静态IP安装的方法，配置了静态IP信息。

安装配置yaml文件创建后，我们调用installer，把他们转化成ignition等真正的安装配置文件，并且和baremetal installer二进制文件一起，传递到物理机上。

这里面有2个二进制文件，一个是openshift installer，这个一般场景下，比如对接公有云，私有云，就够了，它会创建ignition文件，并且调用各种云的接口，创建虚拟机，开始安装。

但是如果是baremetal场景，有一个单独的baremetal installer二进制文件，它读取配置文件，调用物理机BMC接口信息，来开始安装，这个区别是目前openshift版本上的情况，不知道未来会不会有变化。

```bash
# create a user and create the cluster under the user

useradd -m 3nodeipi

usermod -aG wheel 3nodeipi

su - 3nodeipi

ssh-keygen

cat << EOF > ~/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

chmod 600 ~/.ssh/config

cat << 'EOF' >> ~/.bashrc

export BASE_DIR='/home/3nodeipi/'

EOF

export BASE_DIR='/home/3nodeipi/'
export BUILDNUMBER=4.13.0-rc.2

mkdir -p ${BASE_DIR}/data/{sno/disconnected,install}

# set some parameter of you rcluster

NODE_SSH_KEY="$(cat ${BASE_DIR}/.ssh/id_rsa.pub)"
INSTALL_IMAGE_REGISTRY=quaylab.infra.wzhlab.top:5443

# PULL_SECRET='{"auths":{"registry.redhat.io": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"registry.ocp4.redhat.ren:5443": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"'${INSTALL_IMAGE_REGISTRY}'": {"auth": "'$( echo -n 'admin:redhatadmin' | openssl base64 )'","email": "noemail@localhost"}}}'
PULL_SECRET=$(cat /data/pull-secret.json)

NTP_SERVER=192.168.77.11
# HELP_SERVER=192.168.77.11
# KVM_HOST=192.168.77.11
# API_VIP=192.168.77.99
# INGRESS_VIP=192.168.77.98
# CLUSTER_PROVISION_IP=192.168.77.98
MACHINE_NETWORK='192.168.77.0/24'

# 定义单节点集群的节点信息
SNO_CLUSTER_NAME=acm-demo-one
SNO_BASE_DOMAIN=wzhlab.top

# BOOTSTRAP_IP=192.168.77.22
MASTER_01_IP=192.168.77.23
# MASTER_02_IP=192.168.77.24
# MASTER_03_IP=192.168.77.25
WORKER_01_IP=192.168.77.26

# BOOTSTRAP_HOSTNAME=bootstrap-demo
MASTER_01_HOSTNAME=master-01-demo
# MASTER_02_HOSTNAME=master-02-demo
# MASTER_03_HOSTNAME=master-03-demo
WORKER_01_HOSTNAME=worker-01-demo

# BOOTSTRAP_INTERFACE=enp1s0
MASTER_01_INTERFACE=enp1s0
# MASTER_02_INTERFACE=enp1s0
# MASTER_03_INTERFACE=enp1s0
WORKER_01_INTERFACE=enp1s0

MASTER_01_INTERFACE_MAC=52:54:00:12:A1:01
# MASTER_02_INTERFACE_MAC=52:54:00:12:A1:02
# MASTER_03_INTERFACE_MAC=52:54:00:12:A1:03
# MASTER_03_INTERFACE_MAC=52:54:00:12:A1:03

# BOOTSTRAP_DISK=/dev/vda
MASTER_01_DISK=/dev/vda
# MASTER_02_DISK=/dev/vda
# MASTER_03_DISK=/dev/vda
WORKER_01_DISK=/dev/vda

OCP_GW=192.168.77.9
OCP_NETMASK=255.255.255.0
OCP_NETMASK_S=24
OCP_DNS=192.168.77.11

# echo ${SNO_IF_MAC} > /data/sno/sno.mac

mkdir -p ${BASE_DIR}/data/install
cd ${BASE_DIR}/data/install

/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap manifests master*[0-9] worker*[0-9] *

cat << EOF > ${BASE_DIR}/data/install/install-config.yaml 
apiVersion: v1
baseDomain: $SNO_BASE_DOMAIN
compute:
- name: worker
  replicas: 0 
controlPlane:
  name: master
  replicas: 1
metadata:
  name: $SNO_CLUSTER_NAME
networking:
  # OVNKubernetes , OpenShiftSDN
  clusterNetwork:
    - cidr: 172.21.0.0/16
      hostPrefix: 23
    # - cidr: fd02::/48
    #   hostPrefix: 64
  machineNetwork:
    - cidr: $MACHINE_NETWORK
    # - cidr: 2001:DB8::/32
  serviceNetwork:
    - 172.22.0.0/16
    # - fd03::/112
platform: 
  none: {}
pullSecret: '${PULL_SECRET}'
sshKey: |
$( cat ${BASE_DIR}/.ssh/id_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /etc/crts/redhat.ren.ca.crt | sed 's/^/   /g' )
# imageContentSources:
# - mirrors:
#   - ${INSTALL_IMAGE_REGISTRY}/ocp4/openshift4
#   source: quay.io/openshift-release-dev/ocp-release
# - mirrors:
#   - ${INSTALL_IMAGE_REGISTRY}/ocp4/openshift4
#   source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
# - mirrors:
#   - ${INSTALL_IMAGE_REGISTRY}/ocp4/openshift4
#   source: quay.io/openshift-release-dev/ocp-release-nightly
EOF

cat << EOF > ${BASE_DIR}/data/install/agent-config.yaml
apiVersion: v1alpha1
kind: AgentConfig
metadata:
  name: $SNO_CLUSTER_NAME
rendezvousIP: $MASTER_01_IP
additionalNTPSources:
- $NTP_SERVER
hosts:
  - hostname: $MASTER_01_HOSTNAME
    role: master
    rootDeviceHints:
      deviceName: "$MASTER_01_DISK"
    interfaces:
      - name: $MASTER_01_INTERFACE
        macAddress: $MASTER_01_INTERFACE_MAC
    networkConfig:
      interfaces:
        - name: $MASTER_01_INTERFACE
          type: ethernet
          state: up
          mac-address: $MASTER_01_INTERFACE_MAC
          ipv4:
            enabled: true
            address:
              - ip: $MASTER_01_IP
                prefix-length: $OCP_NETMASK_S
            dhcp: false
      dns-resolver:
        config:
          server:
            - $OCP_DNS
      routes:
        config:
          - destination: 0.0.0.0/0
            next-hop-address: $OCP_GW
            next-hop-interface: $MASTER_01_INTERFACE
            table-id: 254        
EOF

/bin/cp -f ${BASE_DIR}/data/install/install-config.yaml ${BASE_DIR}/data/install/install-config.yaml.bak

openshift-install --dir=${BASE_DIR}/data/install agent create cluster-manifests

sudo bash -c "/bin/cp -f mirror/registries.conf /etc/containers/registries.conf.d/; chmod +r /etc/containers/registries.conf.d/*"

# /bin/cp -f  /data/ocp4/ansible-helper/files/* ${BASE_DIR}/data/install/openshift/

sudo bash -c "cd /data/ocp4 ; bash image.registries.conf.sh quaylab.infra.wzhlab.top:5443 ;"

/bin/cp -f /data/ocp4/99-worker-container-registries.yaml ${BASE_DIR}/data/install/openshift
/bin/cp -f /data/ocp4/99-master-container-registries.yaml ${BASE_DIR}/data/install/openshift

cd ${BASE_DIR}/data/install/

# openshift-install --dir=${BASE_DIR}/data/install create ignition-configs 

mkdir -p ~/.cache/agent/image_cache/
/bin/cp -f /data/ocp-$BUILDNUMBER-multi/amd64/coreos-x86_64.iso ~/.cache/agent/image_cache/coreos-x86_64.iso

openshift-install --dir=${BASE_DIR}/data/install agent create image --log-level=debug
# ......
# DEBUG Fetching image from OCP release (oc adm release info --image-for=machine-os-images --insecure=true --icsp-file=/tmp/icsp-file3636774741 quay.io/openshift-release-dev/ocp-release@sha256:96bf74ce789ccb22391deea98e0c5050c41b67cc17defbb38089d32226dba0b8)
# DEBUG The file was found in cache: /home/3node/.cache/agent/image_cache/coreos-x86_64.iso
# INFO Verifying cached file
# DEBUG extracting /coreos/coreos-x86_64.iso.sha256 to /tmp/cache1876698393, oc image extract --path /coreos/coreos-x86_64.iso.sha256:/tmp/cache1876698393 --confirm --icsp-file=/tmp/icsp-file455852761 quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:052130abddf741195b6753888cf8a00757dedeb7010f7d4dcc4b842b5bc705f6
# ......

coreos-installer iso ignition show agent.x86_64.iso > ignition.ign

# HTTP_PATH=http://192.168.7.11:8080/ignition

source /data/ocp4/acm.fn.sh

# 我们会创建一个wzh用户，密码是redhat，这个可以在第一次启动的是，从console/ssh直接用用户名口令登录
# 方便排错和研究
VAR_PWD_HASH="$(python3 -c 'import crypt,getpass; print(crypt.crypt("redhat"))')"

cat ${BASE_DIR}/data/install/ignition.ign \
  | jq --arg VAR "$VAR_PWD_HASH" --arg VAR_SSH "$NODE_SSH_KEY" '.passwd.users += [{ "name": "wzh", "system": true, "passwordHash": $VAR , "sshAuthorizedKeys": [ $VAR_SSH ], "groups": [ "adm", "wheel", "sudo", "systemd-journal"  ] }]' \
  | jq '. += { "kernel_arguments" : { "should_exist" : [ "systemd.debug-shell=1" ] } }' \
  | jq -c . \
  > ${BASE_DIR}/data/install/ignition-iso.ign

coreos-installer iso ignition embed -f -i ignition-iso.ign agent.x86_64.iso

# VAR_IMAGE_VER=rhcos-410.86.202303200936-AnolisOS-0-live.x86_64.iso


```


### cleanup kvm ( 101 )

我们准备了脚本，来清理kvm，把物理机清理成一个干净的系统。

```bash

create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}

virsh destroy ocp4-ipi-osp-master-01
virsh undefine ocp4-ipi-osp-master-01

create_lv vgdata poolA lv-ocp4-ipi-osp-master-01 500G 
create_lv vgdata poolA lv-ocp4-ipi-osp-master-01-data 500G 
create_lv vgdata poolA lv-ocp4-ipi-osp-master-01-data-02 500G 
create_lv vgdata poolA lv-ocp4-ipi-osp-master-01-data-03 500G 

virsh destroy ocp4-ipi-osp-master-02
virsh undefine ocp4-ipi-osp-master-02

create_lv vgdata poolA lv-ocp4-ipi-osp-master-02 500G 
create_lv vgdata poolA lv-ocp4-ipi-osp-master-02-data 500G 
create_lv vgdata poolA lv-ocp4-ipi-osp-master-02-data-02 500G 
create_lv vgdata poolA lv-ocp4-ipi-osp-master-02-data-03 500G 

virsh destroy ocp4-ipi-osp-master-03
virsh undefine ocp4-ipi-osp-master-03

create_lv vgdata poolA lv-ocp4-ipi-osp-master-03 500G 
create_lv vgdata poolA lv-ocp4-ipi-osp-master-03-data 500G 
create_lv vgdata poolA lv-ocp4-ipi-osp-master-03-data-02 500G 
create_lv vgdata poolA lv-ocp4-ipi-osp-master-03-data-03 500G 

# virsh destroy ocp4-ipi-osp-worker-01
# virsh undefine ocp4-ipi-osp-worker-01

# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-01 200G 
# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-01-data 500G 
# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-01-data-02 500G 
# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-01-data-03 500G 

# virsh destroy ocp4-ipi-osp-worker-02
# virsh undefine ocp4-ipi-osp-worker-02

# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-02 200G 
# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-02-data 500G 
# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-02-data-02 500G 
# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-02-data-03 500G 

# virsh destroy ocp4-ipi-osp-worker-03
# virsh undefine ocp4-ipi-osp-worker-03

# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-03 200G 
# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-03-data 500G 
# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-03-data-02 500G 
# create_lv vgdata poolA lv-ocp4-ipi-osp-worker-03-data-03 500G 

VAR_VM=`virsh list --all | grep bootstrap | awk '{print $2}'`
virsh destroy $VAR_VM
virsh undefine $VAR_VM
VAR_POOL=`virsh pool-list --all | grep bootstrap | awk '{print $1}'`
virsh pool-destroy $VAR_POOL
virsh pool-undefine $VAR_POOL
/bin/rm -rf /var/lib/libvirt/openshift-images/*
/bin/rm -rf /var/lib/libvirt/images/*


```

### start kvm ( 101 )


```bash

nmcli con mod baremetal +ipv4.address '192.168.77.101/24'
nmcli con mod baremetal ipv4.dns 192.168.77.11
nmcli con up baremetal

/bin/rm -rf /var/lib/libvirt/images/*


mkdir -p /data/kvm/one/

scp root@192.168.77.11:/home/3nodeipi/data/install/agent.x86_64.iso /data/kvm/one/


create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}

SNO_MEM=50
export KVM_DIRECTORY=/data/kvm

virsh destroy ocp4-ipi-osp-master-01
virsh undefine ocp4-ipi-osp-master-01

create_lv vgdata poolA lv-ocp4-ipi-osp-master-01 500G recreate
create_lv vgdata poolA lv-ocp4-ipi-osp-master-01-data 500G recreate
create_lv vgdata poolA lv-ocp4-ipi-osp-master-01-data-02 500G recreate
create_lv vgdata poolA lv-ocp4-ipi-osp-master-01-data-03 500G recreate

virt-install --name=ocp4-ipi-osp-master-01 --vcpus=16 --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --disk path=/dev/vgdata/lv-ocp4-ipi-osp-master-01,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lv-ocp4-ipi-osp-master-01-data,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lv-ocp4-ipi-osp-master-01-data-02,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lv-ocp4-ipi-osp-master-01-data-03,device=disk,bus=virtio,format=raw \
  --os-variant rhel8.4 \
  --network bridge=baremetal,model=virtio,mac.address=52:54:00:12:A1:01 \
  --graphics vnc,port=59003 --noautoconsole \
  --boot menu=on --cdrom /data/kvm/one/agent.x86_64.iso


```



## on helper to see result

我们需要把物理机上的密钥文件等信息，传回helper节点。方便我们后续的操作。

```bash
# on helper node

cd ${BASE_DIR}/data/install
export KUBECONFIG=${BASE_DIR}/data/install/auth/kubeconfig
echo "export KUBECONFIG=${BASE_DIR}/data/install/auth/kubeconfig" >> ~/.bashrc
# oc completion bash | sudo tee /etc/bash_completion.d/openshift > /dev/null


cd ${BASE_DIR}/data/install
openshift-install --dir=${BASE_DIR}/data/install agent wait-for bootstrap-complete --log-level=debug
# ......
# DEBUG RendezvousIP from the AgentConfig 192.168.77.43
# INFO Bootstrap Kube API Initialized
# INFO Bootstrap configMap status is complete
# INFO cluster bootstrap is complete

cd ${BASE_DIR}/data/install
openshift-install --dir=${BASE_DIR}/data/install agent wait-for install-complete --log-level=debug
# ......
# INFO Cluster is installed
# INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run
# INFO     export KUBECONFIG=/home/3nodeipi/data/install/auth/kubeconfig
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.acm-demo-one.wzhlab.top
# INFO Login to the console with user: "kubeadmin", and password: "zy99x-PCpL3-R3Vyo-KhKuV"


# if you power off cluster for long time
# you will need to re-approve the csr
oc get csr | grep -v Approved
oc get csr -ojson | jq -r '.items[] | select(.status == {} ) | .metadata.name' | xargs oc adm certificate approve

```

## daily operation

### password login and oc config

```bash

# init setting for helper node
cat << EOF > ~/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF
chmod 600 ~/.ssh/config


cat > ${BASE_DIR}/data/install/crack.txt << EOF

echo redhat | sudo passwd --stdin root

sudo sed -i "s|^PasswordAuthentication no$|PasswordAuthentication yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^PermitRootLogin no$|PermitRootLogin yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^#ClientAliveInterval 180$|ClientAliveInterval 1800|g" /etc/ssh/sshd_config

sudo systemctl restart sshd

sudo sh -c 'echo "export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig" >> /root/.bashrc'

sudo sh -c 'echo "RET=\\\`oc config use-context system:admin\\\`" >> /root/.bashrc'

EOF

for i in 23
do
  ssh core@192.168.77.$i < ${BASE_DIR}/data/install/crack.txt
done

```

from other host

```bash
# https://unix.stackexchange.com/questions/230084/send-the-password-through-stdin-in-ssh-copy-id
dnf install -y sshpass

for i in 23
do
  sshpass -p 'redhat' ssh-copy-id root@192.168.77.$i
done

```

### power off / reboot

```bash

action=poweroff
# action=reboot
for i in 23 24 25
do
  ssh root@192.168.12.$i "$action"
done

virsh shutdown openwrt-factory
virsh shutdown openwrt-edge

```

### power on

```bash

virsh start openwrt-factory
virsh start openwrt-edge

for i in {1..3}
do
  virsh start ocp4-ipi-osp-master-0$i
done

```

### check info

我们日常还会有一些集群各个节点收集信息，脚本操作的工作，也提供脚本模板，帮助日常工作。

```bash

cat > ${BASE_DIR}/data/install/crack.txt << 'EOF'

for i in {3..8}
do
  nmcli con down enp${i}s0
  nmcli con del enp${i}s0
done

EOF

for i in 23 24 25
do
  ssh root@192.168.12.$i < ${BASE_DIR}/data/install/crack.txt
done

```


## deploy NFS on hub

```bash

# go to master-03, this is as storage node
# create the storage path
cat > ${BASE_DIR}/data/install/crack.txt << 'EOF'
mkdir -p /var/wzh-local-pv/
chcon -Rt container_file_t /var/wzh-local-pv/
EOF
ssh root@192.168.77.25 < ${BASE_DIR}/data/install/crack.txt

# on helper
cat << EOF > ${BASE_DIR}/data/install/local-pv.yaml
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-volume
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-local-pv
spec:
  capacity:
    storage: 400Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-volume
  local:
    path: /var/wzh-local-pv/
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - one-master-03.acm-demo-one.wzhlab.top
EOF
oc create --save-config -f ${BASE_DIR}/data/install/local-pv.yaml

# oc delete -f ${BASE_DIR}/data/install/local-pv.yaml

# move container image to quay.io
skopeo copy docker://k8s.gcr.io/sig-storage/nfs-provisioner:v3.0.0 docker://quay.io/wangzheng422/nfs-provisioner:v3.0.0

oc create ns nfs-system

# oc project nfs-system

cd ${BASE_DIR}/data/install

export http_proxy="http://127.0.0.1:18801"
export https_proxy=${http_proxy}

wget -O nfs.all.yaml https://raw.githubusercontent.com/wangzheng422/nfs-ganesha-server-and-external-provisioner/wzh/deploy/openshift/nfs.all.local.pv.wzhlab.yaml

unset http_proxy
unset https_proxy

/bin/cp -f nfs.all.yaml nfs.all.yaml.run

# sed -i 's/storageClassName: odf-lvm-vg1/storageClassName: local-volume/' nfs.all.yaml.run
sed -i 's/one-master-03.acm-demo-one.redhat.ren/one-master-03.acm-demo-one.wzhlab.top/' nfs.all.yaml.run
sed -i 's/storage: 5Gi/storage: 400Gi/' nfs.all.yaml.run
sed -i 's#k8s.gcr.io/sig-storage/nfs-provisioner:v3.0.0#quay.io/wangzheng422/nfs-provisioner:v3.0.0#' nfs.all.yaml.run

oc create --save-config -n nfs-system -f nfs.all.yaml.run

# oc delete -n nfs-system -f nfs.all.yaml.run

oc patch storageclass wzhlab-top-nfs -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

```

## add bf2 to sno cluster

### build bf2 iso

```bash
# on helper
cd ${BASE_DIR}/data/install/

/bin/cp -f /data/ocp-$BUILDNUMBER/rhcos-live.aarch64.iso ./ocp-bf2-aarch64.iso

oc extract -n openshift-machine-api secret/worker-user-data-managed --keys=userData --to=- > bf2.ign

coreos-installer iso ignition embed ocp-bf2-aarch64.iso --ignition-file bf2.ign -f

# try to run on bf2
# on helper
scp /data/ocp-$BUILDNUMBER/rhcos-live.aarch64.iso root@192.168.77.55:/root/bf2/ocp-bf2-aarch64.iso
scp bf2.ign root@192.168.77.55:/root/bf2/

# on bf2
cd /root/bf2
wget -O coreos-installer https://mirror.openshift.com/pub/openshift-v4/clients/coreos-installer/latest/coreos-installer_arm64
chmod +x coreos-installer
./coreos-installer iso ignition embed ocp-bf2-aarch64.iso --ignition-file bf2.ign -f


# try x86
cd ${BASE_DIR}/data/install/

/bin/cp -f /data/ocp-$BUILDNUMBER/rhcos-live.x86_64.iso ./worker.iso

oc extract -n openshift-machine-api secret/worker-user-data --keys=userData --to=- > bf2.ign

sudo bash -c "/bin/cp -f bf2.ign /data/dnf/"

coreos-installer iso kargs modify -a "ip=$WORKER_01_IP::$OCP_GW:$OCP_NETMASK:$WORKER_01_HOSTNAME:$WORKER_01_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=$WORKER_01_DISK coreos.inst.ignition_url=http://192.168.77.11:5000/bf2.ign  coreos.inst.insecure systemd.debug-shell=1 " worker.iso

```

### boot the bf2 with the iso

```bash
# copy iso to 103
# on 103
cd /data/down/
scp root@192.168.77.11:/home/3nodeipi/data/install/ocp-bf2-aarch64.iso  ./
# scp root@192.168.77.11:/home/3nodeipi/down/ocp-bf2-aarch64-rootfs.img  ./
scp root@192.168.77.11:/home/3nodeipi/data/install/worker.iso  ./

scp root@192.168.77.55:/root/bf2/ocp-bf2-aarch64.iso ./

# python3 -m http.server 8080

export RHEL_ISO=ocp-bf2-aarch64.iso
# export BASE_ISO=Rocky-8.7-aarch64-minimal.iso
# shell come frome https://github.com/wangzheng422/rhel-on-bf2
bash bluefield_provision.sh -s
# === STATUS === Checking usability of SRIOV for PCI 0000:05:00.0
# SRIOV enabled
# EMBEDDED_CPU mode enabled
# === STATUS === Checking usability of SRIOV for PCI 0000:05:00.1
# SRIOV enabled
# EMBEDDED_CPU mode enabled

setenforce 0

# bash bluefield_provision.sh -p

iptables -F
bash ./PXE_setup_RHEL_install_over_mlx.aarch64.sh -i "${RHEL_ISO}" -p tmfifo 


# uplink_interface="$(ip route |grep ^default | sed 's/.*dev \([^ ]\+\).*/\1/')"
# test -n "${uplink_interface}" || die "need a default route"
    
# iptables -t nat -A POSTROUTING -o "${uplink_interface}" -j MASQUERADE

# on a new terminal
minicom --color on --baudrate 115200 --device /dev/rshim0/console

# change back to original terminal
# to reboot the bf2
echo BOOT_MODE 1 > /dev/rshim0/misc
echo SW_RESET 1 > /dev/rshim0/misc


# sleep 10
# ESC during bf2 boot, to go into bios config
# select boot from network 8
# before continue, restart nic and dhcpd on host
nmcli conn up tmfifo_net0
systemctl restart dhcpd

# iptables -t nat -A POSTROUTING -o "${uplink_interface}" -j MASQUERADE

# after booting
ssh core@192.168.77.55

```
![](imgs/2023-02-17-23-46-46.png)

```bash
# the disk on bf2 only have 64G
# we need to manual set the disk to install
oc get agent -A
# NAMESPACE      NAME                                   CLUSTER   APPROVED   ROLE          STAGE
# acm-demo-two   bc1b207c-5917-ec11-8000-08c0eb3e8cee             true       auto-assign

cat << EOF > ${BASE_DIR}/data/install/patch.yaml
spec:
  hostname: bf2-worker-103
  approved: true
  installation_disk_id: /dev/mmcblk0
  role: ""
EOF
oc patch -n ${ACM_DEMO_CLUSTER} agent/bc1b207c-5917-ec11-8000-08c0eb3e8cee --type merge --patch-file=${BASE_DIR}/data/install/patch.yaml

# make some changes to agent object
oc get agent/bc1b207c-5917-ec11-8000-08c0eb3e8cee -n acm-demo-two -o yaml | yq .spec
# approved: true
# hostname: bf2-worker-103
# installation_disk_id: /dev/mmcblk0
# role: ""

# hack namespace: multicluster-engine -> config map name: assisted-service
# change disk space need to 10G


```


# end

```bash


create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}

SNO_MEM=20
export KVM_DIRECTORY=/data/kvm

virsh destroy ocp4-ipi-osp-worker-01
virsh undefine ocp4-ipi-osp-worker-01

create_lv vgdata poolA lv-ocp4-ipi-osp-worker-01 500G recreate
create_lv vgdata poolA lv-ocp4-ipi-osp-worker-01-data 500G recreate


virt-install --name=ocp4-ipi-osp-worker-01 --vcpus=16 --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --disk path=/dev/vgdata/lv-ocp4-ipi-osp-worker-01,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lv-ocp4-ipi-osp-worker-01-data,device=disk,bus=virtio,format=raw \
  --os-variant rhel8.4 \
  --network bridge=baremetal,model=virtio,mac.address=52:54:00:12:A1:02 \
  --graphics vnc,port=59003 --noautoconsole \
  --boot menu=on --cdrom /data/down/worker.iso


```
