# ocp 4.10 upi

# UPI

## try to config the ocp install

然后，我们就开始定义ocp的安装install配置文件，并且由于我们是UPI安装，我们还要定制iso。

Then, we start to define the installation configuration file of ocp, and since we are installing using UPI, we also need to customize the iso.

```bash

export BUILDNUMBER=4.10.45

pushd /data/ocp4/${BUILDNUMBER}
tar -xzf openshift-client-linux-${BUILDNUMBER}.tar.gz -C /usr/local/bin/
tar -xzf openshift-install-linux-${BUILDNUMBER}.tar.gz -C /usr/local/bin/
# tar -xzf oc-mirror.tar.gz -C /usr/local/bin/
# chmod +x /usr/local/bin/oc-mirror
# install -m 755 /data/ocp4/clients/butane-amd64 /usr/local/bin/butane
# install -m 755 /data/ocp4/clients/coreos-installer_amd64 /usr/local/bin/coreos-installer
popd


# create a user and create the cluster under the user


useradd -m 3node2
# useradd -G wheel 3node

usermod -aG wheel 3node2

echo -e "%wheel\tALL=(ALL)\tNOPASSWD: ALL" > /etc/sudoers.d/020_sudo_for_me

su - 3node2

ssh-keygen

cat << EOF > ~/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

chmod 600 ~/.ssh/config

cat << 'EOF' >> ~/.bashrc

export BASE_DIR='/home/3node2/'

EOF

# export BASE_DIR='/home/3node/'

mkdir -p ${BASE_DIR}/data/{sno/disconnected,install}

# set some parameter of you rcluster

NODE_SSH_KEY="$(cat ${BASE_DIR}/.ssh/id_rsa.pub)"
INSTALL_IMAGE_REGISTRY=quaylab.infra.wzhlab.top:8443

PULL_SECRET='{"auths":{"registry.redhat.io": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"registry.ocp4.redhat.ren:5443": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"'${INSTALL_IMAGE_REGISTRY}'": {"auth": "'$( echo -n 'admin:redhatadmin' | openssl base64 )'","email": "noemail@localhost"}}}'

# NTP_SERVER=192.168.7.11
# HELP_SERVER=192.168.7.11
# KVM_HOST=192.168.7.11
# API_VIP=192.168.7.100
# INGRESS_VIP=192.168.7.101
# CLUSTER_PROVISION_IP=192.168.7.103
# BOOTSTRAP_IP=192.168.7.12

# 定义单节点集群的节点信息
SNO_CLUSTER_NAME=acm-demo-two
SNO_BASE_DOMAIN=wzhlab.top

# echo ${SNO_IF_MAC} > /data/sno/sno.mac

mkdir -p ${BASE_DIR}/data/install
cd ${BASE_DIR}/data/install

/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap manifests master*[0-9] worker*[0-9] 

cat << EOF > ${BASE_DIR}/data/install/install-config.yaml 
apiVersion: v1
baseDomain: $SNO_BASE_DOMAIN
compute:
- name: worker
  replicas: 0 
controlPlane:
  name: master
  replicas: 3 
metadata:
  name: $SNO_CLUSTER_NAME
networking:
  # OVNKubernetes , OpenShiftSDN
  networkType: OVNKubernetes
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  - cidr: fd01::/48
    hostPrefix: 64
  serviceNetwork:
  - 172.30.0.0/16
  - fd02::/112
  machineNetwork:
  - cidr: 10.0.0.0/16
  - cidr: fd03::/64
platform:
  none: {}
pullSecret: '${PULL_SECRET}'
sshKey: |
$( cat ${BASE_DIR}/.ssh/id_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /etc/crts/redhat.ren.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - ${INSTALL_IMAGE_REGISTRY}/openshift/release-images
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - ${INSTALL_IMAGE_REGISTRY}/openshift/release
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

/bin/cp -f ${BASE_DIR}/data/install/install-config.yaml ${BASE_DIR}/data/install/install-config.yaml.bak

openshift-install create manifests --dir=${BASE_DIR}/data/install

# additional ntp config
/bin/cp -f  /data/ocp4/ansible-helper/files/* ${BASE_DIR}/data/install/openshift/

#############################################
# run as root if you have not run below, at least one time
# it will generate registry configuration
# copy image registry proxy related config
# cd /data/ocp4
# bash image.registries.conf.sh nexus.infra.redhat.ren:8083

# /bin/cp -f /data/ocp4/image.registries.conf /etc/containers/registries.conf.d/
#############################################

sudo bash -c "cd /data/ocp4 ; bash image.registries.conf.quay.sh quaylab.infra.wzhlab.top:8443 ;"

/bin/cp -f /data/ocp4/99-worker-container-registries.yaml ${BASE_DIR}/data/install/openshift
/bin/cp -f /data/ocp4/99-master-container-registries.yaml ${BASE_DIR}/data/install/openshift

cd ${BASE_DIR}/data/install/

openshift-install --dir=${BASE_DIR}/data/install create ignition-configs 

BOOTSTRAP_IP=192.168.77.32
MASTER_01_IP=192.168.77.33
MASTER_02_IP=192.168.77.34
MASTER_03_IP=192.168.77.35

BOOTSTRAP_IPv6=fd03::32
MASTER_01_IPv6=fd03::33
MASTER_02_IPv6=fd03::34
MASTER_03_IPv6=fd03::35

BOOTSTRAP_HOSTNAME=bootstrap-demo
MASTER_01_HOSTNAME=master-01-demo
MASTER_02_HOSTNAME=master-02-demo
MASTER_03_HOSTNAME=master-03-demo

BOOTSTRAP_INTERFACE=enp1s0
MASTER_01_INTERFACE=enp1s0
MASTER_02_INTERFACE=enp1s0
MASTER_03_INTERFACE=enp1s0

BOOTSTRAP_DISK=/dev/vda
MASTER_01_DISK=/dev/vda
MASTER_02_DISK=/dev/vda
MASTER_03_DISK=/dev/vda

OCP_GW=192.168.77.11
OCP_NETMASK=255.255.255.0
OCP_NETMASK_S=24
OCP_DNS=192.168.77.11

OCP_GW_v6=fd03::11
OCP_NETMASK_v6=64

# HTTP_PATH=http://192.168.7.11:8080/ignition

source /data/ocp4/acm.fn.sh

# 我们会创建一个wzh用户，密码是redhat，这个可以在第一次启动的是，从console/ssh直接用用户名口令登录
# 方便排错和研究
VAR_PWD_HASH="$(python3 -c 'import crypt,getpass; print(crypt.crypt("redhat"))')"

cat ${BASE_DIR}/data/install/bootstrap.ign \
  | jq --arg VAR "$VAR_PWD_HASH" --arg VAR_SSH "$NODE_SSH_KEY" '.passwd.users += [{ "name": "wzh", "system": true, "passwordHash": $VAR , "sshAuthorizedKeys": [ $VAR_SSH ], "groups": [ "adm", "wheel", "sudo", "systemd-journal"  ] }]' \
  | jq -c . \
  > ${BASE_DIR}/data/install/bootstrap-iso.ign

cat ${BASE_DIR}/data/install/master.ign \
  | jq --arg VAR "$VAR_PWD_HASH" --arg VAR_SSH "$NODE_SSH_KEY" '.passwd.users += [{ "name": "wzh", "system": true, "passwordHash": $VAR , "sshAuthorizedKeys": [ $VAR_SSH ], "groups": [ "adm", "wheel", "sudo", "systemd-journal"  ] }]' \
  | jq -c . \
  > ${BASE_DIR}/data/install/master-iso.ign

# VAR_IMAGE_VER=410.91.202211291516-wzh-0

cd ${BASE_DIR}/data/install/
/bin/cp -f /data/ocp4/rhcos-live.x86_64.iso bootstrap.iso
/bin/cp -f bootstrap.iso master01.iso
/bin/cp -f bootstrap.iso master02.iso
/bin/cp -f bootstrap.iso master03.iso
# sudo /bin/cp -f /data/work/ext-client/iso/rhcos-$VAR_IMAGE_VER-metal.x86_64.raw /data/dnf/
sudo /bin/cp -f ${BASE_DIR}/data/install/{bootstrap,master}-iso.ign /data/dnf/

# for ipv4 only
coreos-installer iso kargs modify -a "ip=$BOOTSTRAP_IP::$OCP_GW:$OCP_NETMASK:$BOOTSTRAP_HOSTNAME:$BOOTSTRAP_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=$BOOTSTRAP_DISK coreos.inst.ignition_url=http://192.168.77.11:5000/bootstrap-iso.ign  coreos.inst.insecure" bootstrap.iso

coreos-installer iso kargs modify -a "ip=$MASTER_01_IP::$OCP_GW:$OCP_NETMASK:$MASTER_01_HOSTNAME:$MASTER_01_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=$MASTER_01_DISK coreos.inst.ignition_url=http://192.168.77.11:5000/master-iso.ign  coreos.inst.insecure" master01.iso

coreos-installer iso kargs modify -a "ip=$MASTER_02_IP::$OCP_GW:$OCP_NETMASK:$MASTER_02_HOSTNAME:$MASTER_02_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=$MASTER_02_DISK coreos.inst.ignition_url=http://192.168.77.11:5000/master-iso.ign coreos.inst.image_url=http://192.168.77.11:5000/rhcos-$VAR_IMAGE_VER-metal.x86_64.raw coreos.inst.insecure" master02.iso

coreos-installer iso kargs modify -a "ip=$MASTER_03_IP::$OCP_GW:$OCP_NETMASK:$MASTER_03_HOSTNAME:$MASTER_03_INTERFACE:none nameserver=$OCP_DNS coreos.inst.install_dev=$MASTER_03_DISK coreos.inst.ignition_url=http://192.168.77.11:5000/master-iso.ign  coreos.inst.insecure" master03.iso


# for ipv4 / ipv6 dual stack
coreos-installer iso kargs modify -a " ip=$BOOTSTRAP_IP::$OCP_GW:$OCP_NETMASK:$BOOTSTRAP_HOSTNAME:$BOOTSTRAP_INTERFACE:none   nameserver=$OCP_DNS   ip=[$BOOTSTRAP_IPv6]::[$OCP_GW_v6]:$OCP_NETMASK_v6:$BOOTSTRAP_HOSTNAME:$BOOTSTRAP_INTERFACE:none   coreos.inst.install_dev=$BOOTSTRAP_DISK   coreos.inst.ignition_url=http://192.168.77.11:5000/bootstrap-iso.ign      coreos.inst.insecure " bootstrap.iso

coreos-installer iso kargs modify -a " ip=$MASTER_01_IP::$OCP_GW:$OCP_NETMASK:$MASTER_01_HOSTNAME:$MASTER_01_INTERFACE:none   nameserver=$OCP_DNS   ip=[$MASTER_01_IPv6]::[$OCP_GW_v6]:$OCP_NETMASK_v6:$MASTER_01_HOSTNAME:$MASTER_01_INTERFACE:none  coreos.inst.install_dev=$MASTER_01_DISK   coreos.inst.ignition_url=http://192.168.77.11:5000/master-iso.ign      coreos.inst.insecure " master01.iso

coreos-installer iso kargs modify -a " ip=$MASTER_02_IP::$OCP_GW:$OCP_NETMASK:$MASTER_02_HOSTNAME:$MASTER_02_INTERFACE:none   nameserver=$OCP_DNS   ip=[$MASTER_02_IPv6]::[$OCP_GW_v6]:$OCP_NETMASK_v6:$MASTER_02_HOSTNAME:$MASTER_02_INTERFACE:none  coreos.inst.install_dev=$MASTER_02_DISK   coreos.inst.ignition_url=http://192.168.77.11:5000/master-iso.ign      coreos.inst.insecure " master02.iso

coreos-installer iso kargs modify -a " ip=$MASTER_03_IP::$OCP_GW:$OCP_NETMASK:$MASTER_03_HOSTNAME:$MASTER_03_INTERFACE:none   nameserver=$OCP_DNS   ip=[$MASTER_03_IPv6]::[$OCP_GW_v6]:$OCP_NETMASK_v6:$MASTER_03_HOSTNAME:$MASTER_03_INTERFACE:none  coreos.inst.install_dev=$MASTER_03_DISK   coreos.inst.ignition_url=http://192.168.77.11:5000/master-iso.ign      coreos.inst.insecure " master03.iso

```



## deploy on kvm host

有了iso文件，我们就可以用他们启动kvm，开始安装了，这一部分，可以参考引用文档，这里就不重复写了。
- [following here](../4.11/4.11.3node.installer.embed.registry.md#on-kvm-host--103-)

With the iso files, we can use them to start kvm and start the installation. For this part, you can refer to the reference document, so I will not repeat it here.



# on kvm host ( 103 )

## cleanup

```bash

create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}

virsh destroy ocp4-acm-two-bootstrap
virsh undefine ocp4-acm-two-bootstrap

create_lv vgdata poolA lvacm-two-bootstrap 100G 
create_lv vgdata poolA lvacm-two-bootstrap-data 100G 

virsh destroy ocp4-acm-two-master-01
virsh undefine ocp4-acm-two-master-01

create_lv vgdata poolA lvacm-two-master-01 100G 
create_lv vgdata poolA lvacm-two-master-01-data 100G 

virsh destroy ocp4-acm-two-master-02
virsh undefine ocp4-acm-two-master-02

create_lv vgdata poolA lvacm-two-master-02 100G 
create_lv vgdata poolA lvacm-two-master-02-data 100G 

virsh destroy ocp4-acm-two-master-03
virsh undefine ocp4-acm-two-master-03

create_lv vgdata poolA lvacm-two-master-03 100G 
create_lv vgdata poolA lvacm-two-master-03-data 100G 

```

## begin

```bash

cat << EOF >> /etc/sysctl.d/99-wzh-sysctl.conf

vm.overcommit_memory = 1

EOF
sysctl --system

# 创建实验用虚拟网络

mkdir -p /data/kvm
cd /data/kvm

cat << 'EOF' > /data/kvm/bridge.sh
#!/usr/bin/env bash

PUB_CONN='eno1'
PUB_IP='172.21.6.103/24'
PUB_GW='172.21.6.254'
PUB_DNS='172.21.1.1'

nmcli con down "$PUB_CONN"
nmcli con delete "$PUB_CONN"
nmcli con down baremetal
nmcli con delete baremetal
# RHEL 8.1 appends the word "System" in front of the connection,delete in case it exists
nmcli con down "System $PUB_CONN"
nmcli con delete "System $PUB_CONN"
nmcli connection add ifname baremetal type bridge con-name baremetal ipv4.method 'manual' \
    ipv4.address "$PUB_IP" \
    ipv4.gateway "$PUB_GW" \
    ipv4.dns "$PUB_DNS"
    
nmcli con add type bridge-slave ifname "$PUB_CONN" master baremetal
nmcli con down "$PUB_CONN";pkill dhclient;dhclient baremetal
nmcli con up baremetal
EOF
bash /data/kvm/bridge.sh

nmcli con mod baremetal +ipv4.addresses "192.168.7.103/24"
nmcli con up baremetal

cat << EOF > /root/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

pvcreate -y /dev/vdb
vgcreate vgdate /dev/vdb

# https://access.redhat.com/articles/766133
lvcreate -y -n poolA -L 500G vgdata
lvcreate -y -n poolA_meta -L 10G vgdata
lvconvert -y --thinpool vgdata/poolA --poolmetadata vgdata/poolA_meta

lvextend -l +100%FREE vgdata/poolA

mkdir -p /data/kvm/one/

for var_file in bootstrap master01 master02 master03
do
  scp root@192.168.77.11:/home/3node2/data/install/$var_file.iso /data/kvm/one/
done

create_lv() {
    var_vg=$1
    var_pool=$2
    var_lv=$3
    var_size=$4
    var_action=$5
    lvremove -f $var_vg/$var_lv
    # lvcreate -y -L $var_size -n $var_lv $var_vg
    if [ "$var_action" == "recreate" ]; then
      lvcreate --type thin -n $var_lv -V $var_size --thinpool $var_vg/$var_pool
      wipefs --all --force /dev/$var_vg/$var_lv
    fi
}

virsh destroy ocp4-acm-two-bootstrap
virsh undefine ocp4-acm-two-bootstrap

create_lv vgdata poolA lvacm-two-bootstrap 100G recreate
create_lv vgdata poolA lvacm-two-bootstrap-data 100G recreate

SNO_MEM=32

virt-install --name=ocp4-acm-two-bootstrap --vcpus=16 --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --disk path=/dev/vgdata/lvacm-two-bootstrap,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lvacm-two-bootstrap-data,device=disk,bus=virtio,format=raw \
  --os-variant rhel8.3 --network bridge=baremetal,model=virtio \
  --graphics vnc,port=59002 \
  --boot menu=on --cdrom /data/kvm/one/bootstrap.iso 

# --disk path=/dev/vgdata/lvacmhub-data,device=disk,bus=virtio,format=raw \

virsh destroy ocp4-acm-two-master-01
virsh undefine ocp4-acm-two-master-01

create_lv vgdata poolA lvacm-two-master-01 100G recreate
create_lv vgdata poolA lvacm-two-master-01-data 100G recreate

virt-install --name=ocp4-acm-two-master-01 --vcpus=16 --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --disk path=/dev/vgdata/lvacm-two-master-01,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lvacm-two-master-01-data,device=disk,bus=virtio,format=raw \
  --os-variant rhel8.3 --network bridge=baremetal,model=virtio \
  --graphics vnc,port=59003 \
  --boot menu=on --cdrom /data/kvm/one/master01.iso 

virsh destroy ocp4-acm-two-master-02
virsh undefine ocp4-acm-two-master-02

create_lv vgdata poolA lvacm-two-master-02 100G recreate
create_lv vgdata poolA lvacm-two-master-02-data 100G recreate

virt-install --name=ocp4-acm-two-master-02 --vcpus=16 --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --disk path=/dev/vgdata/lvacm-two-master-02,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lvacm-two-master-02-data,device=disk,bus=virtio,format=raw \
  --os-variant rhel8.3 --network bridge=baremetal,model=virtio \
  --graphics vnc,port=59004 \
  --boot menu=on --cdrom /data/kvm/one/master02.iso 

virsh destroy ocp4-acm-two-master-03
virsh undefine ocp4-acm-two-master-03

create_lv vgdata poolA lvacm-two-master-03 100G recreate
create_lv vgdata poolA lvacm-two-master-03-data 100G recreate

virt-install --name=ocp4-acm-two-master-03 --vcpus=16 --ram=$(($SNO_MEM*1024)) \
  --cpu=host-model \
  --disk path=/dev/vgdata/lvacm-two-master-03,device=disk,bus=virtio,format=raw \
  --disk path=/dev/vgdata/lvacm-two-master-03-data,device=disk,bus=virtio,format=raw \
  --os-variant rhel8.3 --network bridge=baremetal,model=virtio \
  --graphics vnc,port=59005 \
  --boot menu=on --cdrom /data/kvm/one/master03.iso 

```

# on helper to see result

```bash
cd ${BASE_DIR}/data/install
export KUBECONFIG=${BASE_DIR}/data/install/auth/kubeconfig
echo "export KUBECONFIG=${BASE_DIR}/data/install/auth/kubeconfig" >> ~/.bashrc
# oc completion bash | sudo tee /etc/bash_completion.d/openshift > /dev/null

cd ${BASE_DIR}/data/install
openshift-install wait-for bootstrap-complete --log-level debug
# ......
# INFO Waiting up to 30m0s (until 4:44PM) for bootstrapping to complete...
# DEBUG Bootstrap status: complete
# INFO It is now safe to remove the bootstrap resources
# DEBUG Time elapsed per stage:
# DEBUG Bootstrap Complete: 9m33s
# INFO Time elapsed: 9m33s

openshift-install wait-for install-complete --log-level debug
# INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/3node2/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.acm-demo-two.wzhlab.top
# INFO Login to the console with user: "kubeadmin", and password: "agxSn-2XGoz-qyD79-yWvM2"
# DEBUG Time elapsed per stage:
# DEBUG Cluster Operators: 4m47s
# INFO Time elapsed: 4m47s

```


## result

等着安装完成，什么都不需要做，然后运行下面的命令，就能得到我们集群的登录参数了。

之后，我们登录到节点，就能看到，节点的kernel已经升级好了。

Wait for the installation to complete, you don't need to do anything, and then run the following command to get the login parameters of our cluster.

After that, when we log in to the node, we can see that the kernel of the node has been upgraded.

```bash

openshift-install wait-for install-complete --log-level debug
# ......
# INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/3node/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.acm-demo-one.wzhlab.top
# INFO Login to the console with user: "kubeadmin", and password: "hywxk-JVZvb-ALk38-9ewXh"
# INFO Time elapsed: 0s

```

# password login and oc config

```bash

# init setting for helper node
cat << EOF > ~/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF
chmod 600 ~/.ssh/config

cat > ${BASE_DIR}/data/install/crack.txt << EOF
sudo rm -f /root/.bashrc

echo redhat | sudo passwd --stdin root

sudo sed -i "s|^PasswordAuthentication no$|PasswordAuthentication yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^PermitRootLogin no$|PermitRootLogin yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^#ClientAliveInterval 180$|ClientAliveInterval 1800|g" /etc/ssh/sshd_config

sudo systemctl restart sshd

sudo sh -c 'echo "export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig" >> /root/.bashrc'

sudo sh -c 'echo "RET=\\\`oc config use-context system:admin\\\`" >> /root/.bashrc'

EOF

for i in 33 34 35
do
  ssh core@192.168.77.$i < ${BASE_DIR}/data/install/crack.txt
done

```

## from other host

```bash
# https://unix.stackexchange.com/questions/230084/send-the-password-through-stdin-in-ssh-copy-id
dnf install -y sshpass

for i in 33 34 35
do
  sshpass -p 'redhat' ssh-copy-id root@192.168.77.$i
done

```

## log into ocp to check

我们登录到openshift里面，看看成果吧。

Let's log in to openshift and see the results.

```bash
# login to master-01
uname -a
# Linux master-01-demo 5.14.0-162.6.1.el9_1.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Sep 30 07:36:03 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux

cat /etc/os-release
# NAME="Red Hat Enterprise Linux CoreOS"
# ID="rhcos"
# ID_LIKE="rhel fedora"
# VERSION="410.91.202211291516-wzh-0"
# VERSION_ID="4.10"
# VARIANT="CoreOS"
# VARIANT_ID=coreos
# PLATFORM_ID="platform:el9"
# PRETTY_NAME="Red Hat Enterprise Linux CoreOS 410.91.202211291516-wzh-0 (Plow)"
# ANSI_COLOR="0;31"
# CPE_NAME="cpe:/o:redhat:enterprise_linux:9::coreos"
# HOME_URL="https://www.redhat.com/"
# DOCUMENTATION_URL="https://docs.openshift.com/container-platform/4.10/"
# BUG_REPORT_URL="https://bugzilla.redhat.com/"
# REDHAT_BUGZILLA_PRODUCT="OpenShift Container Platform"
# REDHAT_BUGZILLA_PRODUCT_VERSION="4.10"
# REDHAT_SUPPORT_PRODUCT="OpenShift Container Platform"
# REDHAT_SUPPORT_PRODUCT_VERSION="4.10"
# OPENSHIFT_VERSION="4.10"
# RHEL_VERSION="9.1"
# OSTREE_VERSION="410.91.202211291516-wzh-0"

lscpu
# Architecture:            x86_64
#   CPU op-mode(s):        32-bit, 64-bit
#   Address sizes:         48 bits physical, 48 bits virtual
#   Byte Order:            Little Endian
# CPU(s):                  128
#   On-line CPU(s) list:   0-127
# Vendor ID:               HygonGenuine
#   BIOS Vendor ID:        Chengdu Hygon
#   Model name:            Hygon C86 7285 32-core Processor
#     BIOS Model name:     Hygon C86 7285 32-core Processor
#     CPU family:          24
#     Model:               1
#     Thread(s) per core:  2
#     Core(s) per socket:  32
#     Socket(s):           2
#     Stepping:            1
#     Frequency boost:     enabled
#     CPU max MHz:         2000.0000
#     CPU min MHz:         1200.0000
#     BogoMIPS:            4000.04
#     Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rd
#                          tscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_
#                          2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce
#                           topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clf
#                          lushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid d
#                          ecodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca sme sev sev_es
# Virtualization features:
#   Virtualization:        AMD-V
# Caches (sum of all):
#   L1d:                   2 MiB (64 instances)
#   L1i:                   4 MiB (64 instances)
#   L2:                    32 MiB (64 instances)
#   L3:                    128 MiB (16 instances)
# NUMA:
#   NUMA node(s):          8
#   NUMA node0 CPU(s):     0-7,64-71
#   NUMA node1 CPU(s):     8-15,72-79
#   NUMA node2 CPU(s):     16-23,80-87
#   NUMA node3 CPU(s):     24-31,88-95
#   NUMA node4 CPU(s):     32-39,96-103
#   NUMA node5 CPU(s):     40-47,104-111
#   NUMA node6 CPU(s):     48-55,112-119
#   NUMA node7 CPU(s):     56-63,120-127
# Vulnerabilities:
#   Itlb multihit:         Not affected
#   L1tf:                  Not affected
#   Mds:                   Not affected
#   Meltdown:              Not affected
#   Mmio stale data:       Not affected
#   Retbleed:              Mitigation; untrained return thunk; SMT vulnerable
#   Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl
#   Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization
#   Spectre v2:            Mitigation; Retpolines, IBPB conditional, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected
#   Srbds:                 Not affected
#   Tsx async abort:       Not affected


oc get mcp
# NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
# master   rendered-master-e21c00ca880030866d0c598d24ca301b   True      False      False      3              3                   3                     0                      40m
# worker   rendered-worker-537f39ac419adbe3ede22a4d09132329   True      False      False      0              0                   0                     0                      40m

oc get node
# NAME             STATUS   ROLES           AGE   VERSION
# master-01-demo   Ready    master,worker   45m   v1.23.12+8a6bfe4
# master-02-demo   Ready    master,worker   44m   v1.23.12+8a6bfe4
# master-03-demo   Ready    master,worker   43m   v1.23.12+8a6bfe4

oc get clusterversion
# NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
# version   4.10.41   True        False         5h30m   Cluster version is 4.10.41

oc get co
# NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
# authentication                             4.10.41   True        False         False      19m
# baremetal                                  4.10.41   True        False         False      43m
# cloud-controller-manager                   4.10.41   True        False         False      49m
# cloud-credential                           4.10.41   True        False         False      50m
# cluster-autoscaler                         4.10.41   True        False         False      43m
# config-operator                            4.10.41   True        False         False      44m
# console                                    4.10.41   True        False         False      28m
# csi-snapshot-controller                    4.10.41   True        False         False      32m
# dns                                        4.10.41   True        False         False      32m
# etcd                                       4.10.41   True        False         False      42m
# image-registry                             4.10.41   True        False         False      30m
# ingress                                    4.10.41   True        False         False      32m
# insights                                   4.10.41   True        False         False      90s
# kube-apiserver                             4.10.41   True        False         False      40m
# kube-controller-manager                    4.10.41   True        False         False      41m
# kube-scheduler                             4.10.41   True        False         False      40m
# kube-storage-version-migrator              4.10.41   True        False         False      30m
# machine-api                                4.10.41   True        False         False      43m
# machine-approver                           4.10.41   True        False         False      43m
# machine-config                             4.10.41   True        False         False      43m
# marketplace                                4.10.41   True        False         False      43m
# monitoring                                 4.10.41   True        False         False      36m
# network                                    4.10.41   True        False         False      44m
# node-tuning                                4.10.41   True        False         False      43m
# openshift-apiserver                        4.10.41   True        False         False      32m
# openshift-controller-manager               4.10.41   True        False         False      32m
# openshift-samples                          4.10.41   True        False         False      37m
# operator-lifecycle-manager                 4.10.41   True        False         False      43m
# operator-lifecycle-manager-catalog         4.10.41   True        False         False      43m
# operator-lifecycle-manager-packageserver   4.10.41   True        False         False      32m
# service-ca                                 4.10.41   True        False         False      44m
# storage                                    4.10.41   True        False         False      44m
```

![](imgs/2022-11-30-13-36-03.png)

# end