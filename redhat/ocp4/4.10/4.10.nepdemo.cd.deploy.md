# nepdemo cd deploy

# vultr 

```bash
# sync content to cloud
rm -rf /data/ocp4/

mkdir -p /data/ocp4/
cd /data/ocp4

export BUILDNUMBER=4.10.35

wget -O openshift-client-linux-${BUILDNUMBER}.tar.gz https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/${BUILDNUMBER}/openshift-client-linux-${BUILDNUMBER}.tar.gz
wget -O openshift-install-linux-${BUILDNUMBER}.tar.gz https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/${BUILDNUMBER}/openshift-install-linux-${BUILDNUMBER}.tar.gz

tar -xzf openshift-client-linux-${BUILDNUMBER}.tar.gz -C /usr/local/sbin/
tar -xzf openshift-install-linux-${BUILDNUMBER}.tar.gz -C /usr/local/sbin/


rm -rf /data/ocp4/

mkdir -p /data/ocp4/tmp
cd /data/ocp4/tmp
git clone https://github.com/wangzheng422/openshift4-shell
cd openshift4-shell
git checkout ocp-4.11
/bin/cp -f prepare.content.with.oc.mirror.sh /data/ocp4/

rm -rf /data/ocp4/tmp

cd /data/ocp4

bash prepare.content.with.oc.mirror.sh -v ${BUILDNUMBER}, -m ${BUILDNUMBER%.*} -b ocp-4.11

# download from cloud
cd /root/tmp/ocp4.10 && \
  rsync -e ssh --info=progress2 -P --delete -arz root@v.wzhlab.top:/data/ocp4/ ./

# sync to cd
cd /root/tmp/ocp4.10 && \
  rsync -e "ssh -J root@****.redhat.ren" --info=progress2 -P --delete -arz  ./ root@172.19.3.60:/data/ocp4/

```

# dns setup on aliyun

*.ocp-nep-cd.wzhlab.top -> 172.19.3.252
*.apps.ocp-nep-cd.wzhlab.top -> 172.19.3.252

# mirror

```bash

mkdir -p /data/ocp4
cd /data/ocp4

export BUILDNUMBER=4.10.35
wget -O oc-mirror.tar.gz https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/$BUILDNUMBER/oc-mirror.tar.gz

tar -xzf oc-mirror.tar.gz -C /usr/local/bin/
chmod +x /usr/local/bin/oc-mirror

SEC_FILE="$XDG_RUNTIME_DIR/containers/auth.json"
# $XDG_RUNTIME_DIR/containers
mkdir -p ${SEC_FILE%/*}

cat > /data/ocp4/mirror.yaml << EOF
apiVersion: mirror.openshift.io/v1alpha1
kind: ImageSetConfiguration
# archiveSize: 4
mirror:
  ocp:
    channels:
      - name: stable-4.10
        versions:
          - '4.10.35'
EOF


mkdir -p /data/install
cd /data/install

oc-mirror --config /data/ocp4/mirror.yaml file:///data/install/

# download from cloud
cd /root/tmp/ocp-mirror.4.10 && \
  rsync -e ssh --info=progress2 -P --delete -arz root@v.wzhlab.top:/data/install/ ./

# sync to cd
cd /root/tmp/ocp-mirror.4.10 && \
  rsync -e ssh --info=progress2 -P --delete -arz  ./ root@****-cd.wzhlab.top:/data/ocp-mirror/

```

# kvm host (失败)

```bash

# 配置kvm环境
dnf -y groupinstall "Server with GUI"

dnf -y install qemu-kvm libvirt libguestfs-tools virt-install virt-viewer virt-manager tigervnc-server

systemctl disable --now firewalld
systemctl enable --now libvirtd

# 准备vnc环境
vncpasswd

cat << EOF > ~/.vnc/config
session=gnome
securitytypes=vncauth,tlsvnc
# desktop=sandbox
geometry=1280x800
alwaysshared
EOF

cat << EOF >> /etc/tigervnc/vncserver.users
:1=root
EOF

# systemctl disable vncserver@:1
systemctl start vncserver@:1
# 如果你想停掉vnc server，这么做
systemctl stop vncserver@:1

/usr/libexec/vncsession-start :1
```

## 修改网络（失败）

试图创建bridge，但是实验室环境由于未知原因，直接把hub/switch下面的所有设备断网了。无奈放弃。

```bash
# 创建实验用虚拟网络

mkdir -p /data/kvm
cd /data/kvm

cat << 'EOF' > /data/kvm/bridge.sh
#!/usr/bin/env bash

PUB_CONN='eno1'
PUB_IP='172.19.3.60/24'
PUB_GW='172.19.3.1'
PUB_DNS='114.114.114.114'

nmcli con down "$PUB_CONN"
nmcli con delete "$PUB_CONN"
nmcli con down baremetal
nmcli con delete baremetal
# RHEL 8.1 appends the word "System" in front of the connection,delete in case it exists
nmcli con down "System $PUB_CONN"
nmcli con delete "System $PUB_CONN"
nmcli connection add ifname baremetal type bridge con-name baremetal ipv4.method 'manual' \
    ipv4.address "$PUB_IP" \
    ipv4.gateway "$PUB_GW" \
    ipv4.dns "$PUB_DNS"
    
nmcli con add type bridge-slave ifname "$PUB_CONN" master baremetal
nmcli con down "$PUB_CONN";pkill dhclient;dhclient baremetal
nmcli con up baremetal
EOF
bash /data/kvm/bridge.sh

nmcli con mod baremetal +ipv4.addresses "192.168.7.1/24"
nmcli con up baremetal

cat << EOF > ~/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF

# restore
cat << 'EOF' > /root/restore.sh
nmcli con down bridge-slave-eno1
nmcli con delete bridge-slave-eno1
nmcli con down baremetal
nmcli con delete baremetal
nmcli con down eno1
nmcli con delete eno1

nmcli con add ifname eno1 type ethernet con-name eno1 ipv4.method 'manual' \
    ipv4.address 172.19.3.60/24 \
    ipv4.gateway 172.19.3.1 \
    ipv4.dns 114.114.114.114
nmcli con up eno1
EOF

```

# ocp install env prepare

## setup helper (dns)

```bash

# switch to you install version

export BUILDNUMBER=4.10.35

pushd /data/ocp4/${BUILDNUMBER}
tar -xzf openshift-client-linux-${BUILDNUMBER}.tar.gz -C /usr/local/bin/
tar -xzf openshift-install-linux-${BUILDNUMBER}.tar.gz -C /usr/local/bin/
tar -xzf oc-mirror.tar.gz -C /usr/local/bin/
chmod +x /usr/local/bin/oc-mirror
install -m 755 /data/ocp4/clients/butane-amd64 /usr/local/bin/butane
install -m 755 /data/ocp4/clients/coreos-installer_amd64 /usr/local/bin/coreos-installer
popd

nmcli con mod eno1 +ipv4.addresses "192.168.7.1/24"
nmcli con up eno1

dnf -y install podman pigz skopeo buildah jq python3-pip git python3 ansible unzip

cd /data/ocp4/ansible-helper

cat > var.yaml << EOF
helper:
  ip_addr: 172.19.3.60
  nic: eno1
pdns:
  bind: '127.0.0.1, 172.19.3.60'
  port: 5301
  recursor_port: 53
  forward: 114.114.114.114
  static:
    - base_domain: infra.wzhlab.top
      record:
        - name: registry
          ip_addr: 172.19.3.60
        - name: nexus
          ip_addr: 172.19.3.60
ntp:
  server: 172.19.3.60
cluster:
  - base_domain: ocp-nep-cd.wzhlab.top
    node:
      - ip_addr: 172.19.3.252
        name: sno-master-01
ptr: 
  - addr: 172.19.3
    domain: ptr.redhat.ren
EOF

cd /data/ocp4/ansible-helper
ansible-playbook -e @var.yaml  helper.yaml

```

## setup image registry

```bash

export VAR_DOMAIN=wzhlab.top
export VAR_SUB_DOMAIN=ocp-nep-cd

# 配置registry
mkdir -p /etc/crts/ && cd /etc/crts

# https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.1/html/installation_guide/installing-codeready-workspaces-in-tls-mode-with-self-signed-certificates_crw
openssl genrsa -out /etc/crts/$VAR_DOMAIN.ca.key 4096
openssl req -x509 \
  -new -nodes \
  -key /etc/crts/$VAR_DOMAIN.ca.key \
  -sha256 \
  -days 36500 \
  -out /etc/crts/$VAR_DOMAIN.ca.crt \
  -subj /CN="Local $VAR_DOMAIN Signer" \
  -reqexts SAN \
  -extensions SAN \
  -config <(cat /etc/pki/tls/openssl.cnf \
      <(printf '[SAN]\nbasicConstraints=critical, CA:TRUE\nkeyUsage=keyCertSign, cRLSign, digitalSignature'))

openssl genrsa -out /etc/crts/$VAR_DOMAIN.key 2048

openssl req -new -sha256 \
    -key /etc/crts/$VAR_DOMAIN.key \
    -subj "/O=Local $VAR_DOMAIN /CN=*.$VAR_SUB_DOMAIN.$VAR_DOMAIN" \
    -reqexts SAN \
    -config <(cat /etc/pki/tls/openssl.cnf \
        <(printf "\n[SAN]\nsubjectAltName=DNS:*.$VAR_SUB_DOMAIN.$VAR_DOMAIN,DNS:*.apps.$VAR_SUB_DOMAIN.$VAR_DOMAIN,DNS:*.infra.$VAR_DOMAIN,DNS:*.$VAR_DOMAIN\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth")) \
    -out /etc/crts/$VAR_DOMAIN.csr

openssl x509 \
    -req \
    -sha256 \
    -extfile <(printf "subjectAltName=DNS:*.$VAR_SUB_DOMAIN.$VAR_DOMAIN,DNS:*.apps.$VAR_SUB_DOMAIN.$VAR_DOMAIN,DNS:*.infra.$VAR_DOMAIN,DNS:*.$VAR_DOMAIN\nbasicConstraints=critical, CA:FALSE\nkeyUsage=digitalSignature, keyEncipherment, keyAgreement, dataEncipherment\nextendedKeyUsage=serverAuth") \
    -days 36500 \
    -in /etc/crts/$VAR_DOMAIN.csr \
    -CA /etc/crts/$VAR_DOMAIN.ca.crt \
    -CAkey /etc/crts/$VAR_DOMAIN.ca.key \
    -CAcreateserial -out /etc/crts/$VAR_DOMAIN.crt

openssl x509 -in /etc/crts/$VAR_DOMAIN.crt -text

/bin/cp -f /etc/crts/$VAR_DOMAIN.ca.crt /etc/pki/ca-trust/source/anchors/
update-ca-trust extract

cd /data/ocp4
# systemctl stop docker-distribution

/bin/rm -rf /data/registry
mkdir -p /data/registry

podman load -i /data/ocp4/registry.tgz

podman run --replace --restart always --name local-registry -p 5443:5443 \
  -d --restart=always \
  -v /data/registry/:/var/lib/registry:z \
  -v /etc/crts:/certs:z \
  -e REGISTRY_HTTP_ADDR=0.0.0.0:5443 \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/$VAR_DOMAIN.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/$VAR_DOMAIN.key \
  docker.io/library/registry:2

cd /data/ocp-mirror/
oc-mirror --dest-skip-tls --from mirror_seq1_000000.tar docker://127.0.0.1:5443

# podman pull registry.infra.wzhlab.top:5443/openshift/release@sha256:8f04fec56a29315a6809952184207fa921f4154398d4d1d12a2d541aadedba80

```

# setup nexus

```bash
podman load -i /data/ocp4/nexus-fs-image.tgz 
podman load -i /data/ocp4/nexus.3.33.1.tgz

## import nexus fs
mkdir -p /data/ccn
cd /data/ccn

podman create --name swap quay.io/wangzheng422/qimgs:nexus-fs-image-2022-01-14-2155 ls
podman cp swap:/nexus-image.tgz - > /data/ccn/nexus-image.tgz.tar
podman rm -fv swap
tar vxf nexus-image.tgz.tar
tar zxf nexus-image.tgz
rm -f nexus-image.tgz*

chown -R 200 /data/ccn/nexus-image

## run the nexus for image
# podman run -d -p 8082:8081 -p 8083:8083 -it --name nexus-image -v /data/ccn/nexus-image:/nexus-data:Z docker.io/sonatype/nexus3:3.38.1
podman run -d -p 8082:8081 -p 8083:8083 -it --name nexus-image -v /data/ccn/nexus-image:/nexus-data:Z docker.io/sonatype/nexus3:3.33.1

podman generate systemd --files --name nexus-image
# /root/container-local-registry.service
/bin/cp -Zf container-nexus-image.service   /etc/systemd/system/

systemctl daemon-reload

systemctl enable --now container-nexus-image.service

```

# install single node openshift 4

## operation on helper node

```bash

useradd -m sno

su - sno

ssh-keygen

cat << EOF >> ~/.bashrc

export BASE_DIR='/home/sno/'

EOF

export BASE_DIR='/home/sno/'

mkdir -p ${BASE_DIR}/data/{sno/disconnected,install}

# set some parameter of you rcluster

NODE_SSH_KEY="$(cat ${BASE_DIR}/.ssh/id_rsa.pub)"
INSTALL_IMAGE_REGISTRY=registry.infra.wzhlab.top:5443

PULL_SECRET='{"auths":{"registry.redhat.io": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"registry.ocp4.redhat.ren:5443": {"auth": "ZHVtbXk6ZHVtbXk=","email": "noemail@localhost"},"'${INSTALL_IMAGE_REGISTRY}'": {"auth": "'$( echo -n 'admin:shadowman' | openssl base64 )'","email": "noemail@localhost"}}}'

NTP_SERVER=172.19.3.60
HELP_SERVER=172.19.3.60

# 定义单节点集群的节点信息
SNO_CLUSTER_NAME=ocp-nep-cd
SNO_BASE_DOMAIN=wzhlab.top
SNO_IP=172.19.3.252
SNO_GW=172.19.3.1
SNO_NETMAST=255.255.255.0
SNO_NETMAST_S=24
SNO_HOSTNAME=sno-master-01
SNO_IF=eno1
SNO_IF_MAC=`printf '00:60:2F:%02X:%02X:%02X' $[RANDOM%256] $[RANDOM%256] $[RANDOM%256]`
SNO_DNS=172.19.3.60
SNO_DISK=/dev/sda
SNO_CORE_PWD=redhat
export VAR_DOMAIN=wzhlab.top

# echo ${SNO_IF_MAC} > /data/sno/sno.mac

mkdir -p ${BASE_DIR}/data/install
cd ${BASE_DIR}/data/install

/bin/rm -rf *.ign .openshift_install_state.json auth bootstrap manifests master*[0-9] worker*[0-9] 

cat << EOF > ${BASE_DIR}/data/install/install-config.yaml 
apiVersion: v1
baseDomain: $SNO_BASE_DOMAIN
compute:
- name: worker
  replicas: 0 
controlPlane:
  name: master
  replicas: 1 
metadata:
  name: $SNO_CLUSTER_NAME
networking:
  # OVNKubernetes , OpenShiftSDN
  networkType: OVNKubernetes
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
bootstrapInPlace:
  installationDisk: $SNO_DISK
pullSecret: '${PULL_SECRET}'
sshKey: |
$( cat ${BASE_DIR}/.ssh/id_rsa.pub | sed 's/^/   /g' )
additionalTrustBundle: |
$( cat /etc/crts/$VAR_DOMAIN.ca.crt | sed 's/^/   /g' )
imageContentSources:
- mirrors:
  - ${INSTALL_IMAGE_REGISTRY}/openshift/release-images
  - ${INSTALL_IMAGE_REGISTRY}/openshift/release
  source: quay.io/openshift-release-dev/ocp-release
- mirrors:
  - ${INSTALL_IMAGE_REGISTRY}/openshift/release
  source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
EOF

/bin/cp -f ${BASE_DIR}/data/install/install-config.yaml ${BASE_DIR}/data/install/install-config.yaml.bak

openshift-install create manifests --dir=${BASE_DIR}/data/install

# /bin/cp -f  /data/ocp4/ocp4-upi-helpernode-master/machineconfig/* ${BASE_DIR}/data/install/openshift/

/bin/cp -f  /data/ocp4/ansible-helper/files/* ${BASE_DIR}/data/install/openshift/

#############################################
# run as root if you have not run below, at least one time
# it will generate registry configuration
# copy image registry proxy related config
cd /data/ocp4
bash image.registries.conf.sh nexus.infra.wzhlab.top:8083

/bin/cp -f /data/ocp4/image.registries.conf /etc/containers/registries.conf.d/
#############################################

/bin/cp -f /data/ocp4/99-worker-container-registries.yaml ${BASE_DIR}/data/install/openshift
/bin/cp -f /data/ocp4/99-master-container-registries.yaml ${BASE_DIR}/data/install/openshift

```

## workload partition

- [Workload partitioning on single-node OpenShift](https://docs.openshift.com/container-platform/4.10/scalability_and_performance/sno-du-enabling-workload-partitioning-on-single-node-openshift.html)
- [Deploying distributed units manually on single-node OpenShift](https://docs.openshift.com/container-platform/4.10/scalability_and_performance/ztp-configuring-single-node-cluster-deployment-during-installation.html)


```bash

lscpu  --all --extended
# CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE MAXMHZ    MINMHZ
# 0   0    0      0    0:0:0:0       yes    3100.0000 1200.0000
# 1   0    0      1    1:1:1:0       yes    3100.0000 1200.0000
# 2   0    0      2    2:2:2:0       yes    3100.0000 1200.0000
# 3   0    0      3    3:3:3:0       yes    3100.0000 1200.0000
# 4   0    0      4    4:4:4:0       yes    3100.0000 1200.0000
# 5   0    0      5    5:5:5:0       yes    3100.0000 1200.0000
# 6   0    0      6    6:6:6:0       yes    3100.0000 1200.0000
# 7   0    0      7    7:7:7:0       yes    3100.0000 1200.0000
# 8   0    0      8    8:8:8:0       yes    3100.0000 1200.0000
# 9   0    0      9    9:9:9:0       yes    3100.0000 1200.0000
# 10  0    0      10   10:10:10:0    yes    3100.0000 1200.0000
# 11  0    0      11   11:11:11:0    yes    3100.0000 1200.0000
# 12  0    0      12   12:12:12:0    yes    3100.0000 1200.0000
# 13  0    0      13   13:13:13:0    yes    3100.0000 1200.0000
# 14  0    0      14   14:14:14:0    yes    3100.0000 1200.0000
# 15  0    0      15   15:15:15:0    yes    3100.0000 1200.0000
# 16  0    0      0    0:0:0:0       yes    3100.0000 1200.0000
# 17  0    0      1    1:1:1:0       yes    3100.0000 1200.0000
# 18  0    0      2    2:2:2:0       yes    3100.0000 1200.0000
# 19  0    0      3    3:3:3:0       yes    3100.0000 1200.0000
# 20  0    0      4    4:4:4:0       yes    3100.0000 1200.0000
# 21  0    0      5    5:5:5:0       yes    3100.0000 1200.0000
# 22  0    0      6    6:6:6:0       yes    3100.0000 1200.0000
# 23  0    0      7    7:7:7:0       yes    3100.0000 1200.0000
# 24  0    0      8    8:8:8:0       yes    3100.0000 1200.0000
# 25  0    0      9    9:9:9:0       yes    3100.0000 1200.0000
# 26  0    0      10   10:10:10:0    yes    3100.0000 1200.0000
# 27  0    0      11   11:11:11:0    yes    3100.0000 1200.0000
# 28  0    0      12   12:12:12:0    yes    3100.0000 1200.0000
# 29  0    0      13   13:13:13:0    yes    3100.0000 1200.0000
# 30  0    0      14   14:14:14:0    yes    3100.0000 1200.0000
# 31  0    0      15   15:15:15:0    yes    3100.0000 1200.0000

cat << EOF > ${BASE_DIR}/data/install/crio.wp.conf
[crio.runtime.workloads.management]
activation_annotation = "target.workload.openshift.io/management"
annotation_prefix = "resources.workload.openshift.io"
resources = { "cpushares" = 0, "cpuset" = "0,15" }
EOF

cat << EOF > ${BASE_DIR}/data/install/ocp.wp.conf
{
  "management": {
    "cpuset": "0,15" 
  }
}
EOF

cat << EOF > ${BASE_DIR}/data/install/openshift/workload-part.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 02-master-workload-partitioning
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,$( base64 -w 0 < ${BASE_DIR}/data/install/crio.wp.conf )
        mode: 420
        overwrite: true
        path: /etc/crio/crio.conf.d/01-workload-partitioning
        user:
          name: root
      - contents:
          source: data:text/plain;charset=utf-8;base64,$( base64 -w 0 < ${BASE_DIR}/data/install/ocp.wp.conf )
        mode: 420
        overwrite: true
        path: /etc/kubernetes/openshift-workload-pinning
        user:
          name: root
EOF


cat > ${BASE_DIR}/data/install/openshift/ocp.mount.yaml << 'EOF' 
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: container-mount-namespace-and-kubelet-conf-master
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,IyEvYmluL2Jhc2gKCmRlYnVnKCkgewogIGVjaG8gJEAgPiYyCn0KCnVzYWdlKCkgewogIGVjaG8gVXNhZ2U6ICQoYmFzZW5hbWUgJDApIFVOSVQgW2VudmZpbGUgW3Zhcm5hbWVdXQogIGVjaG8KICBlY2hvIEV4dHJhY3QgdGhlIGNvbnRlbnRzIG9mIHRoZSBmaXJzdCBFeGVjU3RhcnQgc3RhbnphIGZyb20gdGhlIGdpdmVuIHN5c3RlbWQgdW5pdCBhbmQgcmV0dXJuIGl0IHRvIHN0ZG91dAogIGVjaG8KICBlY2hvICJJZiAnZW52ZmlsZScgaXMgcHJvdmlkZWQsIHB1dCBpdCBpbiB0aGVyZSBpbnN0ZWFkLCBhcyBhbiBlbnZpcm9ubWVudCB2YXJpYWJsZSBuYW1lZCAndmFybmFtZSciCiAgZWNobyAiRGVmYXVsdCAndmFybmFtZScgaXMgRVhFQ1NUQVJUIGlmIG5vdCBzcGVjaWZpZWQiCiAgZXhpdCAxCn0KClVOSVQ9JDEKRU5WRklMRT0kMgpWQVJOQU1FPSQzCmlmIFtbIC16ICRVTklUIHx8ICRVTklUID09ICItLWhlbHAiIHx8ICRVTklUID09ICItaCIgXV07IHRoZW4KICB1c2FnZQpmaQpkZWJ1ZyAiRXh0cmFjdGluZyBFeGVjU3RhcnQgZnJvbSAkVU5JVCIKRklMRT0kKHN5c3RlbWN0bCBjYXQgJFVOSVQgfCBoZWFkIC1uIDEpCkZJTEU9JHtGSUxFI1wjIH0KaWYgW1sgISAtZiAkRklMRSBdXTsgdGhlbgogIGRlYnVnICJGYWlsZWQgdG8gZmluZCByb290IGZpbGUgZm9yIHVuaXQgJFVOSVQgKCRGSUxFKSIKICBleGl0CmZpCmRlYnVnICJTZXJ2aWNlIGRlZmluaXRpb24gaXMgaW4gJEZJTEUiCkVYRUNTVEFSVD0kKHNlZCAtbiAtZSAnL15FeGVjU3RhcnQ9LipcXCQvLC9bXlxcXSQvIHsgcy9eRXhlY1N0YXJ0PS8vOyBwIH0nIC1lICcvXkV4ZWNTdGFydD0uKlteXFxdJC8geyBzL15FeGVjU3RhcnQ9Ly87IHAgfScgJEZJTEUpCgppZiBbWyAkRU5WRklMRSBdXTsgdGhlbgogIFZBUk5BTUU9JHtWQVJOQU1FOi1FWEVDU1RBUlR9CiAgZWNobyAiJHtWQVJOQU1FfT0ke0VYRUNTVEFSVH0iID4gJEVOVkZJTEUKZWxzZQogIGVjaG8gJEVYRUNTVEFSVApmaQo=
        mode: 493
        path: /usr/local/bin/extractExecStart
      - contents:
          source: data:text/plain;charset=utf-8;base64,IyEvYmluL2Jhc2gKbnNlbnRlciAtLW1vdW50PS9ydW4vY29udGFpbmVyLW1vdW50LW5hbWVzcGFjZS9tbnQgIiRAIgo=
        mode: 493
        path: /usr/local/bin/nsenterCmns
    systemd:
      units:
      - contents: |
          [Unit]
          Description=Manages a mount namespace that both kubelet and crio can use to share their container-specific mounts

          [Service]
          Type=oneshot
          RemainAfterExit=yes
          RuntimeDirectory=container-mount-namespace
          Environment=RUNTIME_DIRECTORY=%t/container-mount-namespace
          Environment=BIND_POINT=%t/container-mount-namespace/mnt
          ExecStartPre=bash -c "findmnt ${RUNTIME_DIRECTORY} || mount --make-unbindable --bind ${RUNTIME_DIRECTORY} ${RUNTIME_DIRECTORY}"
          ExecStartPre=touch ${BIND_POINT}
          ExecStart=unshare --mount=${BIND_POINT} --propagation slave mount --make-rshared /
          ExecStop=umount -R ${RUNTIME_DIRECTORY}
        enabled: true
        name: container-mount-namespace.service
      - dropins:
        - contents: |
            [Unit]
            Wants=container-mount-namespace.service
            After=container-mount-namespace.service

            [Service]
            ExecStartPre=/usr/local/bin/extractExecStart %n /%t/%N-execstart.env ORIG_EXECSTART
            EnvironmentFile=-/%t/%N-execstart.env
            ExecStart=
            ExecStart=bash -c "nsenter --mount=%t/container-mount-namespace/mnt \
                ${ORIG_EXECSTART}"
          name: 90-container-mount-namespace.conf
        name: crio.service
      - dropins:
        - contents: |
            [Unit]
            Wants=container-mount-namespace.service
            After=container-mount-namespace.service

            [Service]
            ExecStartPre=/usr/local/bin/extractExecStart %n /%t/%N-execstart.env ORIG_EXECSTART
            EnvironmentFile=-/%t/%N-execstart.env
            ExecStart=
            ExecStart=bash -c "nsenter --mount=%t/container-mount-namespace/mnt \
                ${ORIG_EXECSTART} --housekeeping-interval=30s"
          name: 90-container-mount-namespace.conf
        - contents: |
            [Service]
            Environment="OPENSHIFT_MAX_HOUSEKEEPING_INTERVAL_DURATION=60s"
            Environment="OPENSHIFT_EVICTION_MONITORING_PERIOD_DURATION=30s"
          name: 30-kubelet-interval-tuning.conf
        name: kubelet.service
EOF


```

## sctp

```bash

cat << EOF > ${BASE_DIR}/data/install/openshift/sctp-module.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 99-master-rt-load-sctp-module
  labels:
    machineconfiguration.openshift.io/role: master
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/modprobe.d/sctp-blacklist.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,
        - path: /etc/modules-load.d/sctp-load.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,sctp
EOF

```

## generate ignition and iso

```bash
cd ${BASE_DIR}/data/install/

openshift-install --dir=${BASE_DIR}/data/install create single-node-ignition-config

# alias coreos-installer='podman run --privileged --rm \
#         -v /dev:/dev -v /run/udev:/run/udev -v $PWD:/data \
#         -w /data quay.io/coreos/coreos-installer:release'

# /bin/cp -f bootstrap-in-place-for-live-iso.ign iso.ign

cat << EOF > ${BASE_DIR}/data/sno/static.hostname.bu
variant: openshift
version: 4.9.0
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-zzz-master-static-hostname
storage:
  files:
    - path: /etc/hostname
      mode: 0644
      overwrite: true
      contents:
        inline: |
          ${SNO_HOSTNAME}
EOF


cat << EOF > ${BASE_DIR}/data/sno/static.ip.bu
variant: openshift
version: 4.9.0
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-zzz-master-static-ip
storage:
  files:
    - path: /etc/NetworkManager/system-connections/${SNO_IF}.nmconnection
      mode: 0600
      overwrite: true
      contents:
        inline: |
          [connection]
          id=${SNO_IF}
          type=ethernet
          autoconnect-retries=1
          interface-name=${SNO_IF}
          multi-connect=1
          permissions=
          wait-device-timeout=60000

          [ethernet]
          mac-address-blacklist=

          [ipv4]
          address1=${SNO_IP}/${SNO_NETMAST_S=24},${SNO_GW}
          dhcp-hostname=${SNO_HOSTNAME}
          dhcp-timeout=90
          dns=${SNO_DNS};
          dns-search=
          may-fail=false
          method=manual

          [ipv6]
          addr-gen-mode=eui64
          dhcp-hostname=${SNO_HOSTNAME}
          dhcp-timeout=90
          dns-search=
          method=disabled

          [proxy]

EOF

source /data/ocp4/acm.fn.sh

# butane /data/sno/static.bootstrap.ip.bu > /data/sno/disconnected/99-zzz-bootstrap-ip.yaml
# get_file_content_for_ignition "/opt/openshift/openshift/99-zzz-bootstrap-ip.yaml" "/data/sno/disconnected/99-zzz-bootstrap-ip.yaml"
# VAR_99_master_bootstrap_ip=$RET_VAL
# VAR_99_master_bootstrap_ip_2=$RET_VAL_2

# butane -r /data/sno/static.hostname.bu > /data/sno/disconnected/99-zzz-master-static-hostname.ign
butane ${BASE_DIR}/data/sno/static.hostname.bu > ${BASE_DIR}/data/sno/disconnected/99-zzz-master-static-hostname.yaml
get_file_content_for_ignition "/opt/openshift/openshift/99-zzz-master-static-hostname.yaml" "${BASE_DIR}/data/sno/disconnected/99-zzz-master-static-hostname.yaml"
VAR_99_master_master_static_hostname=$RET_VAL
VAR_99_master_master_static_hostname_2=$RET_VAL_2

butane ${BASE_DIR}/data/sno/static.ip.bu > ${BASE_DIR}/data/sno/disconnected/99-zzz-master-ip.yaml
get_file_content_for_ignition "/opt/openshift/openshift/99-zzz-master-ip.yaml" "${BASE_DIR}/data/sno/disconnected/99-zzz-master-ip.yaml"
VAR_99_master_ip=$RET_VAL
VAR_99_master_ip_2=$RET_VAL_2

# 我们会创建一个wzh用户，密码是redhat，这个可以在第一次启动的是，从console/ssh直接用用户名口令登录
# 方便排错和研究
VAR_PWD_HASH="$(python3 -c 'import crypt,getpass; print(crypt.crypt("redhat"))')"

# tmppath=$(mktemp)
cat ${BASE_DIR}/data/install/bootstrap-in-place-for-live-iso.ign \
  | jq --arg VAR "$VAR_PWD_HASH" --arg VAR_SSH "$NODE_SSH_KEY" '.passwd.users += [{ "name": "wzh", "system": true, "passwordHash": $VAR , "sshAuthorizedKeys": [ $VAR_SSH ], "groups": [ "adm", "wheel", "sudo", "systemd-journal"  ] }]' \
  | jq --argjson VAR "$VAR_99_master_master_static_hostname" '.storage.files += [$VAR] ' \
  | jq --argjson VAR "$VAR_99_master_ip_2" '.storage.files += [$VAR] ' \
  | jq --argjson VAR "$VAR_99_master_ip" '.storage.files += [$VAR] ' \
  | jq -c . \
  > ${BASE_DIR}/data/install/iso.ign

# butane -r /data/sno/static.user.bu > /data/sno/static.user.ign

# jump to other document here, if you want to customize the ignition file for partition and user
# then comeback

cd ${BASE_DIR}/data/install/
/bin/cp -f /data/ocp4/rhcos-live.x86_64.iso sno.iso

# coreos-installer iso ignition embed -fi iso.ign sno.iso

coreos-installer iso reset sno.iso

coreos-installer iso ignition embed sno.iso \
  --ignition-file ${BASE_DIR}/data/install/iso.ign 

# coreos-installer iso customize -f sno.iso \
#   --network-keyfile ${BASE_DIR}/data/sno/${SNO_IF}.nmconnection 

#   --dest-ignition /data/install/iso.ign \
#   --dest-device $SNO_DISK \
  # --live-ignition /data/sno/static.user.ign \
  # --live-ignition /data/sno/disconnected/99-zzz-master-static-hostname.ign \
  # --live-ignition /data/install/bootstrap-in-place-for-live-iso.ign 

# coreos-installer iso ignition embed sno.iso \
#   --config /data/sno/disconnected/99-zzz-master-static-hostname.ign

# coreos-installer iso ignition show sno.iso | jq .

```

## boot through bmc

```bash

dnf install -y libX11 libnsl

mkdir -p /root/java/

cat << 'EOF' > /usr/local/bin/j8.sh
#!/usr/bin/env bash

# /bin/rm -rf ~/.java
~/java/jdk1.8.0_341/bin/javaws $*
EOF
chmod +x /usr/local/bin/j8.sh


```

## on helper to see result

```bash
cd ${BASE_DIR}/data/install
export KUBECONFIG=${BASE_DIR}/data/install/auth/kubeconfig
echo "export KUBECONFIG=${BASE_DIR}/data/install/auth/kubeconfig" >> ~/.bashrc
# oc completion bash | sudo tee /etc/bash_completion.d/openshift > /dev/null

cd ${BASE_DIR}/data/install
openshift-install wait-for install-complete --log-level debug
# INFO Install complete!
# INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/sno/data/install/auth/kubeconfig'
# INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp-nep-cd.wzhlab.top
# INFO Login to the console with user: "kubeadmin", and password: "EhENk-HumhP-uYhaI-MbYjv"
# DEBUG Time elapsed per stage:
# DEBUG Cluster Operators: 14m38s
# INFO Time elapsed: 14m38s

```

# password login and oc config

```bash

# init setting for helper node
cat << EOF > ~/.ssh/config
StrictHostKeyChecking no
UserKnownHostsFile=/dev/null
EOF
chmod 600 ~/.ssh/config

# ssh core@*****

# sudo -i

# # change password for root
# echo 'redhat' | passwd --stdin root

# sed -i "s|^PasswordAuthentication no$|PasswordAuthentication yes|g" /etc/ssh/sshd_config
# sed -i "s|^PermitRootLogin no$|PermitRootLogin yes|g" /etc/ssh/sshd_config
# sed -i "s|^#ClientAliveInterval 180$|ClientAliveInterval 1800|g" /etc/ssh/sshd_config

# systemctl restart sshd

# # set env, so oc can be used
# cat << EOF >> ~/.bashrc

# export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig

# RET=`oc config use-context system:admin`

# EOF

cat > ${BASE_DIR}/data/install/crack.txt << EOF

echo redhat | sudo passwd --stdin root

sudo sed -i "s|^PasswordAuthentication no$|PasswordAuthentication yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^PermitRootLogin no$|PermitRootLogin yes|g" /etc/ssh/sshd_config
sudo sed -i "s|^#ClientAliveInterval 180$|ClientAliveInterval 1800|g" /etc/ssh/sshd_config

sudo systemctl restart sshd

sudo sh -c 'echo "export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost.kubeconfig" >> /root/.bashrc'

sudo sh -c 'echo "RET=\\\`oc config use-context system:admin\\\`" >> /root/.bashrc'

EOF

for i in 252
do
  ssh core@172.19.3.$i < ${BASE_DIR}/data/install/crack.txt
done

```

## from other host

```bash
# https://unix.stackexchange.com/questions/230084/send-the-password-through-stdin-in-ssh-copy-id
dnf install -y sshpass

for i in 252
do
  sshpass -p 'redhat' ssh-copy-id root@172.19.3.$i
done

```

## connect direct to the sno node

```bash

ssh root@172.19.3.252

```

# setup for vdu

## import image

```bash

podman image tag 8dd921f5f325 registry.infra.wzhlab.top:5443/nepdemo/flexran_vdu:flexran-20.11-dpdk-19.11-ocp4.9.5-ubi-8.4-core-conf

podman push registry.infra.wzhlab.top:5443/nepdemo/flexran_vdu:flexran-20.11-dpdk-19.11-ocp4.9.5-ubi-8.4-core-conf

```

## rt-kernel

```bash


oc patch mcp/master --patch '{"spec":{"paused":true}}' --type=merge
oc patch mcp/worker --patch '{"spec":{"paused":true}}' --type=merge



oc patch mcp/master --patch '{"spec":{"paused":false}}' --type=merge
oc patch mcp/worker --patch '{"spec":{"paused":false}}' --type=merge



cat << EOF > ${BASE_DIR}/data/install/pao-namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-performance-addon-operator
  annotations:
    workload.openshift.io/allowed: management
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: openshift-performance-addon-operator
  namespace: openshift-performance-addon-operator
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: openshift-performance-addon-operator-subscription
  namespace: openshift-performance-addon-operator
spec:
  channel: "4.10" 
  installPlanApproval: Manual
  name: performance-addon-operator
  source: redhat-operators 
  sourceNamespace: openshift-marketplace
EOF
oc create --save-config -f ${BASE_DIR}/data/install/pao-namespace.yaml


oc get installplan -A
# NAMESPACE                              NAME            CSV                                  APPROVAL   APPROVED
# openshift-performance-addon-operator   install-mx5kb   performance-addon-operator.v4.9.10   Manual     false

VAR_INSTALL_PLAN=`oc get installplan -A -o json | jq -r ' first( .items[].metadata | select( .namespace == "openshift-performance-addon-operator" ) | .name )  '`

oc patch installplan ${VAR_INSTALL_PLAN} \
    --namespace openshift-performance-addon-operator \
    --type merge \
    --patch '{"spec":{"approved":true}}'

oc get csv -n openshift-performance-addon-operator
# NAME                                 DISPLAY                      VERSION   REPLACES   PHASE
# performance-addon-operator.v4.9.10   Performance Addon Operator   4.9.10               Succeeded


cat << EOF > ${BASE_DIR}/data/install/performance-2.yaml
---
apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
   name: wzh-performanceprofile-2
spec:
  additionalKernelArgs:
    - nmi_watchdog=0
    - isolcpus=1-14
    - nohz_full=1-14
    - rcu_nocbs=1-14
    - kthread_cpus=0,15
    - irqaffinity=0,15
    - iommu=pt
    - intel_iommu=on
    - intel_pstate=disable
    # try to upgrade e810 driver
    - module_name.blacklist=1 
    - rd.driver.blacklist=ice
    # profile creator
    - audit=0
    - mce=off
    - nmi_watchdog=0
  globallyDisableIrqLoadBalancing: true
  cpu:
      isolated: "1-14"
      reserved: "0,15"
  hugepages:
    defaultHugepagesSize: "1G"
    pages:
    - size:  "1G"
      count:  24
  realTimeKernel:
      enabled: true
  numa:  
      topologyPolicy: "single-numa-node"
  nodeSelector:
      node-role.kubernetes.io/master: ""
  machineConfigPoolSelector:
    pools.operator.machineconfiguration.openshift.io/master: ""
EOF
oc create  --save-config  -f ${BASE_DIR}/data/install/performance-2.yaml

```

## e810 driver 

### build image

```bash
mkdir -p /data/ostree

export BUILDNUMBER=4.10.35

export http_proxy="http://192.168.195.89:5085"
export https_proxy=${http_proxy}

wget -O openshift-client-linux-${BUILDNUMBER}.tar.gz https://mirror.openshift.com/pub/openshift-v4/clients/ocp/${BUILDNUMBER}/openshift-client-linux-${BUILDNUMBER}.tar.gz
wget -O openshift-install-linux-${BUILDNUMBER}.tar.gz https://mirror.openshift.com/pub/openshift-v4/clients/ocp/${BUILDNUMBER}/openshift-install-linux-${BUILDNUMBER}.tar.gz

tar -xzf openshift-client-linux-${BUILDNUMBER}.tar.gz -C /usr/local/sbin/
tar -xzf openshift-install-linux-${BUILDNUMBER}.tar.gz -C /usr/local/sbin/

oc image extract --path /:/data/ostree --registry-config /data/pull-secret.json   `  curl -s https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/$BUILDNUMBER/release.txt | grep machine-os-content | awk '{print $2}'  `

mv /data/ostree/extensions /data/
rm -rf /data/ostree


mkdir -p /etc/yum.repos.d
cat > /etc/yum.repos.d/rt.repo << 'EOF'
[rt]
name=rt
baseurl=file:///data/extensions
gpgcheck=0
EOF

dnf install -y kernel-rt-core kernel-rt-devel kernel-rt-modules kernel-rt-modules-extra

reboot

mkdir -p /data/dockerfile
cp -r /data/extensions /data/dockerfile/

cd /data/dockerfile

wget https://versaweb.dl.sourceforge.net/project/e1000/ice%20stable/1.9.11/ice-1.9.11.tar.gz


cat > /data/dockerfile/rt.repo << 'EOF'
[rt]
name=rt
baseurl=file:///extensions
gpgcheck=0
EOF

cat << EOF > /data/dockerfile/nepdemo.redhat.ubi8.dockerfile
FROM registry.access.redhat.com/ubi8 AS BUILD

COPY extensions /extensions
COPY rt.repo  /etc/yum.repos.d/rt.repo
COPY ice-1.9.11.tar.gz  /diy/

RUN  dnf -y update 
RUN  dnf -y install make gcc elfutils-libelf-devel binutils kmod procps git autoconf automake kernel-rt-devel

RUN rm -f /etc/yum.repos.d/rt.repo && rm -rf /extensions

RUN cd /diy && tar zvxf ice-1.9.11.tar.gz && cd /diy/ice-1.9.11/src && make && mv /diy/ice-1.9.11/src/*.ko /diy

FROM registry.access.redhat.com/ubi8

RUN dnf -y update 
RUN dnf install -y kmod
RUN rm -rf /var/cache/*

COPY --from=BUILD /diy/*.ko /diy/

EOF

podman build --squash -t quay.io/nepdemo/intel-driver:8.6-rt-1.9.11 -f /data/dockerfile/nepdemo.redhat.ubi8.dockerfile /data/dockerfile/

# test out
podman run --rm -it quay.io/nepdemo/intel-driver:8.6-rt-1.9.11 /bin/bash

# push back to quay.io
podman push quay.io/nepdemo/intel-driver:8.6-rt-1.9.11

```

### install

```bash

cat << EOF > ${BASE_DIR}/data/sno/static-pod.bu
variant: openshift
version: 4.9.0
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 99-zzz-e810-dpdk-driver-static-master
storage:
  files:
    - path: /etc/modprobe.d/blacklist-ice.conf
      mode: 0644
      overwrite: true
      contents:
        inline: |
          blacklist ice
systemd:
  units:
    - name: driver.ice.service
      enabled: true
      contents: |
        [Unit]
        Description=driver.ice service
        Wants=network-online.target
        After=network-online.target

        [Service]
        Type=oneshot
        RemainAfterExit=yes
        User=root
        WorkingDirectory=/root/
        ExecStart=podman run --replace --privileged --rm --name ice-driver -it quay.io/nepdemo/intel-driver:8.6-rt-1.9.11 /bin/sh -c " rmmod ice; rmmod auxiliary ; insmod /diy/auxiliary.ko; insmod /diy/ice.ko ; "

        [Install]
        WantedBy=multi-user.target
    - name: kubelet.service
      dropins:
      - name: 99-after-ice.conf
        contents: |
          [Unit]
          Requires=driver.ice.service
          After=driver.ice.service

EOF

butane -d ${BASE_DIR}/data/install ${BASE_DIR}/data/sno/static-pod.bu > ${BASE_DIR}/data/install/99-zzz-e810-dpdk-driver-static-master.yaml

oc create --save-config -f ${BASE_DIR}/data/install/99-zzz-e810-dpdk-driver-static-master.yaml

# oc apply -f ${BASE_DIR}/data/install/99-zzz-e810-dpdk-driver-static-master.yaml
# oc delete -f /data/install/99-zzz-e810-dpdk-driver-static-worker-rt-2.yaml

```


## linuxptp 3.11

```bash

oc new-project vbbu-demo

oc project vbbu-demo

# export REG_TMP='tmp-registry.ocp4.redhat.ren:5443'

# kernel driver deployment
oc create serviceaccount svcacct-driver -n vbbu-demo
oc adm policy add-scc-to-user privileged -z svcacct-driver -n vbbu-demo
# oc adm policy add-scc-to-user anyuid -z mysvcacct -n vbbu-demo

# !!! remember to disable chronyd on dest host !!!
# we do not use ptp opeerator, so we need to do it manually
# TODO
# https://docs.openshift.com/container-platform/4.10/scalability_and_performance/ztp-configuring-single-node-cluster-deployment-during-installation.html#sno-du-disabling-ntp_sno-du-deploying-distributed-units-manually-on-single-node-openshift

cat << 'EOF' > ${BASE_DIR}/data/install/ptp.chrony.conf
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: disable-chronyd
spec:
  config:
    systemd:
      units:
        - contents: |
            [Unit]
            Description=NTP client/server
            Documentation=man:chronyd(8) man:chrony.conf(5)
            After=ntpdate.service sntp.service ntpd.service
            Conflicts=ntpd.service systemd-timesyncd.service
            ConditionCapability=CAP_SYS_TIME
            [Service]
            Type=forking
            PIDFile=/run/chrony/chronyd.pid
            EnvironmentFile=-/etc/sysconfig/chronyd
            ExecStart=/usr/sbin/chronyd $OPTIONS
            ExecStartPost=/usr/libexec/chrony-helper update-daemon
            PrivateTmp=yes
            ProtectHome=yes
            ProtectSystem=full
            [Install]
            WantedBy=multi-user.target
          enabled: false
          name: chronyd.service
    ignition:
      version: 2.2.0
EOF
oc create -f ${BASE_DIR}/data/install/ptp.chrony.conf

cat << 'EOF' > ${BASE_DIR}/data/install/ptp.ns.conf
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-ptp
  annotations:
    workload.openshift.io/allowed: management
  labels:
    name: openshift-ptp
    openshift.io/cluster-monitoring: "true"
EOF
oc create --save-config -f ${BASE_DIR}/data/install/ptp.ns.conf

# https://docs.openshift.com/container-platform/4.10/networking/using-ptp.html
# it turn out the ptp version using in ptp operator is 2.0.
# we have to use DIY

cat << EOF > ${BASE_DIR}/data/install/ptp4l.conf
[global]
#
# Default Data Set
#
twoStepFlag              1
slaveOnly                0
priority1                128
priority2                128
domainNumber             24
clockClass               248
clockAccuracy            0xFE
offsetScaledLogVariance  0xFFFF
free_running             0
freq_est_interval        0
#
# Port Data Set
# 16 TS a second use logSyncInterval        -4
#
#logAnnounceInterval      4
logAnnounceInterval      1
logSyncInterval          -4
logMinDelayReqInterval   0
logMinPdelayReqInterval  0
announceReceiptTimeout   3
syncReceiptTimeout       0
delayAsymmetry           0
fault_reset_interval     4
neighborPropDelayThresh  20000000
#
# Run time options
#
assume_two_step          0
logging_level            6
path_trace_enabled       0
follow_up_info           0
tx_timestamp_timeout     200
use_syslog               1
verbose                  0
summary_interval         0
kernel_leap              1
check_fup_sync           0
#
# Servo Options
#
pi_proportional_const    0.0
pi_integral_const        0.0
pi_proportional_scale    0.0
pi_proportional_exponent -0.3
pi_proportional_norm_max 0.7
pi_integral_scale        0.0
pi_integral_exponent     0.4
pi_integral_norm_max     0.3
step_threshold           0.00000002
first_step_threshold     0.00002
max_frequency            900000000
clock_servo              nullf
sanity_freq_limit        200000000
ntpshm_segment           0
#
# Transport options
#
transportSpecific        0x0
ptp_dst_mac              01:1B:19:00:00:00
p2p_dst_mac              01:80:C2:00:00:0E
udp6_scope               0x0E
uds_address              /var/run/ptp4l
#
# Default interface options
#
network_transport        UDPv4
#network_transport        L2
delay_mechanism          E2E
time_stamping            hardware
delay_filter             moving_median
delay_filter_length      10
egressLatency            0
ingressLatency           0
boundary_clock_jbod      0
#
# Clock description
#
productDescription       ;;
revisionData             ;;
manufacturerIdentity     00:00:00
userDescription          ;
timeSource               0xA0
EOF

cat << EOF > ${BASE_DIR}/data/install/ts2phc.cfg
[global]
use_syslog              0
verbose                 1
logging_level           7
ts2phc.pulsewidth       100000000
# For GNSS module
ts2phc.nmea_serialport /dev/ttyGNSS_6500_0
[ens2f0]
ts2phc.extts_polarity rising
EOF

oc delete configmap ptp-config -n vbbu-demo

oc create configmap ptp-config --save-config -n vbbu-demo --from-file=${BASE_DIR}/data/install/ptp4l.conf --from-file=${BASE_DIR}/data/install/ts2phc.cfg --save-config=true

# 06 for fifo
# 07 for nice
export VAR_IMAGE='quay.io/nepdemo/linuxptp:3.1.1-ubi-8.4-v06'

cat << EOF > ${BASE_DIR}/data/install/ptp.demo.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nepdemo-linuxptp-daemon
  labels:
    app: nepdemo-linuxptp-daemon
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nepdemo-linuxptp-daemon
  template:
    metadata:
      annotations:
      labels:
        app: nepdemo-linuxptp-daemon
      name: nepdemo-linuxptp-daemon
      # namespace: openshift-ptp
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchFields:
              - key: metadata.name
                operator: In
                values:
                - sno-master-01
      tolerations:
      - key: "vbbu"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: ptp4l
        image: $VAR_IMAGE
        command: ["/bin/sh", "-c", "--"]
        args: [" /usr/local/sbin/ptp4l.sh ;"]
        env:
        - name: DEMO_ENV_PTP4L_ARG
          value: " -i ens2f0 -2 "
        - name: DEMO_ENV_PRIO
          value: "65"
        securityContext:
          privileged: true
          runAsUser: 0 
        volumeMounts:
        - mountPath: /etc/ptp4l.conf
          subPath: ptp4l.conf
          name: config-volume
        - mountPath: /var/run
          name: socket-dir
      - name: phc2sys
        image: $VAR_IMAGE
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "-c", "--"]
        args: [" /usr/local/sbin/phc2sys.sh ;"]
        env:
        - name: DEMO_ENV_PHC2SYS_ARG
          # value: " -s ens2f0 -O 0 -R 8 "  
          value: " -s ens2f0 -r -u 1 -O 0 -R 8 "
        - name: DEMO_ENV_PRIO
          value: "65"
        securityContext:
          privileged: true
          runAsUser: 0     
        volumeMounts:
        - mountPath: /etc/ptp4l.conf
          subPath: ptp4l.conf
          name: config-volume
        - mountPath: /var/run
          name: socket-dir
      - name: ts2phc
        image: $VAR_IMAGE
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "-c", "--"]
        args: [" /usr/local/sbin/ts2phc.sh ;"]
        env:
        - name: DEMO_ENV_TS2PHC_ARG
          value: " -s generic -c ens2f0 "
        - name: DEMO_ENV_PRIO
          value: "65"
        securityContext:
          privileged: true
          runAsUser: 0      
        volumeMounts:
        - mountPath: /etc/ts2phc.cfg
          subPath: ts2phc.cfg
          name: config-volume
        - mountPath: /var/run
          name: socket-dir
        - name: dev
          mountPath: /dev
      hostNetwork: true
      # hostPID: true
      serviceAccountName: svcacct-driver
      volumes:
      - configMap:
          defaultMode: 420
          name: ptp-config
        name: config-volume
      - name: socket-dir
        emptyDir: {}
      - name: dev
        hostPath:
          path: "/dev"
EOF

oc create --save-config -n vbbu-demo -f ${BASE_DIR}/data/install/ptp.demo.yaml

# oc delete -n vbbu-demo -f ${BASE_DIR}/data/install/ptp.demo.yaml

```
