# 在 RHEL9 上训练 LLM 并实现个人知识库 （RAG）

随着chatgpt出圈，大预言模型火遍全球。那么在红帽的平台上，如何训练/微调一个大预言模型，如何使用大预言模型运行一个应用？

现在，我们就在红帽的RHEL操作系统上，来一步一步的从零开始，下载一个大预言模型，部署一个应用，并且微调这个大预言模型。在这个过程中，体验大语言模型的魅力和给我们带来的挑战。

由于大语言模型的火爆，相关的开源项目欣欣向荣，我们主要会使用2个开源项目，来逐步完成我们的实验。

这里是实验架构图：

![](imgs/2023-11-27-22-59-47.png)

# 驱动安装

我们的实验环境是一台rhel9主机，开始实验的第一件事情，就是按照nvidia驱动

```bash

grubby --update-kernel=ALL --args="amd_iommu=on iommu=on modprobe.blacklist=nouveau"

grub2-mkconfig -o /etc/grub2.cfg

dnf update -y

reboot

dnf groupinstall -y 'development'

dnf groupinstall -y 'server with gui'


# or using rpm network
# https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Rocky&target_version=9&target_type=rpm_network
dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo
dnf clean all
dnf -y module install nvidia-driver:latest-dkms
dnf -y install cuda datacenter-gpu-manager
# dnf install -y cuda-11

# systemctl enable --now dcgm

# NCCL
# https://developer.nvidia.com/nccl
dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo

dnf install -y libnccl libnccl-devel libnccl-static

curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash
yum install -y git-lfs

dnf install -y conda


```

# 下载模型

大语言模型的数据文件非常大，我们需要提前下载，不然应用运行的时候，会自动下载，为了方便管理，我们先手动下载下来

```bash

# config python to point to python 3.11
# alternatives --config python
# alternatives --config python3

# rm -rf /data/py_env/hg_cli/venv
# mkdir -p /data/py_env/hg_cli/

# cd /data/py_env/hg_cli
# python3 -m venv venv

# # very important, run every time when using python
# source /data/py_env/hg_cli/venv/bin/activate

mkdir -p /data/env/
conda create -y -p /data/env/hg_cli python=3.10

conda init bash

conda activate /data/env/hg_cli
# conda deactivate

# python -m pip install --upgrade pip setuptools wheel

pip install --upgrade huggingface_hub

# on helper
# for chatglm2-6b
VAR_NAME=THUDM/ChatGLM2-6B

VAR_NAME_FULL=${VAR_NAME//\//-}
echo $VAR_NAME_FULL
# THUDM-ChatGLM2-6B

mkdir -p /data01/huggingface/${VAR_NAME_FULL}
cd /data01/huggingface/${VAR_NAME_FULL}

while true; do
    huggingface-cli download --repo-type model --revision main --cache-dir /data01/huggingface/cache --local-dir ./ --local-dir-use-symlinks False --resume-download ${VAR_NAME} 
    if [ $? -eq 0 ]; then
        break
    fi
    sleep 1  # Optional: waits for 1 second before trying again
done

################
# for m3e-large

VAR_NAME=moka-ai/m3e-large

VAR_NAME_FULL=${VAR_NAME//\//-}
echo $VAR_NAME_FULL
# moka-ai-m3e-large

mkdir -p /data01/huggingface/${VAR_NAME_FULL}
cd /data01/huggingface/${VAR_NAME_FULL}

while true; do
    huggingface-cli download --repo-type model --revision main --cache-dir /data01/huggingface/cache --local-dir ./ --local-dir-use-symlinks False --resume-download ${VAR_NAME} 
    if [ $? -eq 0 ]; then
        break
    fi
    sleep 1  # Optional: waits for 1 second before trying again
done

################
# for Llama-2-7b-chat-hf

VAR_NAME=meta-llama/Llama-2-7b-chat-hf

VAR_NAME_FULL=${VAR_NAME//\//-}
echo $VAR_NAME_FULL
# meta-llama-Llama-2-7b-chat-hf

mkdir -p /data01/huggingface/${VAR_NAME_FULL}
cd /data01/huggingface/${VAR_NAME_FULL}

while true; do
    huggingface-cli download --token ${HUGGINGFACE_TOKEN} --repo-type model --revision main --cache-dir /data01/huggingface/cache --local-dir ./ --local-dir-use-symlinks False --resume-download ${VAR_NAME} 
    if [ $? -eq 0 ]; then
        break
    fi
    sleep 1  # Optional: waits for 1 second before trying again
done

```

# 运行LLM应用

基于LLM的应用，现在看有3个大的方向，一个是chatgpt这样的智能问答，一个是构建个人知识库（RAG），最后一个是AI Agent (function call)。

由于我们使用的是离线的LLM，泛化能力不够，所以AI Agent相关的功能并不能很好的支撑，那么我们就集中精力，在前个场景。

我们选择了一个开源项目，[Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat)，类似的开源项目有很多，我们选择这个项目，是因为这个项目文档很丰富，源代码结构简单，支持的应用场景很完整。

```bash

mkdir -p /data/env/
/bin/rm -rf /data/env/chatchat
conda create -y -p /data/env/chatchat python=3.10

conda init bash

conda activate /data/env/chatchat
# conda deactivate

pip3 install --upgrade pip
pip install peft


mkdir -p /data/git_env
cd /data/git_env

git clone https://github.com/chatchat-space/Langchain-Chatchat

# git checkout pre-release
git restore ./
git checkout v0.2.6

export ENV_CWD="/data/git_env/Langchain-Chatchat"

cd ${ENV_CWD}
pip install -U -r requirements.txt


cd ${ENV_CWD}/configs
/bin/cp -f model_config.py.example model_config.py
/bin/cp -f server_config.py.example server_config.py
/bin/cp -f kb_config.py.example kb_config.py
# /bin/cp -f kb_config.py.exmaple kb_config.py
/bin/cp -f basic_config.py.example basic_config.py
/bin/cp -f prompt_config.py.example prompt_config.py

# apply custom config
cd ${ENV_CWD}/configs
/bin/cp -f model_config.multi.py model_config.py
# /bin/cp -f server_config.multi.py server_config.py

# init vector db
cd ${ENV_CWD}
# /bin/rm -rf ${ENV_CWD}/info.db
# /bin/rm -rf ${ENV_CWD}/samples/vector_store
/bin/rm -rf ${ENV_CWD}/knowledge_base/*/vector_store
python3 init_database.py --recreate-vs

# apply custom config
cd ${ENV_CWD}/configs
/bin/cp -f model_config.multi.py model_config.py
/bin/cp -f prompt_config.multi.py prompt_config.py
# /bin/cp -f server_config.multi.py server_config.py

# startup the UI
# no proxy to run ...
unset http_proxy
unset https_proxy
unset no_proxy

cd ${ENV_CWD}
python startup.py -a

# 服务端运行信息：
#     OpenAI API Server: http://0.0.0.0:20000/v1
#     Chatchat  API  Server: http://0.0.0.0:7861
#     Chatchat WEBUI Server: http://0.0.0.0:8501
#     Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.

# http://172.21.6.98:8501

```

# 微调LLM

微调LLM，涉及到要理解模型，理解微调算法，理解算法参数，并不容易，但是现在有了开源项目，比如llama-factory，帮助我们跨越了很多障碍，能够快速的喂给LLM我们需要的增量数据，实现微调LLM。

我们现在就来试试吧。

```bash

cd /data/git_env

# v0.2.0 right now
git clone --single-branch -b main https://github.com/hiyouga/LLaMA-Factory

rm -rf /data/env/llama_factory
conda create --yes -p /data/env/llama_factory python=3.10

conda activate /data/env/llama_factory
# conda deactivate


cd /data/git_env/LLaMA-Factory
pip install -r requirements.txt

pip install -U flash-attn transformers_stream_generator

unset http_proxy
unset https_proxy
unset no_proxy
unset PROXY

cd /data/git_env/LLaMA-Factory
CUDA_VISIBLE_DEVICES=0 python src/train_web.py


```

(from chatgpt) 以下是LLM（Language Learning Model）在不同Fine-tuning（微调）阶段的比较：

| 阶段                               | 主要目标                   | 数据需求                    | 训练复杂性 | 应用场景                       |
| ---------------------------------- | -------------------------- | --------------------------- | ---------- | ------------------------------ |
| Pre-training                       | 学习语言的基础结构和语义   | 无标签的大量文本数据        | 中等       | 任何需要基础语言理解的场景     |
| Supervised Fine-tune               | 优化模型以完成特定任务     | 标注的数据集，针对特定任务  | 中到高     | 特定任务，如文本分类、NER等    |
| Reward Modeling                    | 通过外部奖励来改进模型行为 | 一些带有奖励信号的示例      | 高         | 强化学习场景，交互式任务       |
| PPO (Proximal Policy Optimization) | 优化策略以获取更多奖励     | 环境反馈                    | 高         | 高度交互或动态环境             |
| DPO (Data-Parallel Optimization)   | 利用并行化提高训练效率     | 与PPO类似，但可以分布式处理 | 非常高     | 大规模并行训练，资源丰富的场景 |

## 试用

我们试用openai api的模式，加载微调的lora模型，这种方法比较通用，因为有一些微调的算法，比如p-tuning，是无法合并到基座模型的

```bash

########################
# a new terminal
# or start llm api directly
conda activate /data/env/llama_factory
# conda deactivate

cd /data/git_env/LLaMA-Factory
python src/api_demo.py \
    --model_name_or_path  /data/huggingface/chatglm2-6b \
    --template chatglm2 \
    --finetuning_type lora \
    --checkpoint_dir /data/git_env/LLaMA-Factory/saves/ChatGLM2-6B-Chat/lora/2023-10-26-23-58-56


```

# end

